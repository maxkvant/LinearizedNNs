{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jax_on_cifar10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f026ddbf726f4d75a5403369c26af682": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_cce8225eaf5149a6b05bd7733d1c2dda",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e5336a900f9a427ca2f16c0016f12d42",
              "IPY_MODEL_8cbf5e90878b45769b626b8f35e7ae10"
            ]
          }
        },
        "cce8225eaf5149a6b05bd7733d1c2dda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e5336a900f9a427ca2f16c0016f12d42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_74c523f79076496ebd2751de28632e58",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1fce878106f24b339ce486be43335d61"
          }
        },
        "8cbf5e90878b45769b626b8f35e7ae10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c56c47facb4a4befb30f4c1e8d4aed90",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:06&lt;00:00, 25008317.16it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_413909e648964b3d9a458077159ca27d"
          }
        },
        "74c523f79076496ebd2751de28632e58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1fce878106f24b339ce486be43335d61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c56c47facb4a4befb30f4c1e8d4aed90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "413909e648964b3d9a458077159ca27d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fb0rfDrH4nQ",
        "colab_type": "code",
        "outputId": "2ef75a7c-a9c0-42c0-8c9c-2896d454d484",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "source": [
        "!pip install neural-tangents"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting neural-tangents\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/e3/c191dd23f6a15199902157557b3ac59427673c1f5f0bc06580dca8003fe5/neural_tangents-0.1.9-py2.py3-none-any.whl (77kB)\n",
            "\r\u001b[K     |████▏                           | 10kB 19.3MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 30kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 71kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 2.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from neural-tangents) (0.7)\n",
            "Requirement already satisfied: jax>=0.1.58 in /usr/local/lib/python3.6/dist-packages (from neural-tangents) (0.1.62)\n",
            "Collecting frozendict\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/55/a12ded2c426a4d2bee73f88304c9c08ebbdbadb82569ebdd6a0c007cfd08/frozendict-1.2.tar.gz\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.6/dist-packages (from jax>=0.1.58->neural-tangents) (3.2.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from jax>=0.1.58->neural-tangents) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from jax>=0.1.58->neural-tangents) (1.18.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py->jax>=0.1.58->neural-tangents) (1.12.0)\n",
            "Building wheels for collected packages: frozendict\n",
            "  Building wheel for frozendict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for frozendict: filename=frozendict-1.2-cp36-none-any.whl size=3149 sha256=71aab260a5c79f912a0fc684b89233c85a3f4379d8d1f7255afa8cb8df37a092\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/6c/e9/534386165bd12cf1885582c75eb6d0ffcb321b65c23fe0f834\n",
            "Successfully built frozendict\n",
            "Installing collected packages: frozendict, neural-tangents\n",
            "Successfully installed frozendict-1.2 neural-tangents-0.1.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIYVjUNJyZl-",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbuYAvktiDw0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import itertools\n",
        "\n",
        "import numpy.random as npr\n",
        "\n",
        "import jax.numpy as np\n",
        "from jax.config import config\n",
        "from jax import jit, grad, random\n",
        "from  jax.nn import log_softmax\n",
        "\n",
        "from jax.experimental import optimizers\n",
        "import jax.experimental.stax as jax_stax\n",
        "import neural_tangents.stax as nt_stax\n",
        "\n",
        "import neural_tangents\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms, datasets\n",
        "from torchvision.datasets import FashionMNIST"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "al_eETI_iMnS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_to_numpy(dataloader):\n",
        "    X = []\n",
        "    y = []\n",
        "    for batch_id, (cur_X, cur_y) in enumerate(dataloader):\n",
        "        X.extend(cur_X.numpy())\n",
        "        y.extend(cur_y.numpy())\n",
        "    X = np.asarray(X)\n",
        "    y = np.asarray(y)\n",
        "    return X, y\n",
        "\n",
        "def _one_hot(x, k, dtype=np.float32):\n",
        "  \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
        "  return np.array(x[:, None] == np.arange(k), dtype)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxobG2mUlVkl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cifar_10():\n",
        "  torch.manual_seed(0)\n",
        "\n",
        "  D = 32\n",
        "  num_classes = 10\n",
        "\n",
        "  torch.manual_seed(0)\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "      device = torch.device('cuda:0')\n",
        "  else:\n",
        "      device = torch.device('cpu')\n",
        "\n",
        "  cifar10_stats = {\n",
        "      \"mean\" : (0.4914, 0.4822, 0.4465),\n",
        "      \"std\"  : (0.24705882352941178, 0.24352941176470588, 0.2615686274509804),\n",
        "  }\n",
        "\n",
        "  simple_transform = transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(cifar10_stats['mean'], cifar10_stats['std']),\n",
        "  ])\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "                    datasets.CIFAR10(root='./data', train=True, download=True, transform=simple_transform),\n",
        "                batch_size=2048, shuffle=True, pin_memory=True)\n",
        "\n",
        "  test_loader  = torch.utils.data.DataLoader(\n",
        "                    datasets.CIFAR10(root='./data', train=False, download=True, transform=simple_transform),\n",
        "                batch_size=2048, shuffle=True, pin_memory=True)\n",
        "  \n",
        "  train_images, train_labels = data_to_numpy(train_loader)\n",
        "  test_images,  test_labels  = data_to_numpy(test_loader)\n",
        "\n",
        "  train_images = np.transpose(train_images, (0, 2, 3, 1))\n",
        "  test_images  = np.transpose(test_images , (0, 2, 3, 1))\n",
        "\n",
        "  train_labels = _one_hot(train_labels, num_classes)\n",
        "  test_labels  = _one_hot(test_labels,  num_classes)\n",
        "  return train_images, train_labels, test_images, test_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Po4zTFUOlqc_",
        "colab_type": "code",
        "outputId": "dad86e6c-dbd5-4cde-a066-c375b0d6d659",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150,
          "referenced_widgets": [
            "f026ddbf726f4d75a5403369c26af682",
            "cce8225eaf5149a6b05bd7733d1c2dda",
            "e5336a900f9a427ca2f16c0016f12d42",
            "8cbf5e90878b45769b626b8f35e7ae10",
            "74c523f79076496ebd2751de28632e58",
            "1fce878106f24b339ce486be43335d61",
            "c56c47facb4a4befb30f4c1e8d4aed90",
            "413909e648964b3d9a458077159ca27d"
          ]
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "train_images, train_labels, test_images, test_labels = cifar_10()\n",
        "train_images.shape, train_labels.shape, test_images.shape, test_labels.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f026ddbf726f4d75a5403369c26af682",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "\n",
            "CPU times: user 1min 22s, sys: 26.3 s, total: 1min 48s\n",
            "Wall time: 1min 31s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPdVuVLYoXBg",
        "colab_type": "code",
        "outputId": "1c7dccbe-d0b2-4703-e958-c3c32dfdf9d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_images.shape, train_labels.shape, test_images.shape, test_labels.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50000, 32, 32, 3), (50000, 10), (10000, 32, 32, 3), (10000, 10))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHqvbQrHxldR",
        "colab_type": "text"
      },
      "source": [
        "## Define training primitives\n",
        "\n",
        "Note: The training code is based on the following example: https://github.com/google/jax/blob/master/examples/mnist_classifier.py."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfZ3JeDRonFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(params, batch):\n",
        "  inputs, targets = batch\n",
        "  preds = predict(params, inputs)\n",
        "  return -np.mean(np.sum(log_softmax(preds, axis=1) * targets, axis=1))\n",
        "\n",
        "def accuracy(params, batch):\n",
        "  inputs, targets = batch\n",
        "  target_class = np.argmax(targets, axis=1)\n",
        "  predicted_class = np.argmax(predict(params, inputs), axis=1)\n",
        "  return np.mean(predicted_class == target_class)\n",
        "\n",
        "@jit\n",
        "def update(i, opt_state, batch):\n",
        "  params = get_params(opt_state)\n",
        "  return opt_update(i, grad(loss)(params, batch), opt_state)\n",
        "\n",
        "rng_state = npr.RandomState(0)\n",
        "\n",
        "def data_stream_of(images, labels, batch_size=500, batch_limit=None):\n",
        "  assert len(images) == len(labels)\n",
        "  rng = npr.RandomState(0)\n",
        "\n",
        "  n = len(images)\n",
        "  perm = rng.permutation(n)\n",
        "  for i in range(n // batch_size):\n",
        "    if (batch_limit is not None) and i >= batch_limit:\n",
        "      break\n",
        "    batch_idx = perm[i * batch_size:(i + 1) * batch_size]\n",
        "    yield images[batch_idx], labels[batch_idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cee5VLeZxzVG",
        "colab_type": "text"
      },
      "source": [
        "## Train a small CNN in JAX with NTK parameterization\n",
        "\n",
        "Here I do a few epochs to make sure that my training code works.\n",
        "\n",
        "I do mix `jax.stax` with `neural_tangents.stax` because I want to use both BatchNorm and NTK parameterizaton. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qw_2zOQ1JHa-",
        "colab_type": "code",
        "outputId": "6f95166a-b5a1-49e8-e314-b88678e0bbbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        }
      },
      "source": [
        "channels = 32\n",
        "num_classes = 10\n",
        "\n",
        "init_random_params, predict = jax_stax.serial(\n",
        "      nt_stax.Conv(channels, (3, 3), padding='SAME'),                jax_stax.BatchNorm(), nt_stax.Relu(),\n",
        "      nt_stax.Conv(channels, (3, 3), strides=(2,2), padding='SAME'), jax_stax.BatchNorm(), nt_stax.Relu(),\n",
        "      nt_stax.Conv(channels, (3, 3), strides=(2,2), padding='SAME'), jax_stax.BatchNorm(), nt_stax.Relu(),\n",
        "      nt_stax.Conv(channels, (3, 3), strides=(2,2), padding='SAME'), jax_stax.BatchNorm(), nt_stax.Relu(),\n",
        "      nt_stax.AvgPool((1, 1)),   nt_stax.Flatten(), \n",
        "      nt_stax.Dense(num_classes), jax_stax.Identity,\n",
        ")\n",
        "rng = random.PRNGKey(0)\n",
        "\n",
        "step_size = 10.\n",
        "num_epochs = 10\n",
        "batch_size = 500\n",
        "momentum_mass = 0.9\n",
        "\n",
        "opt_init, opt_update, get_params = optimizers.momentum(step_size, mass=momentum_mass)\n",
        "\n",
        "_, init_params = init_random_params(rng, (batch_size, 32, 32, 3))\n",
        "opt_state = opt_init(init_params)\n",
        "itercount = itertools.count()\n",
        "\n",
        "print(\"\\nStarting training...\")\n",
        "for epoch in range(num_epochs):\n",
        "  start_time = time.time()\n",
        "\n",
        "  for batch in data_stream_of(train_images, train_labels):\n",
        "    opt_state = update(next(itercount), opt_state, batch)\n",
        "  params = get_params(opt_state)\n",
        "\n",
        "  train_accs = [accuracy(params, batch) for batch in data_stream_of(train_images, train_labels)]  \n",
        "  train_acc  = np.average(train_accs)\n",
        "  test_accs  = [accuracy(params, batch) for batch in data_stream_of(test_images, test_labels)]  \n",
        "  test_acc   = np.average(test_accs)\n",
        "  \n",
        "  epoch_time = time.time() - start_time\n",
        "\n",
        "  print(f\"Epoch {epoch} in {epoch_time:0.2f} sec\")\n",
        "  print(f\"Training set accuracy {train_acc}\")\n",
        "  print(f\"Test set accuracy {test_acc}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Starting training...\n",
            "Epoch 0 in 13.36 sec\n",
            "Training set accuracy 0.4674600064754486\n",
            "Test set accuracy 0.46229997277259827\n",
            "Epoch 1 in 11.62 sec\n",
            "Training set accuracy 0.5352200269699097\n",
            "Test set accuracy 0.5210000872612\n",
            "Epoch 2 in 11.41 sec\n",
            "Training set accuracy 0.5723000168800354\n",
            "Test set accuracy 0.5527001023292542\n",
            "Epoch 3 in 11.30 sec\n",
            "Training set accuracy 0.5975600481033325\n",
            "Test set accuracy 0.57340008020401\n",
            "Epoch 4 in 11.43 sec\n",
            "Training set accuracy 0.6060200333595276\n",
            "Test set accuracy 0.5822001099586487\n",
            "Epoch 5 in 11.45 sec\n",
            "Training set accuracy 0.6427800059318542\n",
            "Test set accuracy 0.607900083065033\n",
            "Epoch 6 in 11.58 sec\n",
            "Training set accuracy 0.648140013217926\n",
            "Test set accuracy 0.6154000759124756\n",
            "Epoch 7 in 11.45 sec\n",
            "Training set accuracy 0.6545000076293945\n",
            "Test set accuracy 0.6219999194145203\n",
            "Epoch 8 in 11.48 sec\n",
            "Training set accuracy 0.6666399836540222\n",
            "Test set accuracy 0.6288000345230103\n",
            "Epoch 9 in 11.50 sec\n",
            "Training set accuracy 0.6705000400543213\n",
            "Test set accuracy 0.6308000087738037\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8lAFJbPxdkX",
        "colab_type": "text"
      },
      "source": [
        "## Train a ResNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7-fX58nnWdT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_classes = 10\n",
        "\n",
        "def WideResnetBlock(channels, strides=(1, 1), channel_mismatch=False):\n",
        "  Main = jax_stax.serial(\n",
        "      nt_stax.Conv(channels, (3, 3), strides, padding='SAME'), jax_stax.BatchNorm(), nt_stax.Relu(), \n",
        "      nt_stax.Conv(channels, (3, 3), padding='SAME'),          jax_stax.BatchNorm(), nt_stax.Relu(), \n",
        "      jax_stax.Identity\n",
        "  )\n",
        "  Shortcut = nt_stax.Identity() if not channel_mismatch else nt_stax.Conv(channels, (3, 3), strides, padding='SAME')\n",
        "  return jax_stax.serial(jax_stax.FanOut(2),\n",
        "                         jax_stax.parallel(Main, Shortcut),\n",
        "                         jax_stax.FanInSum,\n",
        "                         jax_stax.Identity)\n",
        "\n",
        "def WideResnetGroup(n, channels, strides=(1, 1)):\n",
        "  blocks = []\n",
        "  blocks += [WideResnetBlock(channels, strides, channel_mismatch=True)]\n",
        "  for _ in range(n - 1):\n",
        "    blocks += [WideResnetBlock(channels, (1, 1))]\n",
        "  return jax_stax.serial(*blocks)\n",
        "\n",
        "def WideResnet(num_classes, num_channels=32, block_size=1):\n",
        "  return jax_stax.serial(\n",
        "      nt_stax.Conv(num_channels, (3, 3), padding='SAME'), jax_stax.BatchNorm(), nt_stax.Relu(),\n",
        "      WideResnetGroup(block_size, num_channels),\n",
        "      WideResnetGroup(block_size, num_channels, (2, 2)),\n",
        "      WideResnetGroup(block_size, num_channels, (2, 2)),\n",
        "      nt_stax.AvgPool((1, 1)),\n",
        "      nt_stax.Flatten(),\n",
        "      nt_stax.Dense(num_classes),\n",
        "      jax_stax.Identity\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z46NcGJd3HxL",
        "colab_type": "code",
        "outputId": "a8421e1b-6cc8-4c18-ce5d-01f22c8b5d47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        }
      },
      "source": [
        "init_random_params, predict = WideResnet(num_classes)\n",
        "\n",
        "rng = random.PRNGKey(0)\n",
        "\n",
        "step_size = 10.\n",
        "num_epochs = 10\n",
        "momentum_mass = 0.9\n",
        "\n",
        "opt_init, opt_update, get_params = optimizers.momentum(step_size, mass=momentum_mass)\n",
        "\n",
        "_, init_params = init_random_params(rng, (batch_size, 32, 32, 3))\n",
        "opt_state = opt_init(init_params)\n",
        "itercount = itertools.count()\n",
        "\n",
        "print(\"\\nStarting training...\")\n",
        "for epoch in range(num_epochs):\n",
        "  start_time = time.time()\n",
        "\n",
        "  for batch in data_stream_of(train_images, train_labels):\n",
        "    opt_state = update(next(itercount), opt_state, batch)\n",
        "  params = get_params(opt_state)\n",
        "\n",
        "  train_accs = [accuracy(params, batch) for batch in data_stream_of(train_images, train_labels, batch_limit=4)]  \n",
        "  train_acc  = np.average(train_accs)\n",
        "  test_accs  = [accuracy(params, batch) for batch in data_stream_of(test_images, test_labels, batch_limit=4)]  \n",
        "  test_acc   = np.average(test_accs)\n",
        "  \n",
        "  epoch_time = time.time() - start_time\n",
        "\n",
        "  print(f\"Epoch {epoch} in {epoch_time:0.2f} sec\")\n",
        "  print(f\"Training set accuracy {train_acc}\")\n",
        "  print(f\"Test set accuracy {test_acc}\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Starting training...\n",
            "Epoch 0 in 10.01 sec\n",
            "Training set accuracy 0.4259999990463257\n",
            "Test set accuracy 0.44600000977516174\n",
            "Epoch 1 in 5.16 sec\n",
            "Training set accuracy 0.44450002908706665\n",
            "Test set accuracy 0.4360000193119049\n",
            "Epoch 2 in 5.13 sec\n",
            "Training set accuracy 0.5074999928474426\n",
            "Test set accuracy 0.5080000162124634\n",
            "Epoch 3 in 5.18 sec\n",
            "Training set accuracy 0.5400000214576721\n",
            "Test set accuracy 0.5275000333786011\n",
            "Epoch 4 in 5.23 sec\n",
            "Training set accuracy 0.5705000162124634\n",
            "Test set accuracy 0.5400000214576721\n",
            "Epoch 5 in 5.19 sec\n",
            "Training set accuracy 0.5830000042915344\n",
            "Test set accuracy 0.5649999976158142\n",
            "Epoch 6 in 5.16 sec\n",
            "Training set accuracy 0.6200000047683716\n",
            "Test set accuracy 0.5915000438690186\n",
            "Epoch 7 in 5.16 sec\n",
            "Training set accuracy 0.6640000343322754\n",
            "Test set accuracy 0.6075000762939453\n",
            "Epoch 8 in 5.14 sec\n",
            "Training set accuracy 0.655500054359436\n",
            "Test set accuracy 0.6155000329017639\n",
            "Epoch 9 in 5.18 sec\n",
            "Training set accuracy 0.6620000004768372\n",
            "Test set accuracy 0.6234999895095825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqghpbVIwomC",
        "colab_type": "text"
      },
      "source": [
        "## Train a linearization of ResNet\n",
        "\n",
        "Note: I have removed the BatchNorm layers because with them training didn't work. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQIdS3uzktGU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "from jax.tree_util import tree_multimap\n",
        "from jax.api import jvp\n",
        "from jax.api import vjp\n",
        "\n",
        "# copied from \n",
        "def linearize(f, params):\n",
        "  \"\"\"Returns a function `f_lin`, the first order taylor approximation to `f`.\n",
        "  Example:\n",
        "    >>> # Compute the MSE of the first order Taylor series of a function.\n",
        "    >>> f_lin = linearize(f, params)\n",
        "    >>> mse = np.mean((f(new_params, x) - f_lin(new_params, x)) ** 2)\n",
        "  \"\"\"\n",
        "  @jit\n",
        "  def f_lin(p, *args, **kwargs):\n",
        "    dparams = tree_multimap(lambda x, y: x - y, p, params)\n",
        "    f_params_x, proj = jvp(lambda param: f(param, *args, **kwargs),\n",
        "                           (params,), (dparams,))\n",
        "    return f_params_x + proj\n",
        "  return f_lin"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bzk2t2JUqh7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def WideResnetBlock(channels, strides=(1, 1), channel_mismatch=False):\n",
        "  Main = jax_stax.serial(\n",
        "      nt_stax.Conv(channels, (3, 3), strides, padding='SAME'), nt_stax.Relu(), \n",
        "      nt_stax.Conv(channels, (3, 3), padding='SAME'),          nt_stax.Relu(), \n",
        "      jax_stax.Identity\n",
        "  )\n",
        "  Shortcut = nt_stax.Identity() if not channel_mismatch else nt_stax.Conv(channels, (3, 3), strides, padding='SAME')\n",
        "  return jax_stax.serial(jax_stax.FanOut(2),\n",
        "                         jax_stax.parallel(Main, Shortcut),\n",
        "                         jax_stax.FanInSum,\n",
        "                         jax_stax.Identity)\n",
        "\n",
        "def WideResnetGroup(n, channels, strides=(1, 1)):\n",
        "  blocks = []\n",
        "  blocks += [WideResnetBlock(channels, strides, channel_mismatch=True)]\n",
        "  for _ in range(n - 1):\n",
        "    blocks += [WideResnetBlock(channels, (1, 1))]\n",
        "  return jax_stax.serial(*blocks)\n",
        "\n",
        "def WideResnet(num_classes, num_channels=32, block_size=1):\n",
        "  return jax_stax.serial(\n",
        "      nt_stax.Conv(num_channels, (3, 3), padding='SAME'), nt_stax.Relu(),\n",
        "      WideResnetGroup(block_size, num_channels),\n",
        "      WideResnetGroup(block_size, num_channels, (2, 2)),\n",
        "      WideResnetGroup(block_size, num_channels, (2, 2)),\n",
        "      nt_stax.AvgPool((1, 1)),\n",
        "      nt_stax.Flatten(),\n",
        "      nt_stax.Dense(num_classes),\n",
        "      jax_stax.Identity\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tlTPH4ckoF5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "87b8e043-3f2b-4662-b1ac-d9f137151763"
      },
      "source": [
        "num_classes = 10\n",
        "\n",
        "init_random_params, predict = WideResnet(num_classes, num_channels=512)\n",
        "\n",
        "rng = random.PRNGKey(0)\n",
        "\n",
        "step_size = 1.\n",
        "num_epochs = 100\n",
        "momentum_mass = 0.9\n",
        "\n",
        "opt_init, opt_update, get_params = optimizers.momentum(step_size, mass=momentum_mass)\n",
        "\n",
        "_, init_params = init_random_params(rng, (batch_size, 32, 32, 3))\n",
        "opt_state = opt_init(init_params)\n",
        "itercount = itertools.count()\n",
        "\n",
        "predict = linearize(predict, init_params) # !important: linearization\n",
        "\n",
        "print(\"\\nStarting training...\")\n",
        "for epoch in range(num_epochs):\n",
        "  start_time = time.time()\n",
        "\n",
        "  for batch in data_stream_of(train_images, train_labels, batch_size=100):\n",
        "    opt_state = update(next(itercount), opt_state, batch)\n",
        "  params = get_params(opt_state)\n",
        "\n",
        "  train_accs = [accuracy(params, batch) for batch in data_stream_of(train_images, train_labels, batch_size=100, batch_limit=20)]  \n",
        "  train_acc  = np.average(train_accs)\n",
        "  test_accs  = [accuracy(params, batch) for batch in data_stream_of(test_images, test_labels, batch_size=100, batch_limit=20)]  \n",
        "  test_acc   = np.average(test_accs)\n",
        "  \n",
        "  epoch_time = time.time() - start_time\n",
        "\n",
        "  print(f\"Epoch {epoch} in {epoch_time:0.2f} sec\")\n",
        "  print(f\"Training set accuracy {train_acc}\")\n",
        "  print(f\"Test set accuracy {test_acc}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Starting training...\n",
            "Epoch 0 in 297.33 sec\n",
            "Training set accuracy 0.46000003814697266\n",
            "Test set accuracy 0.40749993920326233\n",
            "Epoch 1 in 293.79 sec\n",
            "Training set accuracy 0.4874999523162842\n",
            "Test set accuracy 0.4374999701976776\n",
            "Epoch 2 in 293.76 sec\n",
            "Training set accuracy 0.5044999718666077\n",
            "Test set accuracy 0.4364999234676361\n",
            "Epoch 3 in 293.63 sec\n",
            "Training set accuracy 0.5345000624656677\n",
            "Test set accuracy 0.4519999623298645\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}