{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jax_on_cifar10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbuYAvktiDw0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import itertools\n",
        "\n",
        "import numpy.random as npr\n",
        "\n",
        "import jax.numpy as np\n",
        "from jax.config import config\n",
        "from jax import jit, grad, random\n",
        "from jax.experimental import optimizers\n",
        "from jax.experimental import stax\n",
        "from jax.experimental.stax import Dense, Relu, LogSoftmax\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms, datasets\n",
        "from torchvision.datasets import FashionMNIST"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "al_eETI_iMnS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_to_numpy(dataloader):\n",
        "    X = []\n",
        "    y = []\n",
        "    for batch_id, (cur_X, cur_y) in enumerate(dataloader):\n",
        "        X.extend(cur_X.numpy())\n",
        "        y.extend(cur_y.numpy())\n",
        "    X = np.asarray(X)\n",
        "    y = np.asarray(y)\n",
        "    return X, y\n",
        "\n",
        "def _one_hot(x, k, dtype=np.float32):\n",
        "  \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
        "  return np.array(x[:, None] == np.arange(k), dtype)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxobG2mUlVkl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cifar_10():\n",
        "  torch.manual_seed(0)\n",
        "\n",
        "  D = 32\n",
        "  num_classes = 10\n",
        "\n",
        "  torch.manual_seed(0)\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "      device = torch.device('cuda:0')\n",
        "  else:\n",
        "      device = torch.device('cpu')\n",
        "\n",
        "  cifar10_stats = {\n",
        "      \"mean\" : (0.4914, 0.4822, 0.4465),\n",
        "      \"std\"  : (0.24705882352941178, 0.24352941176470588, 0.2615686274509804),\n",
        "  }\n",
        "\n",
        "  simple_transform = transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(cifar10_stats['mean'], cifar10_stats['std']),\n",
        "  ])\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "                    datasets.CIFAR10(root='./data', train=True, download=True, transform=simple_transform),\n",
        "                batch_size=2048, shuffle=True, pin_memory=True)\n",
        "\n",
        "  test_loader  = torch.utils.data.DataLoader(\n",
        "                    datasets.CIFAR10(root='./data', train=False, download=True, transform=simple_transform),\n",
        "                batch_size=2048, shuffle=True, pin_memory=True)\n",
        "  \n",
        "  train_images, train_labels = data_to_numpy(train_loader)\n",
        "  test_images,  test_labels  = data_to_numpy(test_loader)\n",
        "\n",
        "  train_images = np.transpose(train_images, (0, 2, 3, 1))\n",
        "  test_images  = np.transpose(test_images , (0, 2, 3, 1))\n",
        "\n",
        "  train_labels = _one_hot(train_labels, num_classes)\n",
        "  test_labels  = _one_hot(test_labels,  num_classes)\n",
        "  return train_images, train_labels, test_images, test_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Po4zTFUOlqc_",
        "colab_type": "code",
        "outputId": "71c75152-468a-4cb9-d1d5-87c9bf180159",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "train_images, train_labels, test_images, test_labels = cifar_10()\n",
        "train_images.shape, train_labels.shape, test_images.shape, test_labels.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "CPU times: user 54.7 s, sys: 17.3 s, total: 1min 12s\n",
            "Wall time: 56.2 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPdVuVLYoXBg",
        "colab_type": "code",
        "outputId": "9bfef3c4-c07b-4097-ba6d-9b04b8f0205d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_images.shape, train_labels.shape, test_images.shape, test_labels.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50000, 32, 32, 3), (50000, 10), (10000, 32, 32, 3), (10000, 10))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfZ3JeDRonFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(params, batch):\n",
        "  inputs, targets = batch\n",
        "  preds = predict(params, inputs)\n",
        "  return -np.mean(np.sum(preds * targets, axis=1))\n",
        "\n",
        "def accuracy(params, batch):\n",
        "  inputs, targets = batch\n",
        "  target_class = np.argmax(targets, axis=1)\n",
        "  predicted_class = np.argmax(predict(params, inputs), axis=1)\n",
        "  return np.mean(predicted_class == target_class)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQlpRpDBotR-",
        "colab_type": "code",
        "outputId": "0e952d7f-96c6-4511-f651-88f626bc4962",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        }
      },
      "source": [
        "channels = 32\n",
        "num_classes = 10\n",
        "\n",
        "init_random_params, predict = stax.serial(\n",
        "      stax.Conv(channels, (3, 3), padding='SAME'),                stax.BatchNorm(), stax.Relu,\n",
        "      stax.Conv(channels, (3, 3), strides=(2,2), padding='SAME'), stax.BatchNorm(), stax.Relu,\n",
        "      stax.Conv(channels, (3, 3), strides=(2,2), padding='SAME'), stax.BatchNorm(), stax.Relu,\n",
        "      stax.Conv(channels, (3, 3), strides=(2,2), padding='SAME'), stax.BatchNorm(), stax.Relu,\n",
        "      stax.AvgPool((1, 1)), stax.Flatten, \n",
        "      stax.Dense(num_classes), stax.LogSoftmax\n",
        ")\n",
        "rng = random.PRNGKey(0)\n",
        "\n",
        "step_size = 0.05\n",
        "num_epochs = 10\n",
        "batch_size = 500\n",
        "momentum_mass = 0.9\n",
        "\n",
        "num_complete_batches, leftover = divmod(num_train, batch_size)\n",
        "num_batches = num_complete_batches + bool(leftover)\n",
        "\n",
        "def data_stream_of(images, labels):\n",
        "  assert len(images) == len(labels)\n",
        "  rng = npr.RandomState(0)\n",
        "\n",
        "  n = len(images)\n",
        "  perm = rng.permutation(n)\n",
        "  for i in range(n // batch_size):\n",
        "    batch_idx = perm[i * batch_size:(i + 1) * batch_size]\n",
        "    yield images[batch_idx], labels[batch_idx]\n",
        "\n",
        "opt_init, opt_update, get_params = optimizers.momentum(step_size, mass=momentum_mass)\n",
        "\n",
        "@jit\n",
        "def update(i, opt_state, batch):\n",
        "  params = get_params(opt_state)\n",
        "  return opt_update(i, grad(loss)(params, batch), opt_state)\n",
        "\n",
        "_, init_params = init_random_params(rng, (batch_size, 32, 32, 3))\n",
        "opt_state = opt_init(init_params)\n",
        "itercount = itertools.count()\n",
        "\n",
        "print(\"\\nStarting training...\")\n",
        "for epoch in range(num_epochs):\n",
        "  start_time = time.time()\n",
        "\n",
        "  for batch in data_stream_of(train_images, train_labels):\n",
        "    opt_state = update(next(itercount), opt_state, batch)\n",
        "  params = get_params(opt_state)\n",
        "\n",
        "  train_accs = [accuracy(params, batch) for batch in data_stream_of(train_images, train_labels)]  \n",
        "  train_acc  = np.average(train_accs)\n",
        "  test_accs  = [accuracy(params, batch) for batch in data_stream_of(test_images, test_labels)]  \n",
        "  test_acc   = np.average(test_accs)\n",
        "  \n",
        "  epoch_time = time.time() - start_time\n",
        "\n",
        "  print(f\"Epoch {epoch} in {epoch_time:0.2f} sec\")\n",
        "  print(f\"Training set accuracy {train_acc}\")\n",
        "  print(f\"Test set accuracy {test_acc}\")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Starting training...\n",
            "Epoch 0 in 11.06 sec\n",
            "Training set accuracy 0.541379988193512\n",
            "Test set accuracy 0.5235000252723694\n",
            "Epoch 1 in 9.74 sec\n",
            "Training set accuracy 0.6169600486755371\n",
            "Test set accuracy 0.5885000824928284\n",
            "Epoch 2 in 9.74 sec\n",
            "Training set accuracy 0.6624999642372131\n",
            "Test set accuracy 0.621399998664856\n",
            "Epoch 3 in 9.62 sec\n",
            "Training set accuracy 0.6905400156974792\n",
            "Test set accuracy 0.6450001001358032\n",
            "Epoch 4 in 9.67 sec\n",
            "Training set accuracy 0.7164599299430847\n",
            "Test set accuracy 0.6626001000404358\n",
            "Epoch 5 in 9.90 sec\n",
            "Training set accuracy 0.7408199906349182\n",
            "Test set accuracy 0.6811999678611755\n",
            "Epoch 6 in 9.88 sec\n",
            "Training set accuracy 0.7523201107978821\n",
            "Test set accuracy 0.6846001148223877\n",
            "Epoch 7 in 9.83 sec\n",
            "Training set accuracy 0.7669000029563904\n",
            "Test set accuracy 0.6910000443458557\n",
            "Epoch 8 in 9.65 sec\n",
            "Training set accuracy 0.76910001039505\n",
            "Test set accuracy 0.6869000792503357\n",
            "Epoch 9 in 9.68 sec\n",
            "Training set accuracy 0.7783799767494202\n",
            "Test set accuracy 0.6927000284194946\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}