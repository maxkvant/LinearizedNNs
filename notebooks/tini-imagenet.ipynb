{"nbformat":4,"nbformat_minor":5,"metadata":{"notebookId":"2b07f015-22f0-4d11-95f5-4617e0e6c876","language_info":{"file_extension":".py","pygments_lexer":"ipython3","nbconvert_exporter":"python","name":"python","codemirror_mode":{"version":3,"name":"ipython"},"mimetype":"text/x-python","version":"3.7.7"},"kernelspec":{"name":"python3","display_name":"Yandex DataSphere Kernel","language":"python"}},"cells":[{"cell_type":"code","source":"#!L\nimport time\nimport numpy as np\nimport seaborn as sns\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms, datasets\nfrom torch import utils\n\nfrom linearized_nns.estimator import Estimator\nfrom linearized_nns.pytorch_impl.estimators import SgdEstimator\nfrom linearized_nns.pytorch_impl.nns import Myrtle5, Myrtle7, Myrtle10\nfrom linearized_nns.pytorch_impl import ClassifierTraining\nfrom linearized_nns.pytorch_impl.matrix_exp import matrix_exp, compute_exp_term\nfrom linearized_nns.pytorch_impl.nns.utils import to_one_hot, print_sizes\nfrom linearized_nns.from_neural_kernels import to_zca, CustomTensorDataset, get_cifar_zca","metadata":{"cellId":"ngodmhphrupapddzqzvte","trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"#!L\ndevice = torch.device('cuda:0') if (torch.cuda.is_available()) else torch.device('cpu')\ndevice","metadata":{"cellId":"b0t66s3fn5ny46ywexoixf","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"https://gist.github.com/bveliqi/5efe7d20c99025d02df87e4c595711c1","metadata":{"cellId":"mth5fsg4k1q2izbm2c37"}},{"cell_type":"code","source":"#!L\n\ntrain_dir = 'tiny-imagenet-200/train'\ntest_dir  = 'tiny-imagenet-200/val'\n\nnormalize = transforms.Normalize([0.4802, 0.4481, 0.3975], [0.2302, 0.2265, 0.2262])\n\ntrain_transforms = transforms.Compose([\n    transforms.ToTensor(),\n    normalize,\n])\n\ntest_transforms = transforms.Compose([\n    transforms.ToTensor(),\n    normalize,\n])\n\nnp.random.seed(0)\nrandom.seed(0)\ntorch.manual_seed(0)\n\ntrainset = datasets.ImageFolder(train_dir, transform=train_transforms) \ntestset  = datasets.ImageFolder(test_dir,  transform=test_transforms)\n\ntrainloader = utils.data.DataLoader(trainset, pin_memory=True, shuffle=True, batch_size=1280)\ntestloader  = utils.data.DataLoader(testset,  pin_memory=True, shuffle=True, batch_size=1000)","metadata":{"cellId":"di3atlh5bwqk45hn8yuesn","trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"#!L\ndef get_tiny_image_net_zca():\n    np.random.seed(0)\n    random.seed(0)\n    torch.manual_seed(0)\n    \n    trainset = datasets.ImageFolder(train_dir, transform=transforms.ToTensor()) \n    testset  = datasets.ImageFolder(test_dir, transform=transforms.ToTensor())\n    \n    trainloader = utils.data.DataLoader(trainset, pin_memory=True, shuffle=True, batch_size=100000)\n    testloader  = utils.data.DataLoader(testset,  pin_memory=True, shuffle=True, batch_size=10000)\n    \n    _, (X_train, y_train)  = next(enumerate(trainloader))\n    _, (X_test,  y_test)   = next(enumerate(testloader))\n    \n    X_train = X_train.numpy().astype(np.float64)\n    X_test  = X_test.numpy().astype(np.float64)\n    \n    (X_train, X_test), global_ZCA = to_zca(X_train, X_test)\n\n    X_train = np.transpose(X_train, (0,3,1,2))\n    X_test  = np.transpose(X_test,  (0,3,1,2))\n\n    return torch.tensor(X_train), y_train, torch.tensor(X_test), y_test","metadata":{"cellId":"bcnbve3hou8nujhxzudpxq","trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"#!L\n%%time\nX_train, y_train, X_test, y_test = get_tiny_image_net_zca()","metadata":{"cellId":"5dnixrzgephgz9nrwtpayt","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"CPU times: user 2h 23min 37s, sys: 25min 49s, total: 2h 49min 26s\nWall time: 30min 44s\n"}],"execution_count":5},{"cell_type":"code","source":"#!L\ntorch.save(X_train, train_dir + '/X_train.pt')\ntorch.save(y_train, train_dir + '/y_train.pt')\ntorch.save(X_test,  test_dir  + '/X_val.pt')\ntorch.save(y_test,  test_dir  + '/y_val.pt')","metadata":{"cellId":"hzzkmkv5tch5vl1kv21xb","trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"#!L\nX_train.shape, X_test.shape","metadata":{"cellId":"hr1bixf9dzvhoww3o4cz78","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"(torch.Size([100000, 64, 3, 64]), torch.Size([10000, 64, 3, 64]))"},"metadata":{}}],"execution_count":216},{"cell_type":"code","source":"#!L\nlabels_train_full = y_train\nlabels_test_full  = y_test\n\nX_train_full = X_train\nX_test_full  = X_test","metadata":{"cellId":"1zb1fufjwqlx8c5jhkrd4","trusted":true},"outputs":[],"execution_count":217},{"cell_type":"code","source":"#!L \nX_train_full = X_train_full.permute((0, 2, 1, 3))\nX_test_full  =  X_test_full.permute((0, 2, 1, 3))","metadata":{"cellId":"kwtqgd2ga281gobp17vler","trusted":true},"outputs":[],"execution_count":218},{"cell_type":"code","source":"#!L\ndef compute_kernels(models, X_train, X_test, device):\n    with torch.no_grad():\n        X_train = X_train.to(device)\n        X_test  = X_test.to(device)\n\n        n_train = len(X_train)\n        n_test  = len(X_test)\n\n        train_kernel = torch.zeros([n_train, n_train]).double().to(device)\n        test_kernel  = torch.zeros([n_test,  n_train]).double().to(device)\n\n        m = 0\n        start_time = time.time()\n\n        for model_i, model in enumerate(models):\n            model = model.to(device)\n            if model_i & (model_i - 1) == 0:\n                print(f\"{model_i} models done. time {time.time() - start_time:.0f}s\")\n\n            train_features = model.readout(X_train) \n            test_features  = model.readout(X_test)\n\n            m += 1\n\n            train_kernel += torch.matmul(train_features, train_features.T).double()\n            test_kernel  += torch.matmul(test_features,  train_features.T).double()\n\n        train_kernel /= m\n        test_kernel  /= m\n\n        return train_kernel.float(), test_kernel.float()","metadata":{"cellId":"feesxqle8ka0yfdbf2a3f","trusted":true},"outputs":[],"execution_count":219},{"cell_type":"code","source":"#!L\nfrom linearized_nns.pytorch_impl.nns.primitives import *\n\nclass Myrtle9(nn.Module):\n    def __init__(self, num_classes=1, input_filters=3, num_filters=1, groups=1):\n        super(Myrtle9, self).__init__()\n        filters = num_filters\n\n        def Activation():\n            return ReLU2()\n\n        self.layers = nn.Sequential(\n            Conv(input_filters, filters * groups), Activation(),\n            Conv(filters, filters * 2, groups),    Activation(),\n            nn.AvgPool2d(kernel_size=2, stride=2),\n\n            Conv(filters * 2, filters * 4, groups), Activation(),\n            Conv(filters * 4, filters * 8, groups), Activation(),\n            nn.AvgPool2d(kernel_size=2, stride=2),\n\n            Conv(filters *  8, filters * 16, groups), Activation(),\n            Conv(filters * 16, filters * 32, groups), Activation(),\n            nn.AvgPool2d(kernel_size=2, stride=2),\n            \n            Conv(filters *  32, filters * 32, groups), Activation(),\n            Conv(filters *  32, filters * 32, groups), Activation(),\n            nn.AvgPool2d(kernel_size=8, stride=8),\n\n            Flatten(),\n            Normalize(filters * 32)\n        )\n        self.classifier = nn.Linear(filters * 32 * groups, num_classes, bias=True)\n\n    def readout(self, x):\n        return self.layers(x)\n\n    def forward(self, x):\n        x = self.readout(x)\n        return self.classifier(x)","metadata":{"cellId":"5tv6m0yl5btjlh0wqmzn0a","trusted":true},"outputs":[],"execution_count":223},{"cell_type":"code","source":"#!L\nN_train = 1280 * 10\nN_test  = 1000\n\nX_train = X_train_full[:N_train].float()\nX_test  = X_test_full[:N_test].float()","metadata":{"cellId":"iqdqeq6hihkn5dyn961ca","trusted":true},"outputs":[],"execution_count":250},{"cell_type":"code","source":"#!L\nlabels_train = labels_train_full[:N_train] \nlabels_test  = labels_test_full[:N_test]\n\nnum_classes = 200\n\ny_train = to_one_hot(labels_train, num_classes).to(device)\ny_test  = to_one_hot(labels_test,  num_classes).to(device)","metadata":{"cellId":"w0vk7q2j9ha44rsl8r7oc","trusted":true},"outputs":[],"execution_count":251},{"cell_type":"code","source":"#!L\nX_train.shape, X_test.shape","metadata":{"cellId":"1g8ddqv20ger3ms2wn8aqi","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"(torch.Size([12800, 3, 64, 64]), torch.Size([1000, 3, 64, 64]))"},"metadata":{}}],"execution_count":253},{"cell_type":"code","source":"#!L\ntrain_kernel, test_kernel = compute_kernels(models, X_train, X_test, device)\n\ntrain_kernel = train_kernel.float().to(device)\ntest_kernel  = test_kernel.float().to(device)","metadata":{"cellId":"m9g3udbsb2nbqxf3xox1sq","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"0 models done. time 0s\n1 models done. time 0s\n2 models done. time 1s\n4 models done. time 1s\n8 models done. time 3s\n16 models done. time 6s\n32 models done. time 12s\n64 models done. time 23s\n128 models done. time 46s\n256 models done. time 92s\n512 models done. time 185s\n1024 models done. time 370s\n2048 models done. time 741s\n4096 models done. time 1481s\n"}],"execution_count":254},{"cell_type":"code","source":"#!L\ntrain_kernel[:5,:5]","metadata":{"cellId":"ff73ivumrlxxpjry0z6j","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"tensor([[1.0000, 0.9841, 0.9956, 0.9929, 0.9942],\n        [0.9841, 1.0000, 0.9854, 0.9878, 0.9904],\n        [0.9956, 0.9854, 1.0000, 0.9955, 0.9926],\n        [0.9929, 0.9878, 0.9955, 1.0000, 0.9921],\n        [0.9942, 0.9904, 0.9926, 0.9921, 1.0000]], device='cuda:0')"},"metadata":{}}],"execution_count":255},{"cell_type":"code","source":"#!L\ntest_kernel[:5,:5]","metadata":{"cellId":"7d6vtcta6tomi1c7l5g99","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"tensor([[0.9930, 0.9905, 0.9945, 0.9935, 0.9939],\n        [0.9956, 0.9858, 0.9936, 0.9930, 0.9943],\n        [0.9971, 0.9801, 0.9922, 0.9895, 0.9924],\n        [0.9936, 0.9853, 0.9940, 0.9947, 0.9915],\n        [0.9903, 0.9878, 0.9909, 0.9931, 0.9928]], device='cuda:0')"},"metadata":{}}],"execution_count":256},{"cell_type":"code","source":"#!L\ny_train.shape","metadata":{"cellId":"eln0vtl12e5pbjkm75886b","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"torch.Size([12800, 200])"},"metadata":{}}],"execution_count":257},{"cell_type":"code","source":"#!L\nlr = 1e5\n\nn = len(train_kernel)\n\nexp_term = - lr * compute_exp_term(- lr * train_kernel, device)\n","metadata":{"cellId":"nkvkg158a7kb9wmbtgomd","trusted":true},"outputs":[],"execution_count":258},{"cell_type":"code","source":"#!L\ny_train = y_train.to(device)","metadata":{"cellId":"1ir7nhku1a7695r64g31ax","trusted":true},"outputs":[],"execution_count":259},{"cell_type":"code","source":"#!L\ntrain_kernel.size()","metadata":{"cellId":"hdw1bxpz3n62mlvex6on74","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"torch.Size([12800, 12800])"},"metadata":{}}],"execution_count":260},{"cell_type":"code","source":"#!L\nexp_term.size()","metadata":{"cellId":"wj11q1sw5bevbiztvqj0n","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"torch.Size([12800, 12800])"},"metadata":{}}],"execution_count":261},{"cell_type":"code","source":"#!L\nlabels_test.size()","metadata":{"cellId":"j1cl5kiteomswyukc4jm9","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"torch.Size([1000])"},"metadata":{}}],"execution_count":262},{"cell_type":"code","source":"#!L\ny_pred = torch.matmul(test_kernel, torch.matmul(exp_term, - y_train))\n(y_pred.argmax(dim=1) == labels_test.to(device)).float().mean()","metadata":{"cellId":"msdiugsx11roezdq6y6a5p","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"tensor(0.2270, device='cuda:0')"},"metadata":{}}],"execution_count":263},{"cell_type":"code","source":"#!L\nimport time\nimport numpy as np\nimport seaborn as sns\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms, datasets\nfrom torchvision.datasets import FashionMNIST\n\nfrom linearized_nns.estimator import Estimator\nfrom linearized_nns.pytorch_impl.estimators import SgdEstimator\nfrom linearized_nns.pytorch_impl.nns import Myrtle5, Myrtle7, Myrtle10\nfrom linearized_nns.pytorch_impl import ClassifierTraining\nfrom linearized_nns.pytorch_impl.matrix_exp import matrix_exp, compute_exp_term\nfrom linearized_nns.pytorch_impl.nns.utils import to_one_hot, print_sizes\nfrom linearized_nns.from_neural_kernels import to_zca, CustomTensorDataset, get_cifar_zca","metadata":{"cellId":"orvsya8j1jx5rgxzn2oy","trusted":true},"outputs":[],"execution_count":264},{"cell_type":"code","source":"#!L\nclass GpEstimator(Estimator):\n    def __init__(self, models, n_classes, learning_rate, x_example, device, groups=1):\n        super(GpEstimator, self).__init__()\n        self.models    = [model.to(device) for model in models]\n        self.lr        = learning_rate\n        self.n_classes = n_classes\n        self.device    = device\n        \n        n = len(models)\n        X = torch.stack([x_example]).to(device)\n        \n        model = models[0].to(device)\n        readout_size = model.readout(X).size()[1]\n    \n        # TODO: Assert that models have the same readout size\n        \n        self.w      = torch.zeros([n, readout_size, n_classes]).to(device)\n        self.w_size = n * groups\n        \n    def get_w_update(self, X, right_vector):\n        with torch.no_grad():\n            assert len(X) == len(right_vector)\n\n            X            = X.to(self.device)\n            right_vector = right_vector.to(self.device)\n\n            n = len(X)\n            w_updates = []\n            \n            for model in self.models:\n                features = self.to_model_features(X, model)\n                update = torch.matmul(features.T, right_vector)\n                w_updates.append(update)\n            return torch.stack(w_updates)\n                                  \n    def to_model_features(self, X, model):\n        with torch.no_grad():\n            model = model.to(device)\n            return model.readout(X) * (1. / np.sqrt(self.w_size))\n        \n    def calc_kernel(self, X):\n        with torch.no_grad():\n            X = X.to(device)\n            \n            res = torch.zeros([len(X), len(X)]).to(device).double()\n            for model in self.models:\n                features_x = self.to_model_features(X, model)\n                \n                res += torch.matmul(features_x, features_x.T).double()\n            return res.float()\n        \n    def calc_kernel_pred(self, X):\n        with torch.no_grad():\n            X = X.to(device)\n            \n            n = len(X)\n            y_pred = torch.zeros([n, self.n_classes]).to(device).double()\n            kernel = torch.zeros([len(X), len(X)]).to(device).double()\n            \n            for model, w in zip(self.models, self.w):\n                features = self.to_model_features(X, model)\n                \n                kernel += torch.matmul(features, features.T).double()\n                y_pred += torch.matmul(features, w).double()\n            return kernel, y_pred.float()\n        \n    def calc_kernels(self, X_train, X_test):\n        with torch.no_grad():\n            X_train = X_train.to(device)\n            X_test  = X_test.to(device)\n            \n            res_train = torch.zeros([len(X_train), len(X_train)]).to(device)\n            res_test  = torch.zeros([len(X_test),  len(X_train)]).to(device)\n            for model in self.models:\n                features_train = self.to_model_features(X_train, model)\n                features_test  = self.to_model_features(X_test,  model)\n                \n                res_train += torch.matmul(features_train, features_train.T)\n                res_test  += torch.matmul(features_test,  features_train.T)\n            return res_train, res_test\n            \n    def predict(self, X, cur_w=None):\n        X = X.to(self.device)\n        if cur_w is None:\n            cur_w = self.w\n         \n        with torch.no_grad():\n            n = len(X)\n            res = torch.zeros([n, self.n_classes]).double().to(device)\n\n            for model, w in zip(self.models, cur_w):\n                features = self.to_model_features(X, model)\n                res += torch.matmul(features, w).double()\n            return res.float()","metadata":{"cellId":"g7cla86yh0ssuolnxh70mb","trusted":true},"outputs":[],"execution_count":265},{"cell_type":"code","source":"#!g1.1\ndef calc_right_vector(kernel, y, learning_rate=1e5, reg_param=0):\n    with torch.no_grad():\n        y      = y.to(device)\n        kernel = kernel.to(device)\n        \n        n      = len(kernel)\n        reg = torch.eye(n).to(device) * reg_param\n        \n        exp_term = - learning_rate * compute_exp_term(- learning_rate * (kernel + reg), device)\n        right_vector = torch.matmul(exp_term, - y)\n        return right_vector","metadata":{"cellId":"gkutaoiksnjzw3xman6dzl","trusted":true},"outputs":[],"execution_count":266},{"cell_type":"code","source":"#!g1.1\ndef boosting(estimator, train_loader, test_loader, learning_rate=1e5, beta=1., noise_rate=0., n_iter=10):\n    output_kernel = False\n    \n    with torch.no_grad():\n        batches_num = 0\n        for _ in enumerate(train_loader):\n            batches_num += 1\n        n_batches = (batches_num * 2) // 3 + 1\n            \n        for iter_num  in range(n_iter):\n            print(f\"iter {iter_num} ==========================\")\n            \n            w_update = 0\n            iter_start = time.time()\n            \n            for batch_i, (X, y) in enumerate(train_loader):\n                if batch_i >= n_batches:\n                    break\n                \n                X = X.to(device)\n                y = y.to(device)\n                \n                kernel, y_pred = estimator.calc_kernel_pred(X)\n                if output_kernel:\n                    print(f\"kernel\\n{kernel[:5,:5]}\")\n                \n                y_residual = y_pred - y\n                \n                train_acc = (y_pred.argmax(dim=1) == y.argmax(dim=1)).float().mean().item()\n                train_mse = (y_residual ** 2).mean().item()\n                print(f\"batch {batch_i}: train_acc {train_acc:.4f}, train_mse {train_mse:.6f}\")\n                \n                right_vector = calc_right_vector(kernel, y_residual, learning_rate=learning_rate)\n                \n                w_update += estimator.get_w_update(X, right_vector).double()\n                \n                pred_change = torch.matmul(kernel, right_vector.double())\n                \n\n            w_update = (w_update / n_batches).float()\n            \n            _, (X, y) = next(enumerate(train_loader))\n            y = y.to(device)\n            \n            y_pred      = estimator.predict(X)\n            pred_change = estimator.predict(X, w_update)\n            \n            y_residual = (y_pred - y)\n            \n            w_std = w_update.std()\n            noise = w_std * torch.randn(w_update.size()).to(device)\n            estimator.w -= (w_update + noise_rate * noise) * beta \n            \n            test_acc = 0\n            for i, (X_test, labels) in enumerate(test_loader):\n                y_pred = estimator.predict(X_test) \n                cur_acc   = (y_pred.argmax(dim=1) == labels.to(device)).float().mean().item()\n                test_acc += (cur_acc - test_acc) / (i + 1)\n                \n            print(f\"iter {iter_num} done. took {time.time() - iter_start:.0f}s. beta {beta:.3f}, test_acc {test_acc:.4f}\")\n            print()","metadata":{"cellId":"918cq7d315kanef6k69a","trusted":true},"outputs":[],"execution_count":289},{"cell_type":"code","source":"#!g1.1\n\nlabels_train_full.shape, labels_test_full.shape","metadata":{"cellId":"y53leak3760g9dhczwfxtt","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"(torch.Size([100000]), torch.Size([10000]))"},"metadata":{}}],"execution_count":271},{"cell_type":"code","source":"#!g1.1\n\ntrain_set = CustomTensorDataset(X_train_full, to_one_hot(labels_train_full, num_classes))\ntest_set  = CustomTensorDataset(X_test_full,  labels_test_full)\n\ntrain_loader = torch.utils.data.DataLoader(train_set, batch_size=1280 * 2)\ntest_loader  = torch.utils.data.DataLoader(test_set,  batch_size=1000)","metadata":{"cellId":"pkm823bn6h8svvkk7l4ox","trusted":true},"outputs":[],"execution_count":275},{"cell_type":"code","source":"#!g1.1\n\n%state_exclude estimator\n%state_exclude models\n\ntorch.manual_seed(0)\nnp.random.seed(0)\n\nn_models = 1024\nmodels = [Myrtle9(num_filters=1, groups=10) for _ in range(n_models)]\n\nestimator = GpEstimator(models, num_classes, 0.2, X_train[0], device, groups=50)\nboosting(estimator, train_loader, test_loader, learning_rate=1e5, n_iter=30)","metadata":{"cellId":"n8z0rm1pon9jrjc745dzs","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"iter 0 ==========================\nbatch 0: train_acc 0.0055, train_mse 1.000000\nbatch 1: train_acc 0.0066, train_mse 1.000000\nbatch 2: train_acc 0.0031, train_mse 1.000000\nbatch 3: train_acc 0.0039, train_mse 1.000000\nbatch 4: train_acc 0.0039, train_mse 1.000000\nbatch 5: train_acc 0.0055, train_mse 1.000000\nbatch 6: train_acc 0.0031, train_mse 1.000000\nbatch 7: train_acc 0.0051, train_mse 1.000000\nbatch 8: train_acc 0.0070, train_mse 1.000000\nbatch 9: train_acc 0.0043, train_mse 1.000000\nbatch 10: train_acc 0.0047, train_mse 1.000000\nbatch 11: train_acc 0.0051, train_mse 1.000000\nbatch 12: train_acc 0.0035, train_mse 1.000000\nbatch 13: train_acc 0.0035, train_mse 1.000000\nbatch 14: train_acc 0.0051, train_mse 1.000000\nbatch 15: train_acc 0.0051, train_mse 1.000000\nbatch 16: train_acc 0.0059, train_mse 1.000000\nbatch 17: train_acc 0.0074, train_mse 1.000000\nbatch 18: train_acc 0.0039, train_mse 1.000000\nbatch 19: train_acc 0.0047, train_mse 1.000000\nbatch 20: train_acc 0.0059, train_mse 1.000000\nbatch 21: train_acc 0.0051, train_mse 1.000000\nbatch 22: train_acc 0.0078, train_mse 1.000000\nbatch 23: train_acc 0.0035, train_mse 1.000000\nbatch 24: train_acc 0.0051, train_mse 1.000000\nbatch 25: train_acc 0.0039, train_mse 1.000000\nbatch 26: train_acc 0.0063, train_mse 1.000000\niter 0 done. took 3901s. beta 1.000, test_acc 0.3356\n\niter 1 ==========================\nbatch 0: train_acc 0.7070, train_mse 0.017500\nbatch 1: train_acc 0.7066, train_mse 0.017470\nbatch 2: train_acc 0.6969, train_mse 0.017507\nbatch 3: train_acc 0.7027, train_mse 0.017522\nbatch 4: train_acc 0.6852, train_mse 0.017517\nbatch 5: train_acc 0.6922, train_mse 0.017542\nbatch 6: train_acc 0.6961, train_mse 0.017494\nbatch 7: train_acc 0.7066, train_mse 0.017444\nbatch 8: train_acc 0.6910, train_mse 0.017526\nbatch 9: train_acc 0.7004, train_mse 0.017523\nbatch 10: train_acc 0.7031, train_mse 0.017467\nbatch 11: train_acc 0.7016, train_mse 0.017446\nbatch 12: train_acc 0.6953, train_mse 0.017519\nbatch 13: train_acc 0.7063, train_mse 0.017526\nbatch 14: train_acc 0.7105, train_mse 0.017501\nbatch 15: train_acc 0.7176, train_mse 0.017442\nbatch 16: train_acc 0.7188, train_mse 0.017462\nbatch 17: train_acc 0.7086, train_mse 0.017491\nbatch 18: train_acc 0.7102, train_mse 0.017487\nbatch 19: train_acc 0.7102, train_mse 0.017449\nbatch 20: train_acc 0.7004, train_mse 0.017456\nbatch 21: train_acc 0.7027, train_mse 0.017497\nbatch 22: train_acc 0.6961, train_mse 0.017507\nbatch 23: train_acc 0.7004, train_mse 0.017510\nbatch 24: train_acc 0.7090, train_mse 0.017424\nbatch 25: train_acc 0.7117, train_mse 0.017456\nbatch 26: train_acc 0.7082, train_mse 0.017469\niter 1 done. took 3900s. beta 1.000, test_acc 0.3569\n\niter 2 ==========================\nbatch 0: train_acc 0.8680, train_mse 0.016443\nbatch 1: train_acc 0.8535, train_mse 0.016411\nbatch 2: train_acc 0.8496, train_mse 0.016453\nbatch 3: train_acc 0.8637, train_mse 0.016466\nbatch 4: train_acc 0.8301, train_mse 0.016465\nbatch 5: train_acc 0.8543, train_mse 0.016491\nbatch 6: train_acc 0.8562, train_mse 0.016438\nbatch 7: train_acc 0.8586, train_mse 0.016375\nbatch 8: train_acc 0.8434, train_mse 0.016474\nbatch 9: train_acc 0.8652, train_mse 0.016467\nbatch 10: train_acc 0.8590, train_mse 0.016410\nbatch 11: train_acc 0.8594, train_mse 0.016378\nbatch 12: train_acc 0.8559, train_mse 0.016465\nbatch 13: train_acc 0.8609, train_mse 0.016477\nbatch 14: train_acc 0.8594, train_mse 0.016444\nbatch 15: train_acc 0.8672, train_mse 0.016371\nbatch 16: train_acc 0.8707, train_mse 0.016395\nbatch 17: train_acc 0.8602, train_mse 0.016431\nbatch 18: train_acc 0.8637, train_mse 0.016425\nbatch 19: train_acc 0.8613, train_mse 0.016381\nbatch 20: train_acc 0.8598, train_mse 0.016391\nbatch 21: train_acc 0.8477, train_mse 0.016438\nbatch 22: train_acc 0.8535, train_mse 0.016453\nbatch 23: train_acc 0.8598, train_mse 0.016453\nbatch 24: train_acc 0.8629, train_mse 0.016355\nbatch 25: train_acc 0.8605, train_mse 0.016388\nbatch 26: train_acc 0.8582, train_mse 0.016403\niter 2 done. took 3900s. beta 1.000, test_acc 0.3698\n\niter 3 ==========================\nbatch 0: train_acc 0.9273, train_mse 0.015633\nbatch 1: train_acc 0.9195, train_mse 0.015601\nbatch 2: train_acc 0.9223, train_mse 0.015645\nbatch 3: train_acc 0.9242, train_mse 0.015656\nbatch 4: train_acc 0.9199, train_mse 0.015656\nbatch 5: train_acc 0.9273, train_mse 0.015681\nbatch 6: train_acc 0.9270, train_mse 0.015628\nbatch 7: train_acc 0.9320, train_mse 0.015560\nbatch 8: train_acc 0.9141, train_mse 0.015663\nbatch 9: train_acc 0.9266, train_mse 0.015656\nbatch 10: train_acc 0.9309, train_mse 0.015599\nbatch 11: train_acc 0.9246, train_mse 0.015562\nbatch 12: train_acc 0.9262, train_mse 0.015655\nbatch 13: train_acc 0.9348, train_mse 0.015671\nbatch 14: train_acc 0.9297, train_mse 0.015636\nbatch 15: train_acc 0.9258, train_mse 0.015554\nbatch 16: train_acc 0.9398, train_mse 0.015580\nbatch 17: train_acc 0.9297, train_mse 0.015620\nbatch 18: train_acc 0.9332, train_mse 0.015612\nbatch 19: train_acc 0.9293, train_mse 0.015565\nbatch 20: train_acc 0.9258, train_mse 0.015579\nbatch 21: train_acc 0.9238, train_mse 0.015626\nbatch 22: train_acc 0.9258, train_mse 0.015645\nbatch 23: train_acc 0.9344, train_mse 0.015642\nbatch 24: train_acc 0.9313, train_mse 0.015541\nbatch 25: train_acc 0.9297, train_mse 0.015574\nbatch 26: train_acc 0.9336, train_mse 0.015590\niter 3 done. took 3901s. beta 1.000, test_acc 0.3761\n\niter 4 ==========================\nbatch 0: train_acc 0.9582, train_mse 0.014952\nbatch 1: train_acc 0.9516, train_mse 0.014920\nbatch 2: train_acc 0.9582, train_mse 0.014966\nbatch 3: train_acc 0.9543, train_mse 0.014974\nbatch 4: train_acc 0.9590, train_mse 0.014976\nbatch 5: train_acc 0.9625, train_mse 0.014999\nbatch 6: train_acc 0.9582, train_mse 0.014947\nbatch 7: train_acc 0.9582, train_mse 0.014878\nbatch 8: train_acc 0.9543, train_mse 0.014980\nbatch 9: train_acc 0.9594, train_mse 0.014974\nbatch 10: train_acc 0.9613, train_mse 0.014918\nbatch 11: train_acc 0.9574, train_mse 0.014878\nbatch 12: train_acc 0.9555, train_mse 0.014974\nbatch 13: train_acc 0.9594, train_mse 0.014991\nbatch 14: train_acc 0.9617, train_mse 0.014957\nbatch 15: train_acc 0.9555, train_mse 0.014868\nbatch 16: train_acc 0.9680, train_mse 0.014896\nbatch 17: train_acc 0.9609, train_mse 0.014938\nbatch 18: train_acc 0.9633, train_mse 0.014929\nbatch 19: train_acc 0.9563, train_mse 0.014882\nbatch 20: train_acc 0.9586, train_mse 0.014897\nbatch 21: train_acc 0.9547, train_mse 0.014943\nbatch 22: train_acc 0.9633, train_mse 0.014967\nbatch 23: train_acc 0.9664, train_mse 0.014961\nbatch 24: train_acc 0.9617, train_mse 0.014858\nbatch 25: train_acc 0.9582, train_mse 0.014892\nbatch 26: train_acc 0.9617, train_mse 0.014907\niter 4 done. took 3900s. beta 1.000, test_acc 0.3813\n\niter 5 ==========================\nbatch 0: train_acc 0.9766, train_mse 0.014358\nbatch 1: train_acc 0.9680, train_mse 0.014327\nbatch 2: train_acc 0.9727, train_mse 0.014374\nbatch 3: train_acc 0.9730, train_mse 0.014380\nbatch 4: train_acc 0.9750, train_mse 0.014382\nbatch 5: train_acc 0.9770, train_mse 0.014403\nbatch 6: train_acc 0.9719, train_mse 0.014354\nbatch 7: train_acc 0.9750, train_mse 0.014284\nbatch 8: train_acc 0.9746, train_mse 0.014384\nbatch 9: train_acc 0.9789, train_mse 0.014379\nbatch 10: train_acc 0.9750, train_mse 0.014324\nbatch 11: train_acc 0.9742, train_mse 0.014284\nbatch 12: train_acc 0.9734, train_mse 0.014380\nbatch 13: train_acc 0.9730, train_mse 0.014399\nbatch 14: train_acc 0.9766, train_mse 0.014366\nbatch 15: train_acc 0.9789, train_mse 0.014273\nbatch 16: train_acc 0.9809, train_mse 0.014300\nbatch 17: train_acc 0.9777, train_mse 0.014344\nbatch 18: train_acc 0.9758, train_mse 0.014334\nbatch 19: train_acc 0.9762, train_mse 0.014286\nbatch 20: train_acc 0.9727, train_mse 0.014304\nbatch 21: train_acc 0.9746, train_mse 0.014347\nbatch 22: train_acc 0.9766, train_mse 0.014376\nbatch 23: train_acc 0.9801, train_mse 0.014367\nbatch 24: train_acc 0.9750, train_mse 0.014265\nbatch 25: train_acc 0.9723, train_mse 0.014297\nbatch 26: train_acc 0.9801, train_mse 0.014314\niter 5 done. took 3900s. beta 1.000, test_acc 0.3863\n\niter 6 ==========================\nbatch 0: train_acc 0.9859, train_mse 0.013830\nbatch 1: train_acc 0.9781, train_mse 0.013799\nbatch 2: train_acc 0.9805, train_mse 0.013846\nbatch 3: train_acc 0.9812, train_mse 0.013851\nbatch 4: train_acc 0.9828, train_mse 0.013853\nbatch 5: train_acc 0.9848, train_mse 0.013873\nbatch 6: train_acc 0.9824, train_mse 0.013827\nbatch 7: train_acc 0.9859, train_mse 0.013757\nbatch 8: train_acc 0.9848, train_mse 0.013854\nbatch 9: train_acc 0.9832, train_mse 0.013849\nbatch 10: train_acc 0.9828, train_mse 0.013796\nbatch 11: train_acc 0.9812, train_mse 0.013756\nbatch 12: train_acc 0.9820, train_mse 0.013851\nbatch 13: train_acc 0.9840, train_mse 0.013871\nbatch 14: train_acc 0.9855, train_mse 0.013839\nbatch 15: train_acc 0.9883, train_mse 0.013743\nbatch 16: train_acc 0.9891, train_mse 0.013770\nbatch 17: train_acc 0.9871, train_mse 0.013816\nbatch 18: train_acc 0.9824, train_mse 0.013804\nbatch 19: train_acc 0.9844, train_mse 0.013758\nbatch 20: train_acc 0.9855, train_mse 0.013777\nbatch 21: train_acc 0.9820, train_mse 0.013818\nbatch 22: train_acc 0.9848, train_mse 0.013850\nbatch 23: train_acc 0.9871, train_mse 0.013839\nbatch 24: train_acc 0.9844, train_mse 0.013737\nbatch 25: train_acc 0.9820, train_mse 0.013769\nbatch 26: train_acc 0.9879, train_mse 0.013786\niter 6 done. took 3900s. beta 1.000, test_acc 0.3896\n\niter 7 ==========================\nbatch 0: train_acc 0.9906, train_mse 0.013352\nbatch 1: train_acc 0.9852, train_mse 0.013323\nbatch 2: train_acc 0.9875, train_mse 0.013370\nbatch 3: train_acc 0.9867, train_mse 0.013374\nbatch 4: train_acc 0.9863, train_mse 0.013376\nbatch 5: train_acc 0.9902, train_mse 0.013395\nbatch 6: train_acc 0.9895, train_mse 0.013351\nbatch 7: train_acc 0.9914, train_mse 0.013281\nbatch 8: train_acc 0.9902, train_mse 0.013376\nbatch 9: train_acc 0.9871, train_mse 0.013371\nbatch 10: train_acc 0.9898, train_mse 0.013320\nbatch 11: train_acc 0.9887, train_mse 0.013280\nbatch 12: train_acc 0.9859, train_mse 0.013373\nbatch 13: train_acc 0.9891, train_mse 0.013395\nbatch 14: train_acc 0.9891, train_mse 0.013364\nbatch 15: train_acc 0.9941, train_mse 0.013266\nbatch 16: train_acc 0.9918, train_mse 0.013292\nbatch 17: train_acc 0.9918, train_mse 0.013339\nbatch 18: train_acc 0.9879, train_mse 0.013327\nbatch 19: train_acc 0.9910, train_mse 0.013282\nbatch 20: train_acc 0.9898, train_mse 0.013302\nbatch 21: train_acc 0.9883, train_mse 0.013340\nbatch 22: train_acc 0.9898, train_mse 0.013375\nbatch 23: train_acc 0.9906, train_mse 0.013362\nbatch 24: train_acc 0.9887, train_mse 0.013261\nbatch 25: train_acc 0.9867, train_mse 0.013293\nbatch 26: train_acc 0.9918, train_mse 0.013310\niter 7 done. took 3900s. beta 1.000, test_acc 0.3916\n\niter 8 ==========================\nbatch 0: train_acc 0.9941, train_mse 0.012917\nbatch 1: train_acc 0.9914, train_mse 0.012889\nbatch 2: train_acc 0.9922, train_mse 0.012936\nbatch 3: train_acc 0.9898, train_mse 0.012938\nbatch 4: train_acc 0.9910, train_mse 0.012940\nbatch 5: train_acc 0.9930, train_mse 0.012958\nbatch 6: train_acc 0.9938, train_mse 0.012916\nbatch 7: train_acc 0.9938, train_mse 0.012848\nbatch 8: train_acc 0.9930, train_mse 0.012940\nbatch 9: train_acc 0.9898, train_mse 0.012935\nbatch 10: train_acc 0.9926, train_mse 0.012886\nbatch 11: train_acc 0.9914, train_mse 0.012847\nbatch 12: train_acc 0.9902, train_mse 0.012938\nbatch 13: train_acc 0.9898, train_mse 0.012960\nbatch 14: train_acc 0.9914, train_mse 0.012931\nbatch 15: train_acc 0.9961, train_mse 0.012831\nbatch 16: train_acc 0.9938, train_mse 0.012857\nbatch 17: train_acc 0.9945, train_mse 0.012904\nbatch 18: train_acc 0.9895, train_mse 0.012892\nbatch 19: train_acc 0.9938, train_mse 0.012848\nbatch 20: train_acc 0.9934, train_mse 0.012868\nbatch 21: train_acc 0.9910, train_mse 0.012905\nbatch 22: train_acc 0.9930, train_mse 0.012941\nbatch 23: train_acc 0.9930, train_mse 0.012928\nbatch 24: train_acc 0.9930, train_mse 0.012827\nbatch 25: train_acc 0.9914, train_mse 0.012859\nbatch 26: train_acc 0.9941, train_mse 0.012876\niter 8 done. took 3900s. beta 1.000, test_acc 0.3934\n\niter 9 ==========================\nbatch 0: train_acc 0.9957, train_mse 0.012517\nbatch 1: train_acc 0.9938, train_mse 0.012491\nbatch 2: train_acc 0.9957, train_mse 0.012537\nbatch 3: train_acc 0.9910, train_mse 0.012538\nbatch 4: train_acc 0.9926, train_mse 0.012540\nbatch 5: train_acc 0.9945, train_mse 0.012556\nbatch 6: train_acc 0.9953, train_mse 0.012518\nbatch 7: train_acc 0.9957, train_mse 0.012450\nbatch 8: train_acc 0.9949, train_mse 0.012540\nbatch 9: train_acc 0.9938, train_mse 0.012534\nbatch 10: train_acc 0.9953, train_mse 0.012487\nbatch 11: train_acc 0.9938, train_mse 0.012449\nbatch 12: train_acc 0.9926, train_mse 0.012537\nbatch 13: train_acc 0.9918, train_mse 0.012560\nbatch 14: train_acc 0.9930, train_mse 0.012533\nbatch 15: train_acc 0.9965, train_mse 0.012432\nbatch 16: train_acc 0.9957, train_mse 0.012457\nbatch 17: train_acc 0.9961, train_mse 0.012505\nbatch 18: train_acc 0.9926, train_mse 0.012492\nbatch 19: train_acc 0.9953, train_mse 0.012450\nbatch 20: train_acc 0.9957, train_mse 0.012470\nbatch 21: train_acc 0.9941, train_mse 0.012504\nbatch 22: train_acc 0.9938, train_mse 0.012543\nbatch 23: train_acc 0.9945, train_mse 0.012528\nbatch 24: train_acc 0.9953, train_mse 0.012429\nbatch 25: train_acc 0.9945, train_mse 0.012461\nbatch 26: train_acc 0.9961, train_mse 0.012477\niter 9 done. took 3901s. beta 1.000, test_acc 0.3945\n\niter 10 ==========================\nbatch 0: train_acc 0.9977, train_mse 0.012147\nbatch 1: train_acc 0.9949, train_mse 0.012122\nbatch 2: train_acc 0.9969, train_mse 0.012168\nbatch 3: train_acc 0.9930, train_mse 0.012168\nbatch 4: train_acc 0.9934, train_mse 0.012170\nbatch 5: train_acc 0.9957, train_mse 0.012185\nbatch 6: train_acc 0.9965, train_mse 0.012149\nbatch 7: train_acc 0.9980, train_mse 0.012082\nbatch 8: train_acc 0.9977, train_mse 0.012170\nbatch 9: train_acc 0.9945, train_mse 0.012163\nbatch 10: train_acc 0.9969, train_mse 0.012119\nbatch 11: train_acc 0.9961, train_mse 0.012081\nbatch 12: train_acc 0.9945, train_mse 0.012167\nbatch 13: train_acc 0.9922, train_mse 0.012190\nbatch 14: train_acc 0.9953, train_mse 0.012165\nbatch 15: train_acc 0.9980, train_mse 0.012062\nbatch 16: train_acc 0.9969, train_mse 0.012087\nbatch 17: train_acc 0.9980, train_mse 0.012136\nbatch 18: train_acc 0.9941, train_mse 0.012122\nbatch 19: train_acc 0.9965, train_mse 0.012082\nbatch 20: train_acc 0.9973, train_mse 0.012102\nbatch 21: train_acc 0.9949, train_mse 0.012134\nbatch 22: train_acc 0.9957, train_mse 0.012175\nbatch 23: train_acc 0.9957, train_mse 0.012158\nbatch 24: train_acc 0.9961, train_mse 0.012062\nbatch 25: train_acc 0.9965, train_mse 0.012093\nbatch 26: train_acc 0.9965, train_mse 0.012109\niter 10 done. took 3900s. beta 1.000, test_acc 0.3958\n\niter 11 ==========================\nbatch 0: train_acc 0.9984, train_mse 0.011803\nbatch 1: train_acc 0.9953, train_mse 0.011779\nbatch 2: train_acc 0.9969, train_mse 0.011824\nbatch 3: train_acc 0.9938, train_mse 0.011825\nbatch 4: train_acc 0.9949, train_mse 0.011825\nbatch 5: train_acc 0.9961, train_mse 0.011840\nbatch 6: train_acc 0.9977, train_mse 0.011806\nbatch 7: train_acc 0.9988, train_mse 0.011741\nbatch 8: train_acc 0.9988, train_mse 0.011826\nbatch 9: train_acc 0.9953, train_mse 0.011819\nbatch 10: train_acc 0.9984, train_mse 0.011776\nbatch 11: train_acc 0.9980, train_mse 0.011740\nbatch 12: train_acc 0.9961, train_mse 0.011823\nbatch 13: train_acc 0.9922, train_mse 0.011846\nbatch 14: train_acc 0.9957, train_mse 0.011822\nbatch 15: train_acc 0.9984, train_mse 0.011719\nbatch 16: train_acc 0.9973, train_mse 0.011743\nbatch 17: train_acc 0.9992, train_mse 0.011793\nbatch 18: train_acc 0.9965, train_mse 0.011779\nbatch 19: train_acc 0.9969, train_mse 0.011740\nbatch 20: train_acc 0.9977, train_mse 0.011760\nbatch 21: train_acc 0.9961, train_mse 0.011790\nbatch 22: train_acc 0.9961, train_mse 0.011832\nbatch 23: train_acc 0.9965, train_mse 0.011815\nbatch 24: train_acc 0.9965, train_mse 0.011720\nbatch 25: train_acc 0.9977, train_mse 0.011751\nbatch 26: train_acc 0.9969, train_mse 0.011766\niter 11 done. took 3900s. beta 1.000, test_acc 0.3956\n\niter 12 ==========================\nbatch 0: train_acc 0.9988, train_mse 0.011482\nbatch 1: train_acc 0.9965, train_mse 0.011460\nbatch 2: train_acc 0.9980, train_mse 0.011504\nbatch 3: train_acc 0.9953, train_mse 0.011504\nbatch 4: train_acc 0.9961, train_mse 0.011504\nbatch 5: train_acc 0.9969, train_mse 0.011518\nbatch 6: train_acc 0.9980, train_mse 0.011486\nbatch 7: train_acc 0.9988, train_mse 0.011422\nbatch 8: train_acc 1.0000, train_mse 0.011505\nbatch 9: train_acc 0.9965, train_mse 0.011497\nbatch 10: train_acc 0.9992, train_mse 0.011457\nbatch 11: train_acc 0.9984, train_mse 0.011420\nbatch 12: train_acc 0.9969, train_mse 0.011502\nbatch 13: train_acc 0.9934, train_mse 0.011524\nbatch 14: train_acc 0.9965, train_mse 0.011502\nbatch 15: train_acc 0.9992, train_mse 0.011399\nbatch 16: train_acc 0.9977, train_mse 0.011423\nbatch 17: train_acc 0.9992, train_mse 0.011473\nbatch 18: train_acc 0.9969, train_mse 0.011459\nbatch 19: train_acc 0.9973, train_mse 0.011421\nbatch 20: train_acc 0.9980, train_mse 0.011440\nbatch 21: train_acc 0.9969, train_mse 0.011470\nbatch 22: train_acc 0.9977, train_mse 0.011512\nbatch 23: train_acc 0.9973, train_mse 0.011495\nbatch 24: train_acc 0.9969, train_mse 0.011401\nbatch 25: train_acc 0.9984, train_mse 0.011432\nbatch 26: train_acc 0.9969, train_mse 0.011447\niter 12 done. took 3900s. beta 1.000, test_acc 0.3965\n\niter 13 ==========================\nbatch 0: train_acc 0.9992, train_mse 0.011181\nbatch 1: train_acc 0.9969, train_mse 0.011160\nbatch 2: train_acc 0.9984, train_mse 0.011204\nbatch 3: train_acc 0.9969, train_mse 0.011203\nbatch 4: train_acc 0.9961, train_mse 0.011203\nbatch 5: train_acc 0.9977, train_mse 0.011216\nbatch 6: train_acc 0.9992, train_mse 0.011187\nbatch 7: train_acc 0.9988, train_mse 0.011124\nbatch 8: train_acc 1.0000, train_mse 0.011205\nbatch 9: train_acc 0.9980, train_mse 0.011195\nbatch 10: train_acc 0.9996, train_mse 0.011157\nbatch 11: train_acc 0.9988, train_mse 0.011122\nbatch 12: train_acc 0.9977, train_mse 0.011201\nbatch 13: train_acc 0.9957, train_mse 0.011223\nbatch 14: train_acc 0.9969, train_mse 0.011203\nbatch 15: train_acc 0.9996, train_mse 0.011100\nbatch 16: train_acc 0.9977, train_mse 0.011122\nbatch 17: train_acc 0.9996, train_mse 0.011172\nbatch 18: train_acc 0.9973, train_mse 0.011158\nbatch 19: train_acc 0.9980, train_mse 0.011122\nbatch 20: train_acc 0.9984, train_mse 0.011141\nbatch 21: train_acc 0.9980, train_mse 0.011169\nbatch 22: train_acc 0.9977, train_mse 0.011212\nbatch 23: train_acc 0.9980, train_mse 0.011194\nbatch 24: train_acc 0.9969, train_mse 0.011103\nbatch 25: train_acc 0.9984, train_mse 0.011133\nbatch 26: train_acc 0.9984, train_mse 0.011147\niter 13 done. took 3900s. beta 1.000, test_acc 0.3975\n\niter 14 ==========================\nbatch 0: train_acc 0.9992, train_mse 0.010898\nbatch 1: train_acc 0.9977, train_mse 0.010879\nbatch 2: train_acc 0.9988, train_mse 0.010921\nbatch 3: train_acc 0.9984, train_mse 0.010920\nbatch 4: train_acc 0.9969, train_mse 0.010920\nbatch 5: train_acc 0.9984, train_mse 0.010932\nbatch 6: train_acc 0.9992, train_mse 0.010905\nbatch 7: train_acc 0.9988, train_mse 0.010843\nbatch 8: train_acc 1.0000, train_mse 0.010922\nbatch 9: train_acc 0.9984, train_mse 0.010912\nbatch 10: train_acc 0.9996, train_mse 0.010876\nbatch 11: train_acc 0.9988, train_mse 0.010841\nbatch 12: train_acc 0.9984, train_mse 0.010918\nbatch 13: train_acc 0.9961, train_mse 0.010940\nbatch 14: train_acc 0.9973, train_mse 0.010921\nbatch 15: train_acc 1.0000, train_mse 0.010818\nbatch 16: train_acc 0.9977, train_mse 0.010840\nbatch 17: train_acc 1.0000, train_mse 0.010890\nbatch 18: train_acc 0.9980, train_mse 0.010876\nbatch 19: train_acc 0.9980, train_mse 0.010842\nbatch 20: train_acc 0.9988, train_mse 0.010860\nbatch 21: train_acc 0.9980, train_mse 0.010886\nbatch 22: train_acc 0.9984, train_mse 0.010931\nbatch 23: train_acc 0.9980, train_mse 0.010912\nbatch 24: train_acc 0.9977, train_mse 0.010822\nbatch 25: train_acc 0.9984, train_mse 0.010852\nbatch 26: train_acc 0.9984, train_mse 0.010866\niter 14 done. took 3901s. beta 1.000, test_acc 0.3981\n\niter 15 ==========================\nbatch 0: train_acc 0.9996, train_mse 0.010631\nbatch 1: train_acc 0.9977, train_mse 0.010614\nbatch 2: train_acc 0.9988, train_mse 0.010655\nbatch 3: train_acc 0.9984, train_mse 0.010654\nbatch 4: train_acc 0.9973, train_mse 0.010653\nbatch 5: train_acc 0.9984, train_mse 0.010664\nbatch 6: train_acc 0.9992, train_mse 0.010639\nbatch 7: train_acc 0.9992, train_mse 0.010579\nbatch 8: train_acc 1.0000, train_mse 0.010656\nbatch 9: train_acc 0.9988, train_mse 0.010645\nbatch 10: train_acc 1.0000, train_mse 0.010611\nbatch 11: train_acc 0.9992, train_mse 0.010577\nbatch 12: train_acc 0.9988, train_mse 0.010651\nbatch 13: train_acc 0.9973, train_mse 0.010673\nbatch 14: train_acc 0.9977, train_mse 0.010656\nbatch 15: train_acc 1.0000, train_mse 0.010553\nbatch 16: train_acc 0.9977, train_mse 0.010574\nbatch 17: train_acc 1.0000, train_mse 0.010625\nbatch 18: train_acc 0.9980, train_mse 0.010610\nbatch 19: train_acc 0.9980, train_mse 0.010577\nbatch 20: train_acc 0.9992, train_mse 0.010596\nbatch 21: train_acc 0.9984, train_mse 0.010620\nbatch 22: train_acc 0.9984, train_mse 0.010665\nbatch 23: train_acc 0.9988, train_mse 0.010646\nbatch 24: train_acc 0.9980, train_mse 0.010557\nbatch 25: train_acc 0.9984, train_mse 0.010587\nbatch 26: train_acc 0.9984, train_mse 0.010600\niter 15 done. took 3901s. beta 1.000, test_acc 0.3977\n\niter 16 ==========================\nbatch 0: train_acc 0.9996, train_mse 0.010379\nbatch 1: train_acc 0.9984, train_mse 0.010363\nbatch 2: train_acc 0.9996, train_mse 0.010403\nbatch 3: train_acc 0.9984, train_mse 0.010402\nbatch 4: train_acc 0.9980, train_mse 0.010401\nbatch 5: train_acc 0.9984, train_mse 0.010411\nbatch 6: train_acc 0.9996, train_mse 0.010388\nbatch 7: train_acc 0.9992, train_mse 0.010329\nbatch 8: train_acc 1.0000, train_mse 0.010405\nbatch 9: train_acc 0.9988, train_mse 0.010393\nbatch 10: train_acc 1.0000, train_mse 0.010361\nbatch 11: train_acc 0.9992, train_mse 0.010327\nbatch 12: train_acc 0.9992, train_mse 0.010399\nbatch 13: train_acc 0.9980, train_mse 0.010420\nbatch 14: train_acc 0.9980, train_mse 0.010405\nbatch 15: train_acc 1.0000, train_mse 0.010302\nbatch 16: train_acc 0.9980, train_mse 0.010323\nbatch 17: train_acc 1.0000, train_mse 0.010373\nbatch 18: train_acc 0.9980, train_mse 0.010359\nbatch 19: train_acc 0.9984, train_mse 0.010328\nbatch 20: train_acc 0.9992, train_mse 0.010345\nbatch 21: train_acc 0.9984, train_mse 0.010369\nbatch 22: train_acc 0.9984, train_mse 0.010413\nbatch 23: train_acc 0.9996, train_mse 0.010394\nbatch 24: train_acc 0.9992, train_mse 0.010307\nbatch 25: train_acc 0.9988, train_mse 0.010337\nbatch 26: train_acc 0.9984, train_mse 0.010350\niter 16 done. took 3900s. beta 1.000, test_acc 0.3986\n\niter 17 ==========================\nbatch 0: train_acc 0.9996, train_mse 0.010140\nbatch 1: train_acc 0.9992, train_mse 0.010125\nbatch 2: train_acc 0.9996, train_mse 0.010165\nbatch 3: train_acc 0.9984, train_mse 0.010163\nbatch 4: train_acc 0.9984, train_mse 0.010162\nbatch 5: train_acc 0.9984, train_mse 0.010172\nbatch 6: train_acc 0.9996, train_mse 0.010150\nbatch 7: train_acc 0.9996, train_mse 0.010093\nbatch 8: train_acc 1.0000, train_mse 0.010166\nbatch 9: train_acc 0.9992, train_mse 0.010154\nbatch 10: train_acc 1.0000, train_mse 0.010124\nbatch 11: train_acc 0.9996, train_mse 0.010090\nbatch 12: train_acc 1.0000, train_mse 0.010160\nbatch 13: train_acc 0.9980, train_mse 0.010181\nbatch 14: train_acc 0.9980, train_mse 0.010167\nbatch 15: train_acc 1.0000, train_mse 0.010064\nbatch 16: train_acc 0.9980, train_mse 0.010085\nbatch 17: train_acc 1.0000, train_mse 0.010135\nbatch 18: train_acc 0.9980, train_mse 0.010121\nbatch 19: train_acc 0.9988, train_mse 0.010091\nbatch 20: train_acc 0.9996, train_mse 0.010108\nbatch 21: train_acc 0.9984, train_mse 0.010130\nbatch 22: train_acc 0.9984, train_mse 0.010175\nbatch 23: train_acc 0.9996, train_mse 0.010156\nbatch 24: train_acc 0.9996, train_mse 0.010071\nbatch 25: train_acc 0.9992, train_mse 0.010100\nbatch 26: train_acc 0.9988, train_mse 0.010112\niter 17 done. took 3900s. beta 1.000, test_acc 0.3992\n\niter 18 ==========================\nbatch 0: train_acc 0.9996, train_mse 0.009914\nbatch 1: train_acc 0.9992, train_mse 0.009900\nbatch 2: train_acc 1.0000, train_mse 0.009939\nbatch 3: train_acc 0.9988, train_mse 0.009937\nbatch 4: train_acc 0.9992, train_mse 0.009936\nbatch 5: train_acc 0.9988, train_mse 0.009944\nbatch 6: train_acc 0.9996, train_mse 0.009924\nbatch 7: train_acc 0.9996, train_mse 0.009868\nbatch 8: train_acc 1.0000, train_mse 0.009940\nbatch 9: train_acc 0.9992, train_mse 0.009927\nbatch 10: train_acc 1.0000, train_mse 0.009899\nbatch 11: train_acc 0.9996, train_mse 0.009865\nbatch 12: train_acc 1.0000, train_mse 0.009934\nbatch 13: train_acc 0.9980, train_mse 0.009954\nbatch 14: train_acc 0.9980, train_mse 0.009941\nbatch 15: train_acc 1.0000, train_mse 0.009839\nbatch 16: train_acc 0.9988, train_mse 0.009859\nbatch 17: train_acc 1.0000, train_mse 0.009910\nbatch 18: train_acc 0.9984, train_mse 0.009896\nbatch 19: train_acc 0.9992, train_mse 0.009867\nbatch 20: train_acc 0.9996, train_mse 0.009883\nbatch 21: train_acc 0.9992, train_mse 0.009904\nbatch 22: train_acc 0.9988, train_mse 0.009949\nbatch 23: train_acc 0.9996, train_mse 0.009930\nbatch 24: train_acc 0.9996, train_mse 0.009846\nbatch 25: train_acc 0.9992, train_mse 0.009876\nbatch 26: train_acc 0.9988, train_mse 0.009886\niter 18 done. took 3900s. beta 1.000, test_acc 0.4003\n\niter 19 ==========================\nbatch 0: train_acc 1.0000, train_mse 0.009698\nbatch 1: train_acc 0.9992, train_mse 0.009686\nbatch 2: train_acc 1.0000, train_mse 0.009723\nbatch 3: train_acc 0.9988, train_mse 0.009721\nbatch 4: train_acc 0.9992, train_mse 0.009720\nbatch 5: train_acc 0.9988, train_mse 0.009728\nbatch 6: train_acc 0.9996, train_mse 0.009709\nbatch 7: train_acc 0.9996, train_mse 0.009655\nbatch 8: train_acc 1.0000, train_mse 0.009725\nbatch 9: train_acc 0.9992, train_mse 0.009711\nbatch 10: train_acc 1.0000, train_mse 0.009684\nbatch 11: train_acc 0.9996, train_mse 0.009651\nbatch 12: train_acc 1.0000, train_mse 0.009718\nbatch 13: train_acc 0.9988, train_mse 0.009738\nbatch 14: train_acc 0.9984, train_mse 0.009726\nbatch 15: train_acc 1.0000, train_mse 0.009625\nbatch 16: train_acc 0.9988, train_mse 0.009644\nbatch 17: train_acc 1.0000, train_mse 0.009695\nbatch 18: train_acc 0.9984, train_mse 0.009681\nbatch 19: train_acc 0.9992, train_mse 0.009653\nbatch 20: train_acc 1.0000, train_mse 0.009669\nbatch 21: train_acc 0.9992, train_mse 0.009689\nbatch 22: train_acc 0.9992, train_mse 0.009734\nbatch 23: train_acc 0.9996, train_mse 0.009715\nbatch 24: train_acc 0.9996, train_mse 0.009632\nbatch 25: train_acc 0.9992, train_mse 0.009662\nbatch 26: train_acc 0.9988, train_mse 0.009672\niter 19 done. took 3900s. beta 1.000, test_acc 0.4008\n\niter 20 ==========================\nbatch 0: train_acc 1.0000, train_mse 0.009493\nbatch 1: train_acc 0.9992, train_mse 0.009481\nbatch 2: train_acc 1.0000, train_mse 0.009519\nbatch 3: train_acc 0.9988, train_mse 0.009516\nbatch 4: train_acc 0.9992, train_mse 0.009515\nbatch 5: train_acc 0.9988, train_mse 0.009522\nbatch 6: train_acc 0.9996, train_mse 0.009504\nbatch 7: train_acc 0.9996, train_mse 0.009452\nbatch 8: train_acc 1.0000, train_mse 0.009520\nbatch 9: train_acc 0.9992, train_mse 0.009506\nbatch 10: train_acc 1.0000, train_mse 0.009481\nbatch 11: train_acc 1.0000, train_mse 0.009448\nbatch 12: train_acc 1.0000, train_mse 0.009513\nbatch 13: train_acc 0.9988, train_mse 0.009532\nbatch 14: train_acc 0.9984, train_mse 0.009522\nbatch 15: train_acc 1.0000, train_mse 0.009420\nbatch 16: train_acc 0.9992, train_mse 0.009439\nbatch 17: train_acc 1.0000, train_mse 0.009490\nbatch 18: train_acc 0.9988, train_mse 0.009476\nbatch 19: train_acc 0.9992, train_mse 0.009450\nbatch 20: train_acc 1.0000, train_mse 0.009465\nbatch 21: train_acc 0.9996, train_mse 0.009484\nbatch 22: train_acc 0.9992, train_mse 0.009530\nbatch 23: train_acc 0.9996, train_mse 0.009510\nbatch 24: train_acc 0.9996, train_mse 0.009429\nbatch 25: train_acc 0.9992, train_mse 0.009458\nbatch 26: train_acc 0.9988, train_mse 0.009468\niter 20 done. took 3901s. beta 1.000, test_acc 0.4003\n\niter 21 ==========================\nbatch 0: train_acc 1.0000, train_mse 0.009297\nbatch 1: train_acc 0.9992, train_mse 0.009287\nbatch 2: train_acc 1.0000, train_mse 0.009323\nbatch 3: train_acc 0.9992, train_mse 0.009320\nbatch 4: train_acc 0.9992, train_mse 0.009319\nbatch 5: train_acc 0.9988, train_mse 0.009325\nbatch 6: train_acc 1.0000, train_mse 0.009309\nbatch 7: train_acc 0.9996, train_mse 0.009258\nbatch 8: train_acc 1.0000, train_mse 0.009325\nbatch 9: train_acc 0.9992, train_mse 0.009309\nbatch 10: train_acc 1.0000, train_mse 0.009286\nbatch 11: train_acc 1.0000, train_mse 0.009254\nbatch 12: train_acc 1.0000, train_mse 0.009317\nbatch 13: train_acc 0.9988, train_mse 0.009336\nbatch 14: train_acc 0.9984, train_mse 0.009327\nbatch 15: train_acc 1.0000, train_mse 0.009226\nbatch 16: train_acc 0.9992, train_mse 0.009244\nbatch 17: train_acc 1.0000, train_mse 0.009295\nbatch 18: train_acc 0.9988, train_mse 0.009282\nbatch 19: train_acc 1.0000, train_mse 0.009256\nbatch 20: train_acc 1.0000, train_mse 0.009271\nbatch 21: train_acc 1.0000, train_mse 0.009289\nbatch 22: train_acc 0.9992, train_mse 0.009334\nbatch 23: train_acc 0.9996, train_mse 0.009314\nbatch 24: train_acc 0.9996, train_mse 0.009235\nbatch 25: train_acc 0.9992, train_mse 0.009264\nbatch 26: train_acc 0.9988, train_mse 0.009273\niter 21 done. took 3901s. beta 1.000, test_acc 0.3999\n\niter 22 ==========================\nbatch 0: train_acc 1.0000, train_mse 0.009110\nbatch 1: train_acc 0.9992, train_mse 0.009101\nbatch 2: train_acc 1.0000, train_mse 0.009136\nbatch 3: train_acc 1.0000, train_mse 0.009133\nbatch 4: train_acc 0.9996, train_mse 0.009131\nbatch 5: train_acc 0.9988, train_mse 0.009137\nbatch 6: train_acc 1.0000, train_mse 0.009123\nbatch 7: train_acc 0.9996, train_mse 0.009072\nbatch 8: train_acc 1.0000, train_mse 0.009138\nbatch 9: train_acc 0.9992, train_mse 0.009122\nbatch 10: train_acc 1.0000, train_mse 0.009100\nbatch 11: train_acc 1.0000, train_mse 0.009068\nbatch 12: train_acc 1.0000, train_mse 0.009130\nbatch 13: train_acc 0.9988, train_mse 0.009148\nbatch 14: train_acc 0.9984, train_mse 0.009140\nbatch 15: train_acc 1.0000, train_mse 0.009040\nbatch 16: train_acc 0.9992, train_mse 0.009057\nbatch 17: train_acc 1.0000, train_mse 0.009108\nbatch 18: train_acc 0.9988, train_mse 0.009095\nbatch 19: train_acc 1.0000, train_mse 0.009071\nbatch 20: train_acc 1.0000, train_mse 0.009085\nbatch 21: train_acc 1.0000, train_mse 0.009103\nbatch 22: train_acc 0.9992, train_mse 0.009147\nbatch 23: train_acc 0.9996, train_mse 0.009128\nbatch 24: train_acc 0.9996, train_mse 0.009050\nbatch 25: train_acc 0.9992, train_mse 0.009078\nbatch 26: train_acc 0.9988, train_mse 0.009086\niter 22 done. took 3901s. beta 1.000, test_acc 0.3999\n\niter 23 ==========================\nbatch 0: train_acc 1.0000, train_mse 0.008930\nbatch 1: train_acc 0.9992, train_mse 0.008923\nbatch 2: train_acc 1.0000, train_mse 0.008957\nbatch 3: train_acc 1.0000, train_mse 0.008954\nbatch 4: train_acc 0.9996, train_mse 0.008952\nbatch 5: train_acc 0.9988, train_mse 0.008957\nbatch 6: train_acc 1.0000, train_mse 0.008944\nbatch 7: train_acc 0.9996, train_mse 0.008895\nbatch 8: train_acc 1.0000, train_mse 0.008960\nbatch 9: train_acc 0.9992, train_mse 0.008943\nbatch 10: train_acc 1.0000, train_mse 0.008923\nbatch 11: train_acc 1.0000, train_mse 0.008890\nbatch 12: train_acc 1.0000, train_mse 0.008950\nbatch 13: train_acc 0.9988, train_mse 0.008968\nbatch 14: train_acc 0.9992, train_mse 0.008961\nbatch 15: train_acc 1.0000, train_mse 0.008862\nbatch 16: train_acc 0.9996, train_mse 0.008879\nbatch 17: train_acc 1.0000, train_mse 0.008930\nbatch 18: train_acc 0.9988, train_mse 0.008917\nbatch 19: train_acc 1.0000, train_mse 0.008893\nbatch 20: train_acc 1.0000, train_mse 0.008907\nbatch 21: train_acc 1.0000, train_mse 0.008924\nbatch 22: train_acc 0.9996, train_mse 0.008968\nbatch 23: train_acc 0.9996, train_mse 0.008949\nbatch 24: train_acc 0.9996, train_mse 0.008872\nbatch 25: train_acc 0.9992, train_mse 0.008901\nbatch 26: train_acc 0.9988, train_mse 0.008908\niter 23 done. took 3901s. beta 1.000, test_acc 0.3990\n\niter 24 ==========================\nbatch 0: train_acc 1.0000, train_mse 0.008758\nbatch 1: train_acc 0.9992, train_mse 0.008752\nbatch 2: train_acc 1.0000, train_mse 0.008785\nbatch 3: train_acc 1.0000, train_mse 0.008782\nbatch 4: train_acc 0.9996, train_mse 0.008781\nbatch 5: train_acc 0.9992, train_mse 0.008785\nbatch 6: train_acc 1.0000, train_mse 0.008773\nbatch 7: train_acc 0.9996, train_mse 0.008725\nbatch 8: train_acc 1.0000, train_mse 0.008789\nbatch 9: train_acc 0.9992, train_mse 0.008771\nbatch 10: train_acc 1.0000, train_mse 0.008752\nbatch 11: train_acc 1.0000, train_mse 0.008720\nbatch 12: train_acc 1.0000, train_mse 0.008779\nbatch 13: train_acc 0.9992, train_mse 0.008796\nbatch 14: train_acc 0.9992, train_mse 0.008790\nbatch 15: train_acc 1.0000, train_mse 0.008691\nbatch 16: train_acc 0.9996, train_mse 0.008708\nbatch 17: train_acc 1.0000, train_mse 0.008758\nbatch 18: train_acc 0.9988, train_mse 0.008746\nbatch 19: train_acc 1.0000, train_mse 0.008724\nbatch 20: train_acc 1.0000, train_mse 0.008737\nbatch 21: train_acc 1.0000, train_mse 0.008753\nbatch 22: train_acc 0.9996, train_mse 0.008797\nbatch 23: train_acc 0.9996, train_mse 0.008778\nbatch 24: train_acc 0.9996, train_mse 0.008702\nbatch 25: train_acc 0.9992, train_mse 0.008731\nbatch 26: train_acc 0.9992, train_mse 0.008737\niter 24 done. took 3901s. beta 1.000, test_acc 0.3990\n\niter 25 ==========================\nbatch 0: train_acc 1.0000, train_mse 0.008594\nbatch 1: train_acc 0.9996, train_mse 0.008588\nbatch 2: train_acc 1.0000, train_mse 0.008621\nbatch 3: train_acc 1.0000, train_mse 0.008617\nbatch 4: train_acc 0.9996, train_mse 0.008616\nbatch 5: train_acc 0.9992, train_mse 0.008620\nbatch 6: train_acc 1.0000, train_mse 0.008609\nbatch 7: train_acc 0.9996, train_mse 0.008562\nbatch 8: train_acc 1.0000, train_mse 0.008624\nbatch 9: train_acc 0.9992, train_mse 0.008606\nbatch 10: train_acc 1.0000, train_mse 0.008589\nbatch 11: train_acc 1.0000, train_mse 0.008557\nbatch 12: train_acc 1.0000, train_mse 0.008614\nbatch 13: train_acc 0.9992, train_mse 0.008631\nbatch 14: train_acc 0.9992, train_mse 0.008626\nbatch 15: train_acc 1.0000, train_mse 0.008527\nbatch 16: train_acc 0.9996, train_mse 0.008544\nbatch 17: train_acc 1.0000, train_mse 0.008594\nbatch 18: train_acc 0.9988, train_mse 0.008582\nbatch 19: train_acc 1.0000, train_mse 0.008561\nbatch 20: train_acc 1.0000, train_mse 0.008574\nbatch 21: train_acc 1.0000, train_mse 0.008589\nbatch 22: train_acc 0.9996, train_mse 0.008633\nbatch 23: train_acc 0.9996, train_mse 0.008613\nbatch 24: train_acc 0.9996, train_mse 0.008539\nbatch 25: train_acc 0.9992, train_mse 0.008568\nbatch 26: train_acc 0.9992, train_mse 0.008573\niter 25 done. took 3901s. beta 1.000, test_acc 0.3994\n\niter 26 ==========================\nbatch 0: train_acc 1.0000, train_mse 0.008436\nbatch 1: train_acc 0.9996, train_mse 0.008431\nbatch 2: train_acc 1.0000, train_mse 0.008463\nbatch 3: train_acc 1.0000, train_mse 0.008459\nbatch 4: train_acc 1.0000, train_mse 0.008458\nbatch 5: train_acc 0.9992, train_mse 0.008461\nbatch 6: train_acc 1.0000, train_mse 0.008451\nbatch 7: train_acc 0.9996, train_mse 0.008406\nbatch 8: train_acc 1.0000, train_mse 0.008467\nbatch 9: train_acc 0.9992, train_mse 0.008448\nbatch 10: train_acc 1.0000, train_mse 0.008432\nbatch 11: train_acc 1.0000, train_mse 0.008400\nbatch 12: train_acc 1.0000, train_mse 0.008456\nbatch 13: train_acc 0.9992, train_mse 0.008472\nbatch 14: train_acc 0.9992, train_mse 0.008469\nbatch 15: train_acc 1.0000, train_mse 0.008371\nbatch 16: train_acc 0.9996, train_mse 0.008387\nbatch 17: train_acc 1.0000, train_mse 0.008437\nbatch 18: train_acc 0.9992, train_mse 0.008425\nbatch 19: train_acc 1.0000, train_mse 0.008404\nbatch 20: train_acc 1.0000, train_mse 0.008417\nbatch 21: train_acc 1.0000, train_mse 0.008432\nbatch 22: train_acc 0.9996, train_mse 0.008475\nbatch 23: train_acc 0.9996, train_mse 0.008456\nbatch 24: train_acc 0.9996, train_mse 0.008382\nbatch 25: train_acc 0.9992, train_mse 0.008411\nbatch 26: train_acc 0.9992, train_mse 0.008416\niter 26 done. took 3901s. beta 1.000, test_acc 0.3999\n\niter 27 ==========================\nbatch 0: train_acc 1.0000, train_mse 0.008284\nbatch 1: train_acc 1.0000, train_mse 0.008280\nbatch 2: train_acc 1.0000, train_mse 0.008311\nbatch 3: train_acc 1.0000, train_mse 0.008307\nbatch 4: train_acc 1.0000, train_mse 0.008306\nbatch 5: train_acc 0.9992, train_mse 0.008308\nbatch 6: train_acc 1.0000, train_mse 0.008300\nbatch 7: train_acc 1.0000, train_mse 0.008256\nbatch 8: train_acc 1.0000, train_mse 0.008316\nbatch 9: train_acc 0.9992, train_mse 0.008296\nbatch 10: train_acc 1.0000, train_mse 0.008282\nbatch 11: train_acc 1.0000, train_mse 0.008250\nbatch 12: train_acc 1.0000, train_mse 0.008304\nbatch 13: train_acc 0.9992, train_mse 0.008320\nbatch 14: train_acc 0.9996, train_mse 0.008317\nbatch 15: train_acc 1.0000, train_mse 0.008219\nbatch 16: train_acc 0.9996, train_mse 0.008235\nbatch 17: train_acc 1.0000, train_mse 0.008285\nbatch 18: train_acc 0.9992, train_mse 0.008274\nbatch 19: train_acc 1.0000, train_mse 0.008254\nbatch 20: train_acc 1.0000, train_mse 0.008266\nbatch 21: train_acc 1.0000, train_mse 0.008280\nbatch 22: train_acc 0.9996, train_mse 0.008323\nbatch 23: train_acc 1.0000, train_mse 0.008304\nbatch 24: train_acc 0.9996, train_mse 0.008232\nbatch 25: train_acc 0.9992, train_mse 0.008260\nbatch 26: train_acc 0.9996, train_mse 0.008265\niter 27 done. took 3901s. beta 1.000, test_acc 0.3991\n\niter 28 ==========================\nbatch 0: train_acc 1.0000, train_mse 0.008138\nbatch 1: train_acc 1.0000, train_mse 0.008135\nbatch 2: train_acc 1.0000, train_mse 0.008164\nbatch 3: train_acc 1.0000, train_mse 0.008161\nbatch 4: train_acc 1.0000, train_mse 0.008160\nbatch 5: train_acc 0.9992, train_mse 0.008162\nbatch 6: train_acc 1.0000, train_mse 0.008154\nbatch 7: train_acc 1.0000, train_mse 0.008111\nbatch 8: train_acc 1.0000, train_mse 0.008170\nbatch 9: train_acc 0.9992, train_mse 0.008150\nbatch 10: train_acc 1.0000, train_mse 0.008137\nbatch 11: train_acc 1.0000, train_mse 0.008105\nbatch 12: train_acc 1.0000, train_mse 0.008158\nbatch 13: train_acc 0.9996, train_mse 0.008173\nbatch 14: train_acc 0.9996, train_mse 0.008171\nbatch 15: train_acc 1.0000, train_mse 0.008074\nbatch 16: train_acc 0.9996, train_mse 0.008090\nbatch 17: train_acc 1.0000, train_mse 0.008140\nbatch 18: train_acc 0.9992, train_mse 0.008128\nbatch 19: train_acc 1.0000, train_mse 0.008109\nbatch 20: train_acc 1.0000, train_mse 0.008121\nbatch 21: train_acc 1.0000, train_mse 0.008135\nbatch 22: train_acc 0.9996, train_mse 0.008177\nbatch 23: train_acc 1.0000, train_mse 0.008158\nbatch 24: train_acc 0.9996, train_mse 0.008087\nbatch 25: train_acc 0.9992, train_mse 0.008116\nbatch 26: train_acc 1.0000, train_mse 0.008119\niter 28 done. took 3901s. beta 1.000, test_acc 0.3987\n\niter 29 ==========================\nbatch 0: train_acc 1.0000, train_mse 0.007997\nbatch 1: train_acc 1.0000, train_mse 0.007995\nbatch 2: train_acc 1.0000, train_mse 0.008024\nbatch 3: train_acc 1.0000, train_mse 0.008020\nbatch 4: train_acc 1.0000, train_mse 0.008019\nbatch 5: train_acc 0.9992, train_mse 0.008020\nbatch 6: train_acc 1.0000, train_mse 0.008013\nbatch 7: train_acc 1.0000, train_mse 0.007972\nbatch 8: train_acc 1.0000, train_mse 0.008030\nbatch 9: train_acc 0.9992, train_mse 0.008009\nbatch 10: train_acc 1.0000, train_mse 0.007997\nbatch 11: train_acc 1.0000, train_mse 0.007965\nbatch 12: train_acc 1.0000, train_mse 0.008017\nbatch 13: train_acc 1.0000, train_mse 0.008032\nbatch 14: train_acc 0.9996, train_mse 0.008031\nbatch 15: train_acc 1.0000, train_mse 0.007934\nbatch 16: train_acc 0.9996, train_mse 0.007949\nbatch 17: train_acc 1.0000, train_mse 0.007999\nbatch 18: train_acc 0.9996, train_mse 0.007988\nbatch 19: train_acc 1.0000, train_mse 0.007970\nbatch 20: train_acc 1.0000, train_mse 0.007982\nbatch 21: train_acc 1.0000, train_mse 0.007994\nbatch 22: train_acc 0.9996, train_mse 0.008036\nbatch 23: train_acc 1.0000, train_mse 0.008018\nbatch 24: train_acc 0.9996, train_mse 0.007948\nbatch 25: train_acc 0.9996, train_mse 0.007976\nbatch 26: train_acc 1.0000, train_mse 0.007979\niter 29 done. took 3901s. beta 1.000, test_acc 0.3982\n\n"}],"execution_count":290},{"cell_type":"code","source":"","metadata":{"cellId":"g4wyu69xbpw7sv88lj8kyt"},"outputs":[],"execution_count":null}]}