{"nbformat":4,"nbformat_minor":5,"metadata":{"notebookId":"85f64178-6555-4004-96b6-e14c76063ddb","language_info":{"version":"3.7.7","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Yandex DataSphere Kernel","language":"python"}},"cells":[{"cell_type":"code","source":"#!g1.1\nimport time\nimport numpy as np\nimport seaborn as sns\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport random\nimport ipyplot\nfrom sklearn.utils.extmath import randomized_svd\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms, datasets\nfrom torch import utils\n\nfrom linearized_nns.estimator import Estimator\nfrom linearized_nns.pytorch_impl.estimators import SgdEstimator\nfrom linearized_nns.pytorch_impl.nns import Myrtle5, Myrtle7, Myrtle10\nfrom linearized_nns.pytorch_impl import ClassifierTraining\nfrom linearized_nns.pytorch_impl.matrix_exp import matrix_exp, compute_exp_term\nfrom linearized_nns.pytorch_impl.nns.utils import to_one_hot, print_sizes\nfrom linearized_nns.from_neural_kernels import to_zca, CustomTensorDataset, get_cifar_zca","metadata":{"cellId":"sh0i0jlgroqwtnxsuyc05a","trusted":true},"outputs":[],"execution_count":160},{"cell_type":"code","source":"#!g1.1\nDEVICE = 'cuda'\nNUM_CLASSES = 200","metadata":{"cellId":"2xs59kepqowhmhc6rpd0ws","trusted":true},"outputs":[],"execution_count":208},{"cell_type":"code","source":"#!g1.1\ndef to_zca(train, test, zca_bias=0.0001):\n    orig_train_shape = train.shape\n    orig_test_shape = test.shape\n\n    train = np.ascontiguousarray(train, dtype=np.float32).reshape(train.shape[0], -1).astype(np.float64)\n    test = np.ascontiguousarray(test, dtype=np.float32).reshape(test.shape[0], -1).astype(np.float64)\n\n    n_train = train.shape[0]\n\n    # Zero mean every feature\n    train = train - np.mean(train, axis=1)[:,np.newaxis]\n    test = test - np.mean(test, axis=1)[:,np.newaxis]\n\n    # Normalize\n    train_norms = np.linalg.norm(train, axis=1)\n    test_norms = np.linalg.norm(test, axis=1)\n\n    # Make features unit norm\n    train = train/train_norms[:,np.newaxis]\n    test = test/test_norms[:,np.newaxis]\n\n    train = torch.to_tensor(train, dtype=torch.double).to(DEVICE)\n    train_cov_mat = 1.0/n_train * torch.matmul(train.T, train)\n\n    (E,V) = np.linalg.eig(train_cov_mat)\n\n    E += zca_bias\n    sqrt_zca_eigs = np.sqrt(E)\n    inv_sqrt_zca_eigs = np.diag(np.power(sqrt_zca_eigs, -1))\n    global_ZCA = V.dot(inv_sqrt_zca_eigs).dot(V.T)\n\n    train = (train).dot(global_ZCA)\n    test = (test).dot(global_ZCA)\n\n    return (train.reshape(orig_train_shape).astype(np.float64), test.reshape(orig_test_shape).astype(np.float64)), global_ZCA","metadata":{"cellId":"13uc7t42s02qm7qenfbg88","trusted":true},"outputs":[],"execution_count":182},{"cell_type":"code","source":"#!g1.1\ndef get_cifar_zca():\n    trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=None)\n    testset  = datasets.CIFAR10(root='./data', train=False, download=True, transform=None)\n\n    X_train = np.asarray(trainset.data).astype(np.float64)\n    y_train = np.asarray(trainset.targets)\n    X_test  = np.asarray(testset.data).astype(np.float64)\n    y_test  = np.asarray(testset.targets)\n\n    (X_train, X_test), global_ZCA = to_zca(X_train, X_test)\n\n    X_train = np.transpose(X_train, (0,3,1,2))\n    X_test  = np.transpose(X_test,  (0,3,1,2))\n\n    return X_train, y_train, X_test, y_test","metadata":{"cellId":"bg72k4qolereykmnm5t7tm","trusted":true},"outputs":[],"execution_count":183},{"cell_type":"code","source":"#!g1.1\n%%time\nX_train, _, X_test, _ = get_cifar_zca()","metadata":{"cellId":"im8xwj7lxyk2dr4cgz8gen","trusted":true},"outputs":[],"execution_count":184},{"cell_type":"code","source":"#!g1.1\nX_train[0].shape","metadata":{"cellId":"5q9y72jbhswrhor54yucks","trusted":true},"outputs":[],"execution_count":185},{"cell_type":"code","source":"X_train[0]","metadata":{"cellId":"tojv7qblmrlmam7bfes3zg","trusted":true},"outputs":[],"execution_count":186},{"cell_type":"code","source":"#!g1.1\n\nfor i in range(10):\n    image = np.transpose(X_train[i], (1,2,0))\n    plt.imshow(image)\n    plt.show()","metadata":{"cellId":"fd51777wy7qzxyozpd1zx","trusted":true},"outputs":[],"execution_count":187},{"cell_type":"code","source":"#!g1.1\nfrom sklearn.utils.extmath import randomized_svd\n\ndef to_zca_fast(train, test, zca_bias=0.01, n_components=1000):\n    print('fast zca')\n    orig_train_shape = train.shape\n    orig_test_shape = test.shape\n\n    train = np.ascontiguousarray(tracccccckdbvnbfhthicnccrtlnjfdhrfrjkkfnidkrlbl\n                                 t, dtype=np.float32).reshape(test.shape[0], -1).astype(np.float64)\n\n    n_train = train.shape[0]\n\n    # Zero mean every feature\n    train = train - np.mean(train, axis=1)[:,np.newaxis]\n    test  =  test - np.mean(test, axis=1)[:,np.newaxis]\n\n    # Normalize\n    train_norms = np.linalg.norm(train, axis=1)\n    test_norms = np.linalg.norm(test, axis=1)\n\n    # Make features unit norm\n    train = train/train_norms[:,np.newaxis]\n    test = test/test_norms[:,np.newaxis]\n\n    train_torch = torch.tensor(train, dtype=torch.float).to(DEVICE)\n    train_cov_mat = 1.0/n_train * torch.matmul(train_torch.T, train_torch).cpu().numpy()\n    del train_torch\n\n    U, S, Vt = randomized_svd(train_cov_mat, n_components=n_components)\n    \n    V = Vt.T\n    S += zca_bias\n    sqrt_zca_eigs = S\n    inv_sqrt_zca_eigs = np.diag(np.power(sqrt_zca_eigs, -1))\n    global_ZCA = V.dot(inv_sqrt_zca_eigs).dot(V.T)\n\n    train = (train).dot(global_ZCA)\n    test = (test).dot(global_ZCA)\n\n    return (train.reshape(orig_train_shape).astype(np.float64), test.reshape(orig_test_shape).astype(np.float64)), global_ZCA","metadata":{"cellId":"e9r3dfq7d2nvwwzn3n8ub","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\n\ndef to_zca_faster(train, test, zca_bias=0.01, n_components=1000):\n    print('faster zca')\n    orig_train_shape = train.shape\n    orig_test_shape = test.shape\n\n    train = np.ascontiguousarray(train, dtype=np.float32).reshape(train.shape[0], -1).astype(np.float64)\n    test  = np.ascontiguousarray(test, dtype=np.float32).reshape(test.shape[0], -1).astype(np.float64)\n\n    n_train = train.shape[0]\n    d       = train.shape[1]\n\n    # Zero mean every feature\n    train = train - np.mean(train, axis=1)[:,np.newaxis]\n    test  =  test - np.mean(test, axis=1)[:,np.newaxis]\n\n    # Normalize\n    train_norms = np.linalg.norm(train, axis=1)\n    test_norms  = np.linalg.norm(test, axis=1)\n\n    # Make features unit norm\n    train = train/train_norms[:,np.newaxis]\n    test  = test/test_norms[:,np.newaxis]\n    \n    print(train[0])\n    \n    print('calculating svd...')\n    \n    _, S, V = randomized_svd(train, n_components=n_components)\n    print('svd calculated. applying svd matricies.')\n    S = (S ** 2) / n_train + zca_bias\n    inv_sqrt_zca_eigs = np.diag(np.power(S, -1))\n\n    train = (train).dot(V.T).dot(inv_sqrt_zca_eigs).dot(V)\n    test  =  (test).dot(V.T).dot(inv_sqrt_zca_eigs).dot(V)\n    \n    print('faster zca done.')\n\n    return (train.reshape(orig_train_shape).astype(np.float64), test.reshape(orig_test_shape).astype(np.float64))\n\ndef get_cifar_zca_fast():\n    trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=None)\n    testset  = datasets.CIFAR10(root='./data', train=False, download=True, transform=None)\n\n    X_train = np.asarray(trainset.data).astype(np.float64)\n    y_train = np.asarray(trainset.targets)\n    X_test  = np.asarray(testset.data).astype(np.float64)\n    y_test  = np.asarray(testset.targets)\n\n    X_train, X_test = to_zca_faster(X_train, X_test, n_components=1000)\n\n    X_train = np.transpose(X_train, (0,3,1,2))\n    X_test  = np.transpose(X_test,  (0,3,1,2))\n\n    return X_train, y_train, X_test, y_test","metadata":{"cellId":"jb9suzclhuouptitk0288","trusted":true},"outputs":[],"execution_count":316},{"cell_type":"code","source":"#!g1.1\n%%time\n\nX_train, _, X_test, _ = get_cifar_zca_fast()","metadata":{"cellId":"mwrvkqfrp8pn924it5ozj","trusted":true},"outputs":[],"execution_count":317},{"cell_type":"code","source":"#!g1.1\nX_train[0]","metadata":{"cellId":"47adc1qwvgsox91gvp03","trusted":true},"outputs":[],"execution_count":318},{"cell_type":"code","source":"#!g1.1\nfor i in range(10):\n    image = np.transpose(X_train[i], (1,2,0))\n    plt.imshow(image)\n    plt.show()","metadata":{"cellId":"er4mnf5sg1izib1ihojz9","trusted":true},"outputs":[],"execution_count":319},{"cell_type":"code","source":"#!g1.1\ndef get_tiny_imagenet_zca(train_dir, test_dir):\n    print('getting tiny-imagenet zca:')\n    np.random.seed(0)\n    random.seed(0)\n    torch.manual_seed(0)\n    \n    print('loading datasets...')\n    \n    trainset = datasets.ImageFolder(train_dir, transform=transforms.ToTensor()) \n    testset  = datasets.ImageFolder(test_dir, transform=transforms.ToTensor())\n    \n    trainloader = utils.data.DataLoader(trainset, pin_memory=True, shuffle=True, batch_size=100000)\n    testloader  = utils.data.DataLoader(testset,  pin_memory=True, shuffle=False, batch_size=10000)\n    \n    _, (X_train, y_train)  = next(enumerate(trainloader))\n    _, (X_test,  y_test)   = next(enumerate(testloader))\n    \n    print('datasets loaded.')\n    \n    X_train = X_train.numpy().astype(np.float64)\n    X_test  = X_test.numpy().astype(np.float64)\n    \n    print('calculating zca...')\n    X_train, X_test = to_zca_faster(X_train, X_test, n_components=3000)\n    print('zca_calulated.')\n    return torch.tensor(X_train), y_train, torch.tensor(X_test), y_test\n\nTRAIN_DIR = 'tiny-imagenet-200/train'\nTEST_DIR  = 'tiny-imagenet-200/val'\n\nX_train_full, labels_train_full, X_test_full, labels_test_full = get_tiny_imagenet_zca(TRAIN_DIR, TEST_DIR)","metadata":{"cellId":"oivds8jiulknyxwdl8ig","trusted":true},"outputs":[],"execution_count":292},{"cell_type":"code","source":"#!g1.1\nfor i in range(10):\n    image = np.transpose(X_train_full[i], (1,2,0))\n    plt.imshow(image)\n    plt.show()","metadata":{"cellId":"8uwuxy92soxclf51401jt","trusted":true},"outputs":[],"execution_count":293},{"cell_type":"code","source":"#!g1.1\nfrom linearized_nns.pytorch_impl.nns.primitives import *\n\nclass Myrtle9(nn.Module):\n    def __init__(self, num_classes=1, input_filters=3, num_filters=1, groups=1):\n        super(Myrtle9, self).__init__()\n        filters = num_filters\n\n        def Activation():\n            return ReLU2()\n\n        self.layers = nn.Sequential(\n            Conv(input_filters, filters * groups), Activation(),\n            Conv(filters, filters * 2, groups),    Activation(),\n            nn.AvgPool2d(kernel_size=2, stride=2),\n\n            Conv(filters * 2, filters * 4, groups), Activation(),\n            Conv(filters * 4, filters * 8, groups), Activation(),\n            nn.AvgPool2d(kernel_size=2, stride=2),\n\n            Conv(filters *  8, filters * 16, groups), Activation(),\n            Conv(filters * 16, filters * 32, groups), Activation(),\n            nn.AvgPool2d(kernel_size=2, stride=2),\n            \n            Conv(filters *  32, filters * 32, groups), Activation(),\n            Conv(filters *  32, filters * 32, groups), Activation(),\n            nn.AvgPool2d(kernel_size=8, stride=8),\n\n            Flatten(),\n            Normalize(filters * 32)\n        )\n        self.classifier = nn.Linear(filters * 32 * groups, num_classes, bias=True)\n\n    def readout(self, x):\n        return self.layers(x)\n\n    def forward(self, x):\n        x = self.readout(x)\n        return self.classifier(x)","metadata":{"cellId":"79xwv9f8q4k074wzcuork1r","trusted":true},"outputs":[],"execution_count":294},{"cell_type":"code","source":"#!g1.1\nN_train = 1280 * 5\nN_test  = 1000\n\nX_train = X_train_full[:N_train].float()\nX_test  = X_test_full[:N_test].float()","metadata":{"cellId":"kpdub3v5lnqi3k1ay1er1q","trusted":true},"outputs":[],"execution_count":295},{"cell_type":"code","source":"#!g1.1\nlabels_train = labels_train_full[:N_train] \nlabels_test  = labels_test_full[:N_test]\n\ny_train = to_one_hot(labels_train, NUM_CLASSES).to(DEVICE)\ny_test  = to_one_hot(labels_test,  NUM_CLASSES).to(DEVICE)","metadata":{"cellId":"vx3vhgsok0pwhtxygt0wr","trusted":true},"outputs":[],"execution_count":305},{"cell_type":"code","source":"#!g1.1\ndef compute_kernels(models, X_train, X_test):\n    with torch.no_grad():\n        X_train = X_train.to(DEVICE)\n        X_test  = X_test.to(DEVICE)\n\n        n_train = len(X_train)\n        n_test  = len(X_test)\n\n        train_kernel = torch.zeros([n_train, n_train]).double().to(DEVICE)\n        test_kernel  = torch.zeros([n_test,  n_train]).double().to(DEVICE)\n\n        m = 0\n        start_time = time.time()\n\n        for model_i, model in enumerate(models):\n            model = model.to(DEVICE)\n            if model_i & (model_i - 1) == 0:\n                print(f\"{model_i} models done. time {time.time() - start_time:.0f}s\")\n\n            train_features = model.readout(X_train) \n            test_features  = model.readout(X_test)\n\n            m += 1\n\n            train_kernel += torch.matmul(train_features, train_features.T).double()\n            test_kernel  += torch.matmul(test_features,  train_features.T).double()\n\n        train_kernel /= m\n        test_kernel  /= m\n\n        return train_kernel.float(), test_kernel.float()","metadata":{"cellId":"0eof1assm4qeuqdfxbv6l6","trusted":true},"outputs":[],"execution_count":306},{"cell_type":"code","source":"#!g1.1\nX_train.shape, X_test.shape","metadata":{"cellId":"hbybuho0cznavxisu19on9","trusted":true},"outputs":[],"execution_count":307},{"cell_type":"code","source":"#!g1.1\n\nn_models = 500\nmodels = [Myrtle9(num_filters=1, groups=10) for _ in range(n_models)]\n\ntrain_kernel, test_kernel = compute_kernels(models, X_train, X_test)\n\ntrain_kernel = train_kernel.float().to(DEVICE)\ntest_kernel  = test_kernel.float().to(DEVICE)","metadata":{"cellId":"klh64b9opzpenpulg676ta","trusted":true},"outputs":[],"execution_count":308},{"cell_type":"code","source":"#!g1.1\ntrain_kernel[:5,:5]","metadata":{"cellId":"vhnio81xhdnte1l2ofx4c","trusted":true},"outputs":[],"execution_count":309},{"cell_type":"code","source":"#!g1.1\ntest_kernel[:5,:5]","metadata":{"cellId":"w232fponfhsf0jckdli7bh","trusted":true},"outputs":[],"execution_count":310},{"cell_type":"code","source":"#!g1.1\nlr = 1e5\n\nn = len(train_kernel)\n\nexp_term = - lr * compute_exp_term(- lr * train_kernel, DEVICE)\ny_pred = torch.matmul(test_kernel, torch.matmul(exp_term, - y_train))\n(y_pred.argmax(dim=1) == labels_test.to(DEVICE)).float().mean()","metadata":{"cellId":"cztzwgauiigtop4p1cdo0o","trusted":true},"outputs":[],"execution_count":311},{"cell_type":"code","source":"#!g1.1\nimport time\nimport numpy as np\nimport seaborn as sns\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms, datasets\nfrom torchvision.datasets import FashionMNIST\n\nfrom linearized_nns.estimator import Estimator\nfrom linearized_nns.pytorch_impl.estimators import SgdEstimator\nfrom linearized_nns.pytorch_impl.nns import Myrtle5, Myrtle7, Myrtle10\nfrom linearized_nns.pytorch_impl import ClassifierTraining\nfrom linearized_nns.pytorch_impl.matrix_exp import matrix_exp, compute_exp_term\nfrom linearized_nns.pytorch_impl.nns.utils import to_one_hot, print_sizes\nfrom linearized_nns.from_neural_kernels import to_zca, CustomTensorDataset, get_cifar_zca","metadata":{"cellId":"m37ddog3ijbgz7uqvye4la","trusted":true},"outputs":[],"execution_count":99},{"cell_type":"code","source":"#!g1.1\nclass GpEstimator(Estimator):\n    def __init__(self, models, n_classes, learning_rate, x_example, device, groups=1):\n        super(GpEstimator, self).__init__()\n        self.models    = [model.to(device) for model in models]\n        self.lr        = learning_rate\n        self.n_classes = n_classes\n        self.device    = device\n        \n        n = len(models)\n        X = torch.stack([x_example]).to(self.device)\n        \n        model = models[0].to(self.device)\n        readout_size = model.readout(X).size()[1]\n    \n        # TODO: Assert that models have the same readout size\n        \n        self.w      = torch.zeros([n, readout_size, n_classes]).to(self.device)\n        self.w_size = n * groups\n        \n    def get_w_update(self, X, right_vector):\n        with torch.no_grad():\n            assert len(X) == len(right_vector)\n\n            X            = X.to(self.device)\n            right_vector = right_vector.to(self.device)\n\n            n = len(X)\n            w_updates = []\n            \n            for model in self.models:\n                features = self.to_model_features(X, model)\n                update = torch.matmul(features.T, right_vector)\n                w_updates.append(update)\n            return torch.stack(w_updates)\n                                  \n    def to_model_features(self, X, model):\n        with torch.no_grad():\n            model = model.to(self.device)\n            return model.readout(X) * (1. / np.sqrt(self.w_size))\n        \n    def calc_kernel(self, X):\n        with torch.no_grad():\n            X = X.to(self.device)\n            \n            res = torch.zeros([len(X), len(X)]).to(self.device).double()\n            for model in self.models:\n                features_x = self.to_model_features(X, model)\n                \n                res += torch.matmul(features_x, features_x.T).double()\n            return res.float()\n        \n    def calc_kernel_pred(self, X):\n        with torch.no_grad():\n            X = X.to(self.device)\n            \n            n = len(X)\n            y_pred = torch.zeros([n, self.n_classes]).to(self.device).double()\n            kernel = torch.zeros([len(X), len(X)]).to(self.device).double()\n            \n            for model, w in zip(self.models, self.w):\n                features = self.to_model_features(X, model)\n                \n                kernel += torch.matmul(features, features.T).double()\n                y_pred += torch.matmul(features, w).double()\n            return kernel, y_pred.float()\n        \n    def calc_kernels(self, X_train, X_test):\n        with torch.no_grad():\n            X_train = X_train.to(self.device)\n            X_test  = X_test.to(self.device)\n            \n            res_train = torch.zeros([len(X_train), len(X_train)]).to(self.device)\n            res_test  = torch.zeros([len(X_test),  len(X_train)]).to(self.device)\n            for model in self.models:\n                features_train = self.to_model_features(X_train, model)\n                features_test  = self.to_model_features(X_test,  model)\n                \n                res_train += torch.matmul(features_train, features_train.T)\n                res_test  += torch.matmul(features_test,  features_train.T)\n            return res_train, res_test\n            \n    def predict(self, X, cur_w=None):\n        X = X.to(self.device)\n        if cur_w is None:\n            cur_w = self.w\n         \n        with torch.no_grad():\n            n = len(X)\n            res = torch.zeros([n, self.n_classes]).double().to(self.device)\n\n            for model, w in zip(self.models, cur_w):\n                features = self.to_model_features(X, model)\n                res += torch.matmul(features, w).double()\n            return res.float()","metadata":{"cellId":"0lf3vyd1xrmba3qucow7uwp","trusted":true},"outputs":[],"execution_count":116},{"cell_type":"code","source":"#!g1.1\n\ndef calc_right_vector(kernel, y, learning_rate=1e5, reg_param=0):\n    with torch.no_grad():\n        y      = y.to(DEVICE)\n        kernel = kernel.to(DEVICE)\n        \n        n      = len(kernel)\n        reg = torch.eye(n).to(DEVICE) * reg_param\n        \n        exp_term = - learning_rate * compute_exp_term(- learning_rate * (kernel + reg), DEVICE)\n        right_vector = torch.matmul(exp_term, - y)\n        return right_vector","metadata":{"cellId":"sd8l3ycoj1h6d7zku6y3q4","trusted":true},"outputs":[],"execution_count":101},{"cell_type":"code","source":"#!g1.1\n\nlabels_train_full.shape, labels_test_full.shape","metadata":{"cellId":"ps8kvr9wyv7hiewy5xoxp","trusted":true},"outputs":[],"execution_count":102},{"cell_type":"code","source":"#!g1.1\ndef boosting(estimator, train_loader, test_loader, learning_rate=1e5, beta=1., noise_rate=0., n_iter=10):\n    output_kernel = False\n    \n    with torch.no_grad():\n        batches_num = 0\n        for _ in enumerate(train_loader):\n            batches_num += 1\n        n_batches = (batches_num * 2) // 3 + 1\n            \n        for iter_num  in range(n_iter):\n            print(f\"iter {iter_num} ==========================\")\n            \n            w_update = 0\n            iter_start = time.time()\n            \n            for batch_i, (X, y) in enumerate(train_loader):\n                if batch_i >= n_batches:\n                    break\n                \n                X = X.to(DEVICE)\n                y = y.to(DEVICE)\n                \n                kernel, y_pred = estimator.calc_kernel_pred(X)\n                if output_kernel:\n                    print(f\"kernel\\n{kernel[:5,:5]}\")\n                \n                y_residual = y_pred - y\n                \n                train_acc = (y_pred.argmax(dim=1) == y.argmax(dim=1)).float().mean().item()\n                train_mse = (y_residual ** 2).mean().item()\n                print(f\"batch {batch_i}: train_acc {train_acc:.4f}, train_mse {train_mse:.6f}\")\n                \n                right_vector = calc_right_vector(kernel, y_residual, learning_rate=learning_rate)\n                \n                w_update += estimator.get_w_update(X, right_vector).double()\n                \n                pred_change = torch.matmul(kernel, right_vector.double())\n                \n\n            w_update = (w_update / n_batches).float()\n            \n            _, (X, y) = next(enumerate(train_loader))\n            y = y.to(DEVICE)\n            \n            y_pred      = estimator.predict(X)\n            pred_change = estimator.predict(X, w_update)\n            \n            y_residual = (y_pred - y)\n            \n            w_std = w_update.std()\n            noise = w_std * torch.randn(w_update.size()).to(DEVICE)\n            estimator.w -= (w_update + noise_rate * noise) * beta \n            \n            test_acc = 0\n            for i, (X_test, labels) in enumerate(test_loader):\n                y_pred = estimator.predict(X_test) \n                cur_acc   = (y_pred.argmax(dim=1) == labels.to(DEVICE)).float().mean().item()\n                test_acc += (cur_acc - test_acc) / (i + 1)\n                \n            print(f\"iter {iter_num} done. took {time.time() - iter_start:.0f}s. beta {beta:.3f}, test_acc {test_acc:.4f}\")\n            print()","metadata":{"cellId":"2g4i4dpu9w25kbqpzlx4g","trusted":true},"outputs":[],"execution_count":119},{"cell_type":"code","source":"#!g1.1\ntrain_set = CustomTensorDataset(X_train_full, to_one_hot(labels_train_full, NUM_CLASSES))\ntest_set  = CustomTensorDataset(X_test_full,  labels_test_full)\n\ntrain_loader = torch.utils.data.DataLoader(train_set, batch_size=1280 * 4)\ntest_loader  = torch.utils.data.DataLoader(test_set,  batch_size=2000)","metadata":{"cellId":"jxlilhbxfbjkwyxvfrq7sb","trusted":true},"outputs":[],"execution_count":122},{"cell_type":"code","source":"#!g1.1\n\n%state_exclude estimator\n%state_exclude models\n\ntorch.manual_seed(0)\nnp.random.seed(0)\n\nn_models = 256\nmodels = [Myrtle9(num_filters=1, groups=10) for _ in range(n_models)]\n\nestimator = GpEstimator(models, NUM_CLASSES, 0.2, X_train[0], DEVICE, groups=10)\nboosting(estimator, train_loader, test_loader, learning_rate=1e5, n_iter=30)","metadata":{"cellId":"w1i7foyygffn8d380yq2q","trusted":true},"outputs":[],"execution_count":123},{"cell_type":"code","source":"#!g1.1\n\n%state_exclude estimator\n%state_exclude models\n\ntorch.manual_seed(0)\nnp.random.seed(0)\n\nn_models = 1024\nmodels = [Myrtle9(num_filters=1, groups=10) for _ in range(n_models)]\n\nestimator = GpEstimator(models, NUM_CLASSES, 0.2, X_train[0], DEVICE, groups=10)\nboosting(estimator, train_loader, test_loader, learning_rate=1e5, n_iter=30)","metadata":{"cellId":"2kcxa1bfk37r945rkwk73","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\n","metadata":{"cellId":"kz005oeylsd4exbt5why6"},"outputs":[],"execution_count":null}]}