{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import urllib.request\n",
    "\n",
    "def download_repo(url, save_to):\n",
    "    zip_filename = save_to + '.zip'\n",
    "    urllib.request.urlretrieve(url, zip_filename)\n",
    "    \n",
    "    if os.path.exists(save_to):\n",
    "        shutil.rmtree(save_to)\n",
    "    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "        zip_ref.extractall('.')\n",
    "    del zip_ref\n",
    "    assert os.path.exists(save_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_PATH = 'LinearizedNNs-master'\n",
    "\n",
    "download_repo(url='https://github.com/maxkvant/LinearizedNNs/archive/master.zip', save_to=REPO_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(f\"{REPO_PATH}/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.datasets import FashionMNIST\n",
    "\n",
    "from estimator import Estimator\n",
    "from pytorch_impl.estimators import SgdEstimator\n",
    "from pytorch_impl.nns import Myrtle5, Myrtle7, Myrtle10\n",
    "from pytorch_impl import ClassifierTraining\n",
    "from pytorch_impl.matrix_exp import matrix_exp, compute_exp_term\n",
    "from pytorch_impl.nns.utils import to_one_hot, print_sizes\n",
    "from from_neural_kernels import to_zca, CustomTensorDataset, get_cifar_zca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0') if (torch.cuda.is_available()) else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cifar 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GpEstimator(Estimator):\n",
    "    def __init__(self, models, n_classes, learning_rate, x_example, device, groups=1):\n",
    "        super(GpEstimator, self).__init__()\n",
    "        self.models    = [model.to(device) for model in models]\n",
    "        self.lr        = learning_rate\n",
    "        self.n_classes = n_classes\n",
    "        self.device    = device\n",
    "        \n",
    "        n = len(models)\n",
    "        X = torch.stack([x_example]).to(device)\n",
    "        \n",
    "        model = models[0].to(device)\n",
    "        readout_size = model.readout(X).size()[1]\n",
    "    \n",
    "        # TODO: Assert that models have the same readout size\n",
    "        \n",
    "        self.w      = torch.zeros([n, readout_size, n_classes]).to(device)\n",
    "        self.w_size = n * groups\n",
    "        \n",
    "    def get_w_update(self, X, right_vector):\n",
    "        with torch.no_grad():\n",
    "            assert len(X) == len(right_vector)\n",
    "\n",
    "            X            = X.to(self.device)\n",
    "            right_vector = right_vector.to(self.device)\n",
    "\n",
    "            n = len(X)\n",
    "            w_updates = []\n",
    "            \n",
    "            for model in self.models:\n",
    "                features = self.to_model_features(X, model)\n",
    "                update = torch.matmul(features.T, right_vector)\n",
    "                w_updates.append(update)\n",
    "            return torch.stack(w_updates)\n",
    "                                  \n",
    "    def to_model_features(self, X, model):\n",
    "        with torch.no_grad():\n",
    "            model = model.to(device)\n",
    "            return model.readout(X) * (1. / np.sqrt(self.w_size))\n",
    "        \n",
    "    def calc_kernel(self, X):\n",
    "        with torch.no_grad():\n",
    "            X = X.to(device)\n",
    "            \n",
    "            res = torch.zeros([len(X), len(X)]).to(device).double()\n",
    "            for model in self.models:\n",
    "                features_x = self.to_model_features(X, model)\n",
    "                \n",
    "                res += torch.matmul(features_x, features_x.T).double()\n",
    "            return res.float()\n",
    "        \n",
    "    def calc_kernel_pred(self, X):\n",
    "        with torch.no_grad():\n",
    "            X = X.to(device)\n",
    "            \n",
    "            n = len(X)\n",
    "            y_pred = torch.zeros([n, self.n_classes]).to(device).double()\n",
    "            kernel = torch.zeros([len(X), len(X)]).to(device).double()\n",
    "            \n",
    "            for model, w in zip(self.models, self.w):\n",
    "                features = self.to_model_features(X, model)\n",
    "                \n",
    "                kernel += torch.matmul(features, features.T).double()\n",
    "                y_pred += torch.matmul(features, w).double()\n",
    "            return kernel, y_pred.float()\n",
    "        \n",
    "    def calc_kernels(self, X_train, X_test):\n",
    "        with torch.no_grad():\n",
    "            X_train = X_train.to(device)\n",
    "            X_test  = X_test.to(device)\n",
    "            \n",
    "            res_train = torch.zeros([len(X_train), len(X_train)]).to(device)\n",
    "            res_test  = torch.zeros([len(X_test),  len(X_train)]).to(device)\n",
    "            for model in self.models:\n",
    "                features_train = self.to_model_features(X_train, model)\n",
    "                features_test  = self.to_model_features(X_test,  model)\n",
    "                \n",
    "                res_train += torch.matmul(features_train, features_train.T)\n",
    "                res_test  += torch.matmul(features_test,  features_train.T)\n",
    "            return res_train, res_test\n",
    "            \n",
    "    def predict(self, X, cur_w=None):\n",
    "        X = X.to(self.device)\n",
    "        if cur_w is None:\n",
    "            cur_w = self.w\n",
    "         \n",
    "        with torch.no_grad():\n",
    "            n = len(X)\n",
    "            res = torch.zeros([n, self.n_classes]).double().to(device)\n",
    "\n",
    "            for model, w in zip(self.models, cur_w):\n",
    "                features = self.to_model_features(X, model)\n",
    "                res += torch.matmul(features, w).double()\n",
    "            return res.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_right_vector(kernel, y, learning_rate=1e5, reg_param=0):\n",
    "    with torch.no_grad():\n",
    "        y      = y.to(device)\n",
    "        kernel = kernel.to(device)\n",
    "        \n",
    "        n      = len(kernel)\n",
    "        reg = torch.eye(n).to(device) * reg_param\n",
    "        \n",
    "        exp_term = - learning_rate * compute_exp_term(- learning_rate * (kernel + reg), device)\n",
    "        right_vector = torch.matmul(exp_term, - y)\n",
    "        return right_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CPU times: user 3min 31s, sys: 1min, total: 4min 31s\n",
      "Wall time: 40.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X_train, labels_train, X_test, labels_test = get_cifar_zca()\n",
    "\n",
    "N_train = 12800\n",
    "N_test  = 1000\n",
    "\n",
    "X_train      = torch.tensor(X_train[:N_train]).float()\n",
    "labels_train = torch.tensor(labels_train[:N_train], dtype=torch.long)\n",
    "\n",
    "X_test       = torch.tensor(X_test[:N_test]).float()\n",
    "labels_test  = torch.tensor(labels_test[:N_test],  dtype=torch.long)\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "y_train = to_one_hot(labels_train, num_classes).to(device)\n",
    "y_test  = to_one_hot(labels_test,  num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = np.load('../data/cifar10_targets_2.npz')['targets'][:N_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxkvant/.local/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/maxkvant/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "cifar_train = CustomTensorDataset(torch.tensor(X_train), torch.tensor(y_train).float(), transform='flips')\n",
    "cifar_test  = CustomTensorDataset(torch.tensor(X_test),  torch.tensor(labels_test, dtype=torch.long))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar_train, batch_size=1280 * 2)\n",
    "test_loader  = torch.utils.data.DataLoader(cifar_test, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boosting(estimator, train_loader, test_loader, learning_rate=1e5, n_iter=10):\n",
    "    output_kernel = False\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        batches_num = 0\n",
    "        for _ in enumerate(train_loader):\n",
    "            batches_num += 1\n",
    "        n_batches = (batches_num * 2) // 3 + 1\n",
    "        \n",
    "        test_size = 0\n",
    "        for _, (X, _) in enumerate(test_loader):\n",
    "            test_size += len(X)\n",
    "            \n",
    "        for iter_num  in range(n_iter):\n",
    "            print(f\"iter {iter_num} ==========================\")\n",
    "            \n",
    "            betas = []\n",
    "            w_update = 0\n",
    "            iter_start = time.time()\n",
    "            \n",
    "            for batch_i, (X, y) in enumerate(train_loader):\n",
    "                if batch_i >= n_batches:\n",
    "                    break\n",
    "                \n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                \n",
    "                kernel, y_pred = estimator.calc_kernel_pred(X)\n",
    "                if output_kernel:\n",
    "                    print(f\"kernel\\n{kernel[:5,:5]}\")\n",
    "                \n",
    "                y_residual = y_pred - y\n",
    "                \n",
    "                train_acc = (y_pred.argmax(dim=1) == y.argmax(dim=1)).float().mean().item()\n",
    "                train_mse = (y_residual ** 2).mean().item()\n",
    "                print(f\"batch {batch_i}: train_acc {train_acc:.4f}, train_mse {train_mse:.6f}\")\n",
    "                \n",
    "                right_vector = calc_right_vector(kernel, y_residual, learning_rate=learning_rate)\n",
    "                \n",
    "                w_update += estimator.get_w_update(X, right_vector).double()\n",
    "                \n",
    "                pred_change = torch.matmul(kernel, right_vector)\n",
    "                cur_beta    = (- y_residual * pred_change).sum() / (pred_change ** 2).sum()\n",
    "                \n",
    "                w_update += cur_w_update\n",
    "                betas.append(cur_beta.item())\n",
    "            \n",
    "            w_update = (w_update / n_batches).float()\n",
    "            \n",
    "            _, (X, y) = next(enumerate(train_loader))\n",
    "            \n",
    "            y_pred      = estimator.predict(X)\n",
    "            pred_change = estimator.predict(X, w_update)\n",
    "            \n",
    "            y_residual = y_pred - y\n",
    "            beta    = (- y_residual * pred_change).sum() / (pred_change ** 2).sum().item()\n",
    "                \n",
    "            estimator.w += w_update * beta\n",
    "            \n",
    "            test_acc = 0\n",
    "            for _, (X_test, labels) in enumerate(test_loader):\n",
    "                y_pred = estimator.predict(X_test) \n",
    "                test_acc += (y_pred.argmax(dim=1) == labels.to(device)).float().sum().item() / test_size\n",
    "                \n",
    "            print(f\"iter {iter_num} done. took {time.time() - iter_start:.0f}s. beta {beta:.3f}, test_acc {test_acc:.4f}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models = 500\n",
    "\n",
    "# 500 * 50 * 32  = 800k\n",
    "\n",
    "models = [Myrtle7(num_filters=1, groups=50) for _ in range(n_models)]\n",
    "n_models\n",
    "\n",
    "estimator = GpEstimator(models, num_classes, 0.2, X_train[0], device, groups=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 ==========================\n",
      "batch 0: train_acc 0.1008, train_mse 1.000000\n",
      "batch 1: train_acc 0.0945, train_mse 1.000000\n",
      "batch 2: train_acc 0.1059, train_mse 1.000000\n",
      "batch 3: train_acc 0.1016, train_mse 1.000000\n",
      "iter 0 done. took 294s test_acc 0.7500\n",
      "\n",
      "iter 1 ==========================\n",
      "batch 0: train_acc 0.9004, train_mse 0.144447\n",
      "batch 1: train_acc 0.8973, train_mse 0.145056\n",
      "batch 2: train_acc 0.8918, train_mse 0.144826\n",
      "batch 3: train_acc 0.8965, train_mse 0.144527\n",
      "iter 1 done. took 294s test_acc 0.7680\n",
      "\n"
     ]
    }
   ],
   "source": [
    "boosting(estimator, train_loader, test_loader, learning_rate=1e6, n_iter=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 ==========================\n",
      "batch 0: train_acc 0.9480, train_mse 0.104977\n",
      "batch 1: train_acc 0.9523, train_mse 0.106574\n",
      "batch 2: train_acc 0.9508, train_mse 0.106762\n",
      "batch 3: train_acc 0.9465, train_mse 0.105559\n",
      "iter 0 done. took 294s test_acc 0.7660\n",
      "\n",
      "iter 1 ==========================\n",
      "batch 0: train_acc 0.9809, train_mse 0.082550\n",
      "batch 1: train_acc 0.9734, train_mse 0.085491\n",
      "batch 2: train_acc 0.9691, train_mse 0.085250\n",
      "batch 3: train_acc 0.9742, train_mse 0.084768\n",
      "iter 1 done. took 294s test_acc 0.7790\n",
      "\n",
      "iter 2 ==========================\n",
      "batch 0: train_acc 0.9867, train_mse 0.066001\n",
      "batch 1: train_acc 0.9848, train_mse 0.069532\n",
      "batch 2: train_acc 0.9871, train_mse 0.069126\n",
      "batch 3: train_acc 0.9852, train_mse 0.067711\n",
      "iter 2 done. took 294s test_acc 0.7830\n",
      "\n",
      "iter 3 ==========================\n",
      "batch 0: train_acc 0.9918, train_mse 0.057009\n",
      "batch 1: train_acc 0.9941, train_mse 0.056957\n",
      "batch 2: train_acc 0.9930, train_mse 0.056395\n",
      "batch 3: train_acc 0.9941, train_mse 0.055626\n",
      "iter 3 done. took 294s test_acc 0.7890\n",
      "\n",
      "iter 4 ==========================\n",
      "batch 0: train_acc 0.9949, train_mse 0.048117\n",
      "batch 1: train_acc 0.9980, train_mse 0.047962\n",
      "batch 2: train_acc 0.9957, train_mse 0.048143\n",
      "batch 3: train_acc 0.9949, train_mse 0.047585\n",
      "iter 4 done. took 294s test_acc 0.7870\n",
      "\n",
      "iter 5 ==========================\n",
      "batch 0: train_acc 0.9977, train_mse 0.040000\n",
      "batch 1: train_acc 0.9977, train_mse 0.040473\n",
      "batch 2: train_acc 0.9973, train_mse 0.041284\n",
      "batch 3: train_acc 0.9961, train_mse 0.041027\n",
      "iter 5 done. took 294s test_acc 0.8010\n",
      "\n",
      "iter 6 ==========================\n",
      "batch 0: train_acc 0.9996, train_mse 0.033990\n",
      "batch 1: train_acc 0.9992, train_mse 0.035001\n",
      "batch 2: train_acc 1.0000, train_mse 0.035703\n",
      "batch 3: train_acc 0.9980, train_mse 0.035207\n",
      "iter 6 done. took 294s test_acc 0.7940\n",
      "\n",
      "iter 7 ==========================\n",
      "batch 0: train_acc 1.0000, train_mse 0.030484\n",
      "batch 1: train_acc 0.9996, train_mse 0.030541\n",
      "batch 2: train_acc 0.9996, train_mse 0.030670\n",
      "batch 3: train_acc 0.9984, train_mse 0.030360\n",
      "iter 7 done. took 294s test_acc 0.7990\n",
      "\n"
     ]
    }
   ],
   "source": [
    "boosting(estimator, train_loader, test_loader, learning_rate=1e6, n_iter=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 ==========================\n",
      "batch 0: train_acc 0.0977, train_mse 7.183765\n",
      "batch 1: train_acc 0.1037, train_mse 7.173288\n",
      "batch 2: train_acc 0.0973, train_mse 7.196385\n",
      "iter 0 done. took 100s test_acc 0.6620\n",
      "\n",
      "iter 1 ==========================\n",
      "batch 0: train_acc 0.6648, train_mse 3.893153\n",
      "batch 1: train_acc 0.6594, train_mse 3.899528\n",
      "batch 2: train_acc 0.6680, train_mse 3.843274\n",
      "iter 1 done. took 102s test_acc 0.6920\n",
      "\n",
      "iter 2 ==========================\n",
      "batch 0: train_acc 0.7027, train_mse 3.694586\n",
      "batch 1: train_acc 0.7041, train_mse 3.693863\n",
      "batch 2: train_acc 0.7086, train_mse 3.654493\n",
      "iter 2 done. took 99s test_acc 0.6800\n",
      "\n",
      "iter 3 ==========================\n",
      "batch 0: train_acc 0.7195, train_mse 3.575631\n",
      "batch 1: train_acc 0.7086, train_mse 3.580958\n",
      "batch 2: train_acc 0.7141, train_mse 3.561848\n",
      "iter 3 done. took 102s test_acc 0.7100\n",
      "\n",
      "iter 4 ==========================\n",
      "batch 0: train_acc 0.7357, train_mse 3.459083\n",
      "batch 1: train_acc 0.7262, train_mse 3.465120\n",
      "batch 2: train_acc 0.7371, train_mse 3.448418\n",
      "iter 4 done. took 96s test_acc 0.6890\n",
      "\n",
      "iter 5 ==========================\n",
      "batch 0: train_acc 0.7461, train_mse 3.431979\n",
      "batch 1: train_acc 0.7318, train_mse 3.432266\n",
      "batch 2: train_acc 0.7449, train_mse 3.383013\n",
      "iter 5 done. took 101s test_acc 0.7140\n",
      "\n",
      "iter 6 ==========================\n",
      "batch 0: train_acc 0.7545, train_mse 3.376752\n",
      "batch 1: train_acc 0.7463, train_mse 3.392027\n",
      "batch 2: train_acc 0.7422, train_mse 3.388392\n",
      "iter 6 done. took 95s test_acc 0.7150\n",
      "\n",
      "iter 7 ==========================\n",
      "batch 0: train_acc 0.7605, train_mse 3.333215\n",
      "batch 1: train_acc 0.7574, train_mse 3.335073\n",
      "batch 2: train_acc 0.7352, train_mse 3.382252\n",
      "iter 7 done. took 103s test_acc 0.7220\n",
      "\n",
      "iter 8 ==========================\n",
      "batch 0: train_acc 0.7525, train_mse 3.357162\n",
      "batch 1: train_acc 0.7633, train_mse 3.286419\n",
      "batch 2: train_acc 0.7598, train_mse 3.293091\n",
      "iter 8 done. took 96s test_acc 0.7190\n",
      "\n",
      "iter 9 ==========================\n",
      "batch 0: train_acc 0.7621, train_mse 3.328327\n",
      "batch 1: train_acc 0.7568, train_mse 3.301500\n",
      "batch 2: train_acc 0.7719, train_mse 3.234032\n",
      "iter 9 done. took 103s test_acc 0.7190\n",
      "\n"
     ]
    }
   ],
   "source": [
    "estimator = GpEstimator(models, num_classes, 0.2, X_train[0], device)\n",
    "boosting(estimator, train_loader, test_loader, learning_rate=1e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 ==========================\n",
      "batch 0: train_acc 0.0977, train_mse 7.183765\n",
      "batch 1: train_acc 0.1037, train_mse 7.173288\n",
      "batch 2: train_acc 0.0973, train_mse 7.196385\n",
      "iter 0 done. took 99s test_acc 0.6670\n",
      "\n",
      "iter 1 ==========================\n",
      "batch 0: train_acc 0.6859, train_mse 4.011244\n",
      "batch 1: train_acc 0.6986, train_mse 3.945497\n",
      "batch 2: train_acc 0.7074, train_mse 3.875035\n",
      "iter 1 done. took 103s test_acc 0.6880\n",
      "\n",
      "iter 2 ==========================\n",
      "batch 0: train_acc 0.7076, train_mse 3.973394\n",
      "batch 1: train_acc 0.7105, train_mse 3.957006\n",
      "batch 2: train_acc 0.7211, train_mse 3.942075\n",
      "iter 2 done. took 98s test_acc 0.6740\n",
      "\n",
      "iter 3 ==========================\n",
      "batch 0: train_acc 0.7281, train_mse 3.969994\n",
      "batch 1: train_acc 0.7188, train_mse 3.965328\n",
      "batch 2: train_acc 0.7355, train_mse 3.896935\n",
      "iter 3 done. took 103s test_acc 0.6790\n",
      "\n",
      "iter 4 ==========================\n",
      "batch 0: train_acc 0.7303, train_mse 3.971407\n",
      "batch 1: train_acc 0.7201, train_mse 3.975674\n",
      "batch 2: train_acc 0.7227, train_mse 3.989176\n",
      "iter 4 done. took 100s test_acc 0.6820\n",
      "\n",
      "iter 5 ==========================\n",
      "batch 0: train_acc 0.7400, train_mse 3.962332\n",
      "batch 1: train_acc 0.7293, train_mse 3.992778\n",
      "batch 2: train_acc 0.7359, train_mse 4.015637\n",
      "iter 5 done. took 103s test_acc 0.6770\n",
      "\n",
      "iter 6 ==========================\n",
      "batch 0: train_acc 0.7377, train_mse 4.014328\n",
      "batch 1: train_acc 0.7373, train_mse 3.968863\n",
      "batch 2: train_acc 0.7355, train_mse 4.000456\n",
      "iter 6 done. took 100s test_acc 0.6860\n",
      "\n",
      "iter 7 ==========================\n",
      "batch 0: train_acc 0.7496, train_mse 3.937073\n",
      "batch 1: train_acc 0.7492, train_mse 3.930587\n",
      "batch 2: train_acc 0.7316, train_mse 4.016242\n",
      "iter 7 done. took 102s test_acc 0.6900\n",
      "\n",
      "iter 8 ==========================\n",
      "batch 0: train_acc 0.7514, train_mse 3.972228\n",
      "batch 1: train_acc 0.7463, train_mse 3.970948\n",
      "batch 2: train_acc 0.7516, train_mse 3.968710\n",
      "iter 8 done. took 99s test_acc 0.6980\n",
      "\n",
      "iter 9 ==========================\n",
      "batch 0: train_acc 0.7520, train_mse 3.992492\n",
      "batch 1: train_acc 0.7375, train_mse 3.985981\n",
      "batch 2: train_acc 0.7473, train_mse 3.977951\n",
      "iter 9 done. took 103s test_acc 0.7020\n",
      "\n"
     ]
    }
   ],
   "source": [
    "estimator = GpEstimator(models, num_classes, 0.2, X_train[0], device)\n",
    "boosting(estimator, train_loader, test_loader, learning_rate=1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models = 5000\n",
    "\n",
    "models = [Myrtle5(num_filters=32) for _ in range(n_models)]\n",
    "\n",
    "estimator = GpEstimator(models, num_classes, 0.2, X_train[0], device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 ==========================\n",
      "batch 0: train_acc 0.0992, train_mse 1.000000\n",
      "batch 1: train_acc 0.1023, train_mse 1.000000\n",
      "batch 2: train_acc 0.0867, train_mse 1.000000\n",
      "batch 3: train_acc 0.1023, train_mse 1.000000\n",
      "batch 4: train_acc 0.1063, train_mse 1.000000\n",
      "batch 5: train_acc 0.1055, train_mse 1.000000\n",
      "batch 6: train_acc 0.1086, train_mse 1.000000\n",
      "iter 0 done. test_acc 0.6610\n",
      "iter 1 ==========================\n",
      "batch 0: train_acc 0.6898, train_mse 0.228092\n",
      "batch 1: train_acc 0.6898, train_mse 0.229243\n",
      "batch 2: train_acc 0.6852, train_mse 0.231006\n",
      "batch 3: train_acc 0.6664, train_mse 0.232013\n",
      "batch 4: train_acc 0.6750, train_mse 0.231813\n",
      "batch 5: train_acc 0.6703, train_mse 0.232756\n",
      "batch 6: train_acc 0.6609, train_mse 0.234007\n",
      "iter 1 done. test_acc 0.6930\n",
      "iter 2 ==========================\n",
      "batch 0: train_acc 0.7344, train_mse 0.212709\n",
      "batch 1: train_acc 0.7438, train_mse 0.211246\n",
      "batch 2: train_acc 0.7203, train_mse 0.213644\n",
      "batch 3: train_acc 0.7297, train_mse 0.215032\n",
      "batch 4: train_acc 0.7312, train_mse 0.209461\n",
      "batch 5: train_acc 0.7203, train_mse 0.212636\n",
      "batch 6: train_acc 0.7258, train_mse 0.214520\n",
      "iter 2 done. test_acc 0.7080\n",
      "iter 3 ==========================\n",
      "batch 0: train_acc 0.7688, train_mse 0.199916\n",
      "batch 1: train_acc 0.7766, train_mse 0.197256\n",
      "batch 2: train_acc 0.7688, train_mse 0.199618\n",
      "batch 3: train_acc 0.7594, train_mse 0.201165\n",
      "batch 4: train_acc 0.7508, train_mse 0.201875\n",
      "batch 5: train_acc 0.7445, train_mse 0.202120\n",
      "batch 6: train_acc 0.7547, train_mse 0.201036\n",
      "iter 3 done. test_acc 0.7090\n",
      "iter 4 ==========================\n",
      "batch 0: train_acc 0.8078, train_mse 0.189755\n",
      "batch 1: train_acc 0.7797, train_mse 0.193103\n",
      "batch 2: train_acc 0.7844, train_mse 0.194073\n",
      "batch 3: train_acc 0.7719, train_mse 0.195219\n",
      "batch 4: train_acc 0.7758, train_mse 0.193895\n",
      "batch 5: train_acc 0.7844, train_mse 0.191668\n",
      "batch 6: train_acc 0.7695, train_mse 0.194899\n",
      "iter 4 done. test_acc 0.7240\n",
      "iter 5 ==========================\n",
      "batch 0: train_acc 0.8063, train_mse 0.182190\n",
      "batch 1: train_acc 0.8055, train_mse 0.182030\n",
      "batch 2: train_acc 0.8211, train_mse 0.180914\n",
      "batch 3: train_acc 0.8000, train_mse 0.188087\n",
      "batch 4: train_acc 0.7836, train_mse 0.184472\n",
      "batch 5: train_acc 0.7820, train_mse 0.187855\n",
      "batch 6: train_acc 0.7812, train_mse 0.189223\n",
      "iter 5 done. test_acc 0.7290\n",
      "iter 6 ==========================\n",
      "batch 0: train_acc 0.8180, train_mse 0.177095\n",
      "batch 1: train_acc 0.8258, train_mse 0.176912\n",
      "batch 2: train_acc 0.8273, train_mse 0.178322\n",
      "batch 3: train_acc 0.8117, train_mse 0.179879\n",
      "batch 4: train_acc 0.8141, train_mse 0.179542\n",
      "batch 5: train_acc 0.8117, train_mse 0.178235\n",
      "batch 6: train_acc 0.8039, train_mse 0.181385\n",
      "iter 6 done. test_acc 0.7320\n",
      "iter 7 ==========================\n",
      "batch 0: train_acc 0.8258, train_mse 0.174508\n",
      "batch 1: train_acc 0.8367, train_mse 0.173688\n",
      "batch 2: train_acc 0.8352, train_mse 0.172211\n",
      "batch 3: train_acc 0.8375, train_mse 0.177413\n",
      "batch 4: train_acc 0.8070, train_mse 0.176903\n",
      "batch 5: train_acc 0.8188, train_mse 0.175442\n",
      "batch 6: train_acc 0.8273, train_mse 0.177043\n",
      "iter 7 done. test_acc 0.7330\n",
      "iter 8 ==========================\n",
      "batch 0: train_acc 0.8500, train_mse 0.167686\n",
      "batch 1: train_acc 0.8469, train_mse 0.171867\n",
      "batch 2: train_acc 0.8461, train_mse 0.169275\n",
      "batch 3: train_acc 0.8461, train_mse 0.170154\n",
      "batch 4: train_acc 0.8352, train_mse 0.169808\n",
      "batch 5: train_acc 0.8203, train_mse 0.173348\n",
      "batch 6: train_acc 0.8211, train_mse 0.173705\n",
      "iter 8 done. test_acc 0.7430\n",
      "iter 9 ==========================\n",
      "batch 0: train_acc 0.8656, train_mse 0.164371\n",
      "batch 1: train_acc 0.8375, train_mse 0.166409\n",
      "batch 2: train_acc 0.8562, train_mse 0.163375\n",
      "batch 3: train_acc 0.8484, train_mse 0.170163\n",
      "batch 4: train_acc 0.8445, train_mse 0.167020\n",
      "batch 5: train_acc 0.8250, train_mse 0.169003\n",
      "batch 6: train_acc 0.8391, train_mse 0.168518\n",
      "iter 9 done. test_acc 0.7460\n"
     ]
    }
   ],
   "source": [
    "boosting(estimator, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxkvant/.local/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "cifar_train = CustomTensorDataset(torch.tensor(X_train), torch.tensor(labels_train, dtype=torch.long), transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models = 100\n",
    "\n",
    "models = [Myrtle5(num_filters=32) for _ in range(n_models)]\n",
    "\n",
    "estimator = GpEstimator(models, num_classes, 0.2, X_train[0], device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 ==========================\n",
      "batch 0: train_acc 0.0992, train_mse 1.000000\n",
      "batch 1: train_acc 0.1023, train_mse 1.000000\n",
      "batch 2: train_acc 0.0867, train_mse 1.000000\n",
      "batch 3: train_acc 0.1023, train_mse 1.000000\n",
      "batch 4: train_acc 0.1063, train_mse 1.000000\n",
      "batch 5: train_acc 0.1055, train_mse 1.000000\n",
      "batch 6: train_acc 0.1086, train_mse 1.000000\n",
      "iter 0 done. took 630s test_acc 0.6570\n",
      "\n",
      "iter 1 ==========================\n",
      "batch 0: train_acc 0.6945, train_mse 0.230021\n",
      "batch 1: train_acc 0.6820, train_mse 0.229634\n",
      "batch 2: train_acc 0.6617, train_mse 0.232192\n",
      "batch 3: train_acc 0.6844, train_mse 0.231658\n",
      "batch 4: train_acc 0.6617, train_mse 0.233073\n",
      "batch 5: train_acc 0.6609, train_mse 0.234637\n",
      "batch 6: train_acc 0.6672, train_mse 0.234089\n",
      "iter 1 done. took 627s test_acc 0.6890\n",
      "\n",
      "iter 2 ==========================\n",
      "batch 0: train_acc 0.7398, train_mse 0.210167\n",
      "batch 1: train_acc 0.7594, train_mse 0.208906\n",
      "batch 2: train_acc 0.7469, train_mse 0.209019\n",
      "batch 3: train_acc 0.7258, train_mse 0.212323\n",
      "batch 4: train_acc 0.7227, train_mse 0.214496\n",
      "batch 5: train_acc 0.7148, train_mse 0.212550\n",
      "batch 6: train_acc 0.7289, train_mse 0.212665\n",
      "iter 2 done. took 627s test_acc 0.7060\n",
      "\n",
      "iter 3 ==========================\n",
      "batch 0: train_acc 0.7734, train_mse 0.197508\n",
      "batch 1: train_acc 0.7727, train_mse 0.197043\n",
      "batch 2: train_acc 0.7625, train_mse 0.198872\n",
      "batch 3: train_acc 0.7523, train_mse 0.201083\n",
      "batch 4: train_acc 0.7609, train_mse 0.201622\n",
      "batch 5: train_acc 0.7602, train_mse 0.199801\n",
      "batch 6: train_acc 0.7336, train_mse 0.205749\n",
      "iter 3 done. took 627s test_acc 0.7090\n",
      "\n",
      "iter 4 ==========================\n",
      "batch 0: train_acc 0.7977, train_mse 0.188870\n",
      "batch 1: train_acc 0.8070, train_mse 0.189177\n",
      "batch 2: train_acc 0.7906, train_mse 0.192542\n",
      "batch 3: train_acc 0.7750, train_mse 0.196690\n",
      "batch 4: train_acc 0.7727, train_mse 0.194398\n",
      "batch 5: train_acc 0.7727, train_mse 0.191532\n",
      "batch 6: train_acc 0.7672, train_mse 0.195072\n",
      "iter 4 done. took 627s test_acc 0.7140\n",
      "\n",
      "iter 5 ==========================\n",
      "batch 0: train_acc 0.7875, train_mse 0.187409\n",
      "batch 1: train_acc 0.8063, train_mse 0.182657\n",
      "batch 2: train_acc 0.8195, train_mse 0.180106\n",
      "batch 3: train_acc 0.7961, train_mse 0.188681\n",
      "batch 4: train_acc 0.7961, train_mse 0.186156\n",
      "batch 5: train_acc 0.8008, train_mse 0.186560\n",
      "batch 6: train_acc 0.7945, train_mse 0.187475\n",
      "iter 5 done. took 627s test_acc 0.7170\n",
      "\n",
      "iter 6 ==========================\n",
      "batch 0: train_acc 0.8250, train_mse 0.179773\n",
      "batch 1: train_acc 0.8297, train_mse 0.178388\n",
      "batch 2: train_acc 0.8305, train_mse 0.178857\n",
      "batch 3: train_acc 0.8016, train_mse 0.183270\n",
      "batch 4: train_acc 0.8047, train_mse 0.184140\n",
      "batch 5: train_acc 0.8156, train_mse 0.180444\n",
      "batch 6: train_acc 0.8055, train_mse 0.180003\n",
      "iter 6 done. took 627s test_acc 0.7240\n",
      "\n",
      "iter 7 ==========================\n",
      "batch 0: train_acc 0.8219, train_mse 0.176705\n",
      "batch 1: train_acc 0.8344, train_mse 0.172505\n",
      "batch 2: train_acc 0.8422, train_mse 0.172355\n",
      "batch 3: train_acc 0.8266, train_mse 0.176559\n",
      "batch 4: train_acc 0.8125, train_mse 0.178289\n",
      "batch 5: train_acc 0.8266, train_mse 0.175365\n",
      "batch 6: train_acc 0.8078, train_mse 0.179144\n",
      "iter 7 done. took 627s test_acc 0.7320\n",
      "\n",
      "iter 8 ==========================\n",
      "batch 0: train_acc 0.8422, train_mse 0.173064\n",
      "batch 1: train_acc 0.8383, train_mse 0.171276\n",
      "batch 2: train_acc 0.8406, train_mse 0.168334\n",
      "batch 3: train_acc 0.8344, train_mse 0.171212\n",
      "batch 4: train_acc 0.8234, train_mse 0.171870\n",
      "batch 5: train_acc 0.8414, train_mse 0.168768\n",
      "batch 6: train_acc 0.8328, train_mse 0.173765\n",
      "iter 8 done. took 627s test_acc 0.7340\n",
      "\n",
      "iter 9 ==========================\n",
      "batch 0: train_acc 0.8500, train_mse 0.164879\n",
      "batch 1: train_acc 0.8609, train_mse 0.164670\n",
      "batch 2: train_acc 0.8602, train_mse 0.165061\n",
      "batch 3: train_acc 0.8469, train_mse 0.169882\n",
      "batch 4: train_acc 0.8641, train_mse 0.168871\n",
      "batch 5: train_acc 0.8445, train_mse 0.168475\n",
      "batch 6: train_acc 0.8281, train_mse 0.170304\n",
      "iter 9 done. took 627s test_acc 0.7300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "boosting(estimator, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 ==========================\n",
      "batch 0: train_acc 0.8664, train_mse 0.160979\n",
      "batch 1: train_acc 0.8602, train_mse 0.162237\n",
      "batch 2: train_acc 0.8539, train_mse 0.163230\n",
      "batch 3: train_acc 0.8453, train_mse 0.167403\n",
      "batch 4: train_acc 0.8555, train_mse 0.163886\n",
      "batch 5: train_acc 0.8516, train_mse 0.164142\n",
      "batch 6: train_acc 0.8484, train_mse 0.163351\n",
      "iter 0 done. took 628s test_acc 0.7280\n",
      "\n",
      "iter 1 ==========================\n",
      "batch 0: train_acc 0.8641, train_mse 0.158279\n",
      "batch 1: train_acc 0.8680, train_mse 0.157631\n",
      "batch 2: train_acc 0.8727, train_mse 0.158165\n",
      "batch 3: train_acc 0.8672, train_mse 0.160398\n",
      "batch 4: train_acc 0.8695, train_mse 0.161058\n",
      "batch 5: train_acc 0.8625, train_mse 0.159203\n",
      "batch 6: train_acc 0.8688, train_mse 0.160781\n",
      "iter 1 done. took 627s test_acc 0.7360\n",
      "\n",
      "iter 2 ==========================\n",
      "batch 0: train_acc 0.8781, train_mse 0.157369\n",
      "batch 1: train_acc 0.8758, train_mse 0.155880\n",
      "batch 2: train_acc 0.8727, train_mse 0.156660\n",
      "batch 3: train_acc 0.8602, train_mse 0.157308\n",
      "batch 4: train_acc 0.8578, train_mse 0.160059\n",
      "batch 5: train_acc 0.8625, train_mse 0.159050\n",
      "batch 6: train_acc 0.8727, train_mse 0.158291\n",
      "iter 2 done. took 627s test_acc 0.7420\n",
      "\n",
      "iter 3 ==========================\n",
      "batch 0: train_acc 0.8797, train_mse 0.155567\n",
      "batch 1: train_acc 0.8781, train_mse 0.153480\n",
      "batch 2: train_acc 0.9031, train_mse 0.151057\n",
      "batch 3: train_acc 0.8742, train_mse 0.157775\n",
      "batch 4: train_acc 0.8766, train_mse 0.156732\n",
      "batch 5: train_acc 0.8719, train_mse 0.155898\n",
      "batch 6: train_acc 0.8570, train_mse 0.158740\n",
      "iter 3 done. took 627s test_acc 0.7430\n",
      "\n",
      "iter 4 ==========================\n",
      "batch 0: train_acc 0.8859, train_mse 0.152688\n",
      "batch 1: train_acc 0.8953, train_mse 0.150113\n",
      "batch 2: train_acc 0.8922, train_mse 0.150463\n",
      "batch 3: train_acc 0.8836, train_mse 0.153402\n",
      "batch 4: train_acc 0.8898, train_mse 0.150868\n",
      "batch 5: train_acc 0.8922, train_mse 0.149789\n",
      "batch 6: train_acc 0.8633, train_mse 0.154158\n",
      "iter 4 done. took 627s test_acc 0.7460\n",
      "\n"
     ]
    }
   ],
   "source": [
    "boosting(estimator, train_loader, test_loader, n_iter=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Cifar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CPU times: user 3min 33s, sys: 1min 2s, total: 4min 35s\n",
      "Wall time: 41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X_train, labels_train, X_test, labels_test = get_cifar_zca()\n",
    "\n",
    "X_train      = torch.tensor(X_train).float()\n",
    "labels_train = torch.tensor(labels_train, dtype=torch.long)\n",
    "\n",
    "X_test       = torch.tensor(X_test).float()\n",
    "labels_test  = torch.tensor(labels_test,  dtype=torch.long)\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "y_train = to_one_hot(labels_train, num_classes).to(device)\n",
    "y_test  = to_one_hot(labels_test,  num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = np.load('../data/cifar10_targets_2.npz')['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxkvant/.local/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/maxkvant/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "cifar_train = CustomTensorDataset(torch.tensor(X_train), torch.tensor(y_train).float(), transform='flips')\n",
    "cifar_test  = CustomTensorDataset(torch.tensor(X_test),  torch.tensor(labels_test, dtype=torch.long))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar_train, batch_size=1280 * 2)\n",
    "test_loader  = torch.utils.data.DataLoader(cifar_test,  batch_size=1280 * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models = 500\n",
    "\n",
    "# 500 * 50 * 32  = 800k\n",
    "\n",
    "models = [Myrtle7(num_filters=1, groups=50) for _ in range(n_models)]\n",
    "n_models\n",
    "\n",
    "estimator = GpEstimator(models, num_classes, 0.2, X_train[0], device, groups=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 ==========================\n",
      "batch 0: train_acc 0.1008, train_mse 1.000000\n",
      "batch 1: train_acc 0.0945, train_mse 1.000000\n",
      "batch 2: train_acc 0.1059, train_mse 1.000000\n",
      "batch 3: train_acc 0.1016, train_mse 1.000000\n",
      "batch 4: train_acc 0.0977, train_mse 1.000000\n",
      "batch 5: train_acc 0.1086, train_mse 1.000000\n",
      "batch 6: train_acc 0.0988, train_mse 1.000000\n",
      "batch 7: train_acc 0.1023, train_mse 1.000000\n",
      "batch 8: train_acc 0.1000, train_mse 1.000000\n",
      "batch 9: train_acc 0.1031, train_mse 1.000000\n",
      "batch 10: train_acc 0.1008, train_mse 1.000000\n",
      "batch 11: train_acc 0.1012, train_mse 1.000000\n",
      "batch 12: train_acc 0.0973, train_mse 1.000000\n",
      "batch 13: train_acc 0.0957, train_mse 1.000000\n",
      "iter 0 done. took 1115s test_acc 0.7820\n",
      "\n",
      "iter 1 ==========================\n",
      "batch 0: train_acc 0.8391, train_mse 0.170746\n",
      "batch 1: train_acc 0.8371, train_mse 0.172967\n",
      "batch 2: train_acc 0.8156, train_mse 0.173399\n",
      "batch 3: train_acc 0.8355, train_mse 0.172503\n",
      "batch 4: train_acc 0.8215, train_mse 0.173839\n",
      "batch 5: train_acc 0.8316, train_mse 0.175112\n",
      "batch 6: train_acc 0.8352, train_mse 0.172728\n",
      "batch 7: train_acc 0.8227, train_mse 0.176079\n",
      "batch 8: train_acc 0.8305, train_mse 0.172236\n",
      "batch 9: train_acc 0.8230, train_mse 0.171052\n",
      "batch 10: train_acc 0.8406, train_mse 0.171436\n",
      "batch 11: train_acc 0.8320, train_mse 0.171450\n",
      "batch 12: train_acc 0.8180, train_mse 0.174799\n",
      "batch 13: train_acc 0.8297, train_mse 0.175208\n",
      "iter 1 done. took 1115s test_acc 0.8076\n",
      "\n",
      "iter 2 ==========================\n",
      "batch 0: train_acc 0.8895, train_mse 0.144341\n",
      "batch 1: train_acc 0.8879, train_mse 0.145980\n",
      "batch 2: train_acc 0.8727, train_mse 0.147216\n",
      "batch 3: train_acc 0.8813, train_mse 0.146264\n",
      "batch 4: train_acc 0.8727, train_mse 0.147856\n",
      "batch 5: train_acc 0.8723, train_mse 0.149170\n",
      "batch 6: train_acc 0.8863, train_mse 0.146327\n",
      "batch 7: train_acc 0.8738, train_mse 0.149758\n",
      "batch 8: train_acc 0.8758, train_mse 0.147232\n",
      "batch 9: train_acc 0.8742, train_mse 0.145865\n",
      "batch 10: train_acc 0.8930, train_mse 0.145287\n",
      "batch 11: train_acc 0.8762, train_mse 0.146174\n",
      "batch 12: train_acc 0.8688, train_mse 0.148992\n",
      "batch 13: train_acc 0.8836, train_mse 0.149244\n",
      "iter 2 done. took 1114s test_acc 0.8199\n",
      "\n",
      "iter 3 ==========================\n",
      "batch 0: train_acc 0.9141, train_mse 0.129295\n",
      "batch 1: train_acc 0.9137, train_mse 0.131568\n",
      "batch 2: train_acc 0.9020, train_mse 0.132186\n",
      "batch 3: train_acc 0.9090, train_mse 0.130729\n",
      "batch 4: train_acc 0.9023, train_mse 0.133105\n",
      "batch 5: train_acc 0.9098, train_mse 0.133646\n",
      "batch 6: train_acc 0.9059, train_mse 0.131836\n",
      "batch 7: train_acc 0.9152, train_mse 0.133269\n",
      "batch 8: train_acc 0.9113, train_mse 0.131544\n",
      "batch 9: train_acc 0.9090, train_mse 0.130166\n",
      "batch 10: train_acc 0.9207, train_mse 0.130782\n",
      "batch 11: train_acc 0.9055, train_mse 0.131113\n",
      "batch 12: train_acc 0.9012, train_mse 0.133809\n",
      "batch 13: train_acc 0.9082, train_mse 0.133325\n",
      "iter 3 done. took 1114s test_acc 0.8303\n",
      "\n",
      "iter 4 ==========================\n",
      "batch 0: train_acc 0.9383, train_mse 0.118238\n",
      "batch 1: train_acc 0.9285, train_mse 0.120450\n",
      "batch 2: train_acc 0.9258, train_mse 0.120922\n",
      "batch 3: train_acc 0.9246, train_mse 0.120106\n",
      "batch 4: train_acc 0.9250, train_mse 0.120901\n",
      "batch 5: train_acc 0.9273, train_mse 0.122201\n",
      "batch 6: train_acc 0.9301, train_mse 0.120801\n",
      "batch 7: train_acc 0.9262, train_mse 0.121783\n",
      "batch 8: train_acc 0.9301, train_mse 0.120738\n",
      "batch 9: train_acc 0.9281, train_mse 0.119259\n",
      "batch 10: train_acc 0.9352, train_mse 0.119261\n",
      "batch 11: train_acc 0.9254, train_mse 0.119178\n",
      "batch 12: train_acc 0.9227, train_mse 0.122302\n",
      "batch 13: train_acc 0.9316, train_mse 0.122799\n",
      "iter 4 done. took 1114s test_acc 0.8356\n",
      "\n",
      "iter 5 ==========================\n",
      "batch 0: train_acc 0.9453, train_mse 0.110332\n",
      "batch 1: train_acc 0.9473, train_mse 0.111286\n",
      "batch 2: train_acc 0.9359, train_mse 0.112238\n",
      "batch 3: train_acc 0.9441, train_mse 0.110959\n",
      "batch 4: train_acc 0.9453, train_mse 0.111938\n",
      "batch 5: train_acc 0.9402, train_mse 0.113320\n",
      "batch 6: train_acc 0.9414, train_mse 0.112158\n",
      "batch 7: train_acc 0.9406, train_mse 0.113453\n",
      "batch 8: train_acc 0.9426, train_mse 0.111240\n",
      "batch 9: train_acc 0.9398, train_mse 0.111061\n",
      "batch 10: train_acc 0.9469, train_mse 0.110747\n",
      "batch 11: train_acc 0.9406, train_mse 0.110859\n",
      "batch 12: train_acc 0.9387, train_mse 0.114001\n",
      "batch 13: train_acc 0.9492, train_mse 0.113829\n",
      "iter 5 done. took 1113s test_acc 0.8382\n",
      "\n",
      "iter 6 ==========================\n",
      "batch 0: train_acc 0.9586, train_mse 0.102981\n",
      "batch 1: train_acc 0.9566, train_mse 0.104650\n",
      "batch 2: train_acc 0.9566, train_mse 0.104622\n",
      "batch 3: train_acc 0.9555, train_mse 0.103888\n",
      "batch 4: train_acc 0.9477, train_mse 0.105238\n",
      "batch 5: train_acc 0.9539, train_mse 0.105555\n",
      "batch 6: train_acc 0.9523, train_mse 0.105004\n",
      "batch 7: train_acc 0.9531, train_mse 0.106458\n",
      "batch 8: train_acc 0.9508, train_mse 0.105332\n",
      "batch 9: train_acc 0.9516, train_mse 0.103140\n",
      "batch 10: train_acc 0.9621, train_mse 0.103643\n",
      "batch 11: train_acc 0.9516, train_mse 0.102675\n",
      "batch 12: train_acc 0.9477, train_mse 0.105729\n",
      "batch 13: train_acc 0.9563, train_mse 0.106407\n",
      "iter 6 done. took 1113s test_acc 0.8419\n",
      "\n",
      "iter 7 ==========================\n",
      "batch 0: train_acc 0.9723, train_mse 0.096408\n",
      "batch 1: train_acc 0.9625, train_mse 0.097766\n",
      "batch 2: train_acc 0.9602, train_mse 0.098513\n",
      "batch 3: train_acc 0.9609, train_mse 0.097863\n",
      "batch 4: train_acc 0.9629, train_mse 0.098629\n",
      "batch 5: train_acc 0.9641, train_mse 0.098636\n",
      "batch 6: train_acc 0.9633, train_mse 0.098136\n",
      "batch 7: train_acc 0.9625, train_mse 0.099670\n",
      "batch 8: train_acc 0.9609, train_mse 0.098708\n",
      "batch 9: train_acc 0.9625, train_mse 0.097134\n",
      "batch 10: train_acc 0.9656, train_mse 0.097576\n",
      "batch 11: train_acc 0.9637, train_mse 0.096981\n",
      "batch 12: train_acc 0.9582, train_mse 0.099728\n",
      "batch 13: train_acc 0.9707, train_mse 0.099282\n",
      "iter 7 done. took 1113s test_acc 0.8461\n",
      "\n",
      "iter 8 ==========================\n",
      "batch 0: train_acc 0.9727, train_mse 0.091395\n",
      "batch 1: train_acc 0.9746, train_mse 0.092449\n",
      "batch 2: train_acc 0.9699, train_mse 0.092422\n",
      "batch 3: train_acc 0.9723, train_mse 0.091234\n",
      "batch 4: train_acc 0.9691, train_mse 0.093538\n",
      "batch 5: train_acc 0.9699, train_mse 0.094076\n",
      "batch 6: train_acc 0.9680, train_mse 0.093675\n",
      "batch 7: train_acc 0.9691, train_mse 0.094575\n",
      "batch 8: train_acc 0.9684, train_mse 0.092627\n",
      "batch 9: train_acc 0.9660, train_mse 0.092031\n",
      "batch 10: train_acc 0.9742, train_mse 0.091853\n",
      "batch 11: train_acc 0.9703, train_mse 0.091509\n",
      "batch 12: train_acc 0.9629, train_mse 0.094767\n",
      "batch 13: train_acc 0.9734, train_mse 0.094645\n",
      "iter 8 done. took 1113s test_acc 0.8485\n",
      "\n",
      "iter 9 ==========================\n",
      "batch 0: train_acc 0.9793, train_mse 0.087696\n",
      "batch 1: train_acc 0.9801, train_mse 0.087870\n",
      "batch 2: train_acc 0.9781, train_mse 0.088200\n",
      "batch 3: train_acc 0.9754, train_mse 0.087070\n",
      "batch 4: train_acc 0.9785, train_mse 0.088185\n",
      "batch 5: train_acc 0.9754, train_mse 0.088264\n",
      "batch 6: train_acc 0.9762, train_mse 0.088129\n",
      "batch 7: train_acc 0.9793, train_mse 0.088343\n",
      "batch 8: train_acc 0.9770, train_mse 0.087816\n",
      "batch 9: train_acc 0.9715, train_mse 0.087170\n",
      "batch 10: train_acc 0.9812, train_mse 0.086862\n",
      "batch 11: train_acc 0.9711, train_mse 0.087483\n",
      "batch 12: train_acc 0.9719, train_mse 0.089940\n",
      "batch 13: train_acc 0.9777, train_mse 0.089763\n",
      "iter 9 done. took 1113s test_acc 0.8505\n",
      "\n",
      "iter 10 ==========================\n",
      "batch 0: train_acc 0.9848, train_mse 0.082129\n",
      "batch 1: train_acc 0.9812, train_mse 0.083611\n",
      "batch 2: train_acc 0.9781, train_mse 0.083588\n",
      "batch 3: train_acc 0.9762, train_mse 0.083447\n",
      "batch 4: train_acc 0.9801, train_mse 0.084422\n",
      "batch 5: train_acc 0.9797, train_mse 0.084280\n",
      "batch 6: train_acc 0.9762, train_mse 0.083963\n",
      "batch 7: train_acc 0.9809, train_mse 0.085675\n",
      "batch 8: train_acc 0.9801, train_mse 0.084188\n",
      "batch 9: train_acc 0.9785, train_mse 0.083051\n",
      "batch 10: train_acc 0.9844, train_mse 0.082673\n",
      "batch 11: train_acc 0.9746, train_mse 0.083046\n",
      "batch 12: train_acc 0.9793, train_mse 0.085035\n",
      "batch 13: train_acc 0.9828, train_mse 0.084815\n",
      "iter 10 done. took 1113s test_acc 0.8521\n",
      "\n",
      "iter 11 ==========================\n",
      "batch 0: train_acc 0.9871, train_mse 0.077784\n",
      "batch 1: train_acc 0.9840, train_mse 0.079788\n",
      "batch 2: train_acc 0.9844, train_mse 0.080631\n",
      "batch 3: train_acc 0.9828, train_mse 0.078741\n",
      "batch 4: train_acc 0.9840, train_mse 0.079790\n",
      "batch 5: train_acc 0.9863, train_mse 0.080531\n",
      "batch 6: train_acc 0.9828, train_mse 0.080341\n",
      "batch 7: train_acc 0.9832, train_mse 0.080461\n",
      "batch 8: train_acc 0.9824, train_mse 0.080277\n",
      "batch 9: train_acc 0.9863, train_mse 0.078006\n",
      "batch 10: train_acc 0.9816, train_mse 0.078738\n",
      "batch 11: train_acc 0.9797, train_mse 0.078832\n",
      "batch 12: train_acc 0.9816, train_mse 0.081308\n",
      "batch 13: train_acc 0.9863, train_mse 0.081669\n",
      "iter 11 done. took 1112s test_acc 0.8523\n",
      "\n",
      "iter 12 ==========================\n",
      "batch 0: train_acc 0.9895, train_mse 0.075097\n",
      "batch 1: train_acc 0.9891, train_mse 0.075854\n",
      "batch 2: train_acc 0.9855, train_mse 0.076621\n",
      "batch 3: train_acc 0.9875, train_mse 0.075721\n",
      "batch 4: train_acc 0.9852, train_mse 0.076411\n",
      "batch 5: train_acc 0.9883, train_mse 0.076936\n",
      "batch 6: train_acc 0.9840, train_mse 0.076752\n",
      "batch 7: train_acc 0.9840, train_mse 0.077650\n",
      "batch 8: train_acc 0.9867, train_mse 0.076229\n",
      "batch 9: train_acc 0.9844, train_mse 0.075450\n",
      "batch 10: train_acc 0.9895, train_mse 0.075104\n",
      "batch 11: train_acc 0.9867, train_mse 0.074566\n",
      "batch 12: train_acc 0.9863, train_mse 0.077128\n",
      "batch 13: train_acc 0.9863, train_mse 0.078010\n",
      "iter 12 done. took 1112s test_acc 0.8542\n",
      "\n",
      "iter 13 ==========================\n",
      "batch 0: train_acc 0.9895, train_mse 0.072708\n",
      "batch 1: train_acc 0.9891, train_mse 0.072989\n",
      "batch 2: train_acc 0.9879, train_mse 0.073282\n",
      "batch 3: train_acc 0.9902, train_mse 0.071759\n",
      "batch 4: train_acc 0.9887, train_mse 0.073547\n",
      "batch 5: train_acc 0.9902, train_mse 0.073436\n",
      "batch 6: train_acc 0.9887, train_mse 0.073333\n",
      "batch 7: train_acc 0.9898, train_mse 0.073329\n",
      "batch 8: train_acc 0.9887, train_mse 0.073380\n",
      "batch 9: train_acc 0.9867, train_mse 0.072356\n",
      "batch 10: train_acc 0.9898, train_mse 0.071739\n",
      "batch 11: train_acc 0.9883, train_mse 0.072047\n",
      "batch 12: train_acc 0.9855, train_mse 0.074857\n",
      "batch 13: train_acc 0.9941, train_mse 0.073408\n",
      "iter 13 done. took 1112s test_acc 0.8563\n",
      "\n",
      "iter 14 ==========================\n",
      "batch 0: train_acc 0.9918, train_mse 0.069190\n",
      "batch 1: train_acc 0.9934, train_mse 0.069827\n",
      "batch 2: train_acc 0.9902, train_mse 0.071005\n",
      "batch 3: train_acc 0.9910, train_mse 0.069140\n",
      "batch 4: train_acc 0.9910, train_mse 0.069893\n",
      "batch 5: train_acc 0.9945, train_mse 0.069912\n",
      "batch 6: train_acc 0.9891, train_mse 0.070836\n",
      "batch 7: train_acc 0.9875, train_mse 0.071025\n",
      "batch 8: train_acc 0.9898, train_mse 0.070474\n",
      "batch 9: train_acc 0.9891, train_mse 0.068996\n",
      "batch 10: train_acc 0.9906, train_mse 0.069189\n",
      "batch 11: train_acc 0.9918, train_mse 0.068927\n",
      "batch 12: train_acc 0.9879, train_mse 0.071474\n",
      "batch 13: train_acc 0.9945, train_mse 0.070839\n",
      "iter 14 done. took 1112s test_acc 0.8579\n",
      "\n",
      "iter 15 ==========================\n",
      "batch 0: train_acc 0.9957, train_mse 0.065983\n",
      "batch 1: train_acc 0.9922, train_mse 0.067107\n",
      "batch 2: train_acc 0.9910, train_mse 0.066837\n",
      "batch 3: train_acc 0.9926, train_mse 0.066637\n",
      "batch 4: train_acc 0.9934, train_mse 0.066860\n",
      "batch 5: train_acc 0.9922, train_mse 0.067674\n",
      "batch 6: train_acc 0.9934, train_mse 0.067628\n",
      "batch 7: train_acc 0.9895, train_mse 0.067889\n",
      "batch 8: train_acc 0.9914, train_mse 0.068270\n",
      "batch 9: train_acc 0.9914, train_mse 0.066179\n",
      "batch 10: train_acc 0.9926, train_mse 0.066548\n",
      "batch 11: train_acc 0.9930, train_mse 0.066253\n",
      "batch 12: train_acc 0.9891, train_mse 0.069131\n",
      "batch 13: train_acc 0.9957, train_mse 0.067304\n",
      "iter 15 done. took 1112s test_acc 0.8583\n",
      "\n",
      "iter 16 ==========================\n",
      "batch 0: train_acc 0.9957, train_mse 0.063945\n",
      "batch 1: train_acc 0.9969, train_mse 0.064228\n",
      "batch 2: train_acc 0.9945, train_mse 0.064635\n",
      "batch 3: train_acc 0.9938, train_mse 0.063816\n",
      "batch 4: train_acc 0.9930, train_mse 0.064675\n",
      "batch 5: train_acc 0.9953, train_mse 0.065093\n",
      "batch 6: train_acc 0.9934, train_mse 0.065161\n",
      "batch 7: train_acc 0.9945, train_mse 0.065174\n",
      "batch 8: train_acc 0.9941, train_mse 0.064720\n",
      "batch 9: train_acc 0.9914, train_mse 0.063625\n",
      "batch 10: train_acc 0.9930, train_mse 0.064100\n",
      "batch 11: train_acc 0.9949, train_mse 0.063432\n",
      "batch 12: train_acc 0.9914, train_mse 0.065732\n",
      "batch 13: train_acc 0.9953, train_mse 0.065081\n",
      "iter 16 done. took 1111s test_acc 0.8597\n",
      "\n",
      "iter 17 ==========================\n",
      "batch 0: train_acc 0.9941, train_mse 0.061424\n",
      "batch 1: train_acc 0.9977, train_mse 0.061991\n",
      "batch 2: train_acc 0.9930, train_mse 0.062287\n",
      "batch 3: train_acc 0.9949, train_mse 0.061096\n",
      "batch 4: train_acc 0.9945, train_mse 0.062442\n",
      "batch 5: train_acc 0.9965, train_mse 0.062806\n",
      "batch 6: train_acc 0.9945, train_mse 0.063041\n",
      "batch 7: train_acc 0.9957, train_mse 0.063221\n",
      "batch 8: train_acc 0.9941, train_mse 0.062592\n",
      "batch 9: train_acc 0.9934, train_mse 0.061556\n",
      "batch 10: train_acc 0.9941, train_mse 0.061669\n",
      "batch 11: train_acc 0.9953, train_mse 0.061091\n",
      "batch 12: train_acc 0.9918, train_mse 0.063077\n",
      "batch 13: train_acc 0.9965, train_mse 0.063641\n",
      "iter 17 done. took 1111s test_acc 0.8590\n",
      "\n",
      "iter 18 ==========================\n",
      "batch 0: train_acc 0.9980, train_mse 0.058782\n",
      "batch 1: train_acc 0.9973, train_mse 0.059365\n",
      "batch 2: train_acc 0.9949, train_mse 0.060210\n",
      "batch 3: train_acc 0.9965, train_mse 0.059697\n",
      "batch 4: train_acc 0.9969, train_mse 0.059799\n",
      "batch 5: train_acc 0.9941, train_mse 0.060212\n",
      "batch 6: train_acc 0.9965, train_mse 0.060145\n",
      "batch 7: train_acc 0.9945, train_mse 0.060404\n",
      "batch 8: train_acc 0.9957, train_mse 0.060051\n",
      "batch 9: train_acc 0.9949, train_mse 0.059581\n",
      "batch 10: train_acc 0.9969, train_mse 0.059016\n",
      "batch 11: train_acc 0.9969, train_mse 0.059165\n",
      "batch 12: train_acc 0.9938, train_mse 0.061729\n",
      "batch 13: train_acc 0.9988, train_mse 0.061020\n",
      "iter 18 done. took 1111s test_acc 0.8595\n",
      "\n",
      "iter 19 ==========================\n",
      "batch 0: train_acc 0.9984, train_mse 0.056760\n",
      "batch 1: train_acc 0.9984, train_mse 0.057212\n",
      "batch 2: train_acc 0.9941, train_mse 0.057822\n",
      "batch 3: train_acc 0.9969, train_mse 0.056581\n",
      "batch 4: train_acc 0.9973, train_mse 0.057803\n",
      "batch 5: train_acc 0.9965, train_mse 0.058357\n",
      "batch 6: train_acc 0.9973, train_mse 0.058404\n",
      "batch 7: train_acc 0.9965, train_mse 0.058361\n",
      "batch 8: train_acc 0.9961, train_mse 0.058475\n",
      "batch 9: train_acc 0.9945, train_mse 0.057115\n",
      "batch 10: train_acc 0.9961, train_mse 0.057090\n",
      "batch 11: train_acc 0.9973, train_mse 0.057053\n",
      "batch 12: train_acc 0.9941, train_mse 0.059540\n",
      "batch 13: train_acc 0.9984, train_mse 0.058978\n",
      "iter 19 done. took 1111s test_acc 0.8602\n",
      "\n",
      "iter 20 ==========================\n",
      "batch 0: train_acc 0.9973, train_mse 0.055651\n",
      "batch 1: train_acc 0.9984, train_mse 0.055758\n",
      "batch 2: train_acc 0.9973, train_mse 0.055586\n",
      "batch 3: train_acc 0.9965, train_mse 0.055171\n",
      "batch 4: train_acc 0.9965, train_mse 0.055763\n",
      "batch 5: train_acc 0.9965, train_mse 0.056156\n",
      "batch 6: train_acc 0.9961, train_mse 0.056731\n",
      "batch 7: train_acc 0.9961, train_mse 0.056537\n",
      "batch 8: train_acc 0.9969, train_mse 0.055438\n",
      "batch 9: train_acc 0.9965, train_mse 0.054913\n",
      "batch 10: train_acc 0.9969, train_mse 0.054927\n",
      "batch 11: train_acc 0.9969, train_mse 0.054996\n",
      "batch 12: train_acc 0.9969, train_mse 0.057696\n",
      "batch 13: train_acc 0.9984, train_mse 0.056737\n",
      "iter 20 done. took 1111s test_acc 0.8599\n",
      "\n",
      "iter 21 ==========================\n",
      "batch 0: train_acc 0.9988, train_mse 0.053197\n",
      "batch 1: train_acc 0.9984, train_mse 0.053717\n",
      "batch 2: train_acc 0.9969, train_mse 0.053986\n",
      "batch 3: train_acc 0.9980, train_mse 0.053194\n",
      "batch 4: train_acc 0.9969, train_mse 0.054040\n",
      "batch 5: train_acc 0.9977, train_mse 0.054205\n",
      "batch 6: train_acc 0.9973, train_mse 0.054433\n",
      "batch 7: train_acc 0.9969, train_mse 0.054513\n",
      "batch 8: train_acc 0.9980, train_mse 0.054074\n",
      "batch 9: train_acc 0.9977, train_mse 0.052848\n",
      "batch 10: train_acc 0.9980, train_mse 0.053153\n",
      "batch 11: train_acc 0.9984, train_mse 0.052906\n",
      "batch 12: train_acc 0.9961, train_mse 0.054991\n",
      "batch 13: train_acc 0.9984, train_mse 0.054722\n",
      "iter 21 done. took 1111s test_acc 0.8611\n",
      "\n",
      "iter 22 ==========================\n",
      "batch 0: train_acc 0.9996, train_mse 0.051005\n",
      "batch 1: train_acc 0.9984, train_mse 0.052182\n",
      "batch 2: train_acc 0.9957, train_mse 0.052618\n",
      "batch 3: train_acc 0.9973, train_mse 0.051569\n",
      "batch 4: train_acc 0.9973, train_mse 0.052301\n",
      "batch 5: train_acc 0.9980, train_mse 0.052800\n",
      "batch 6: train_acc 0.9980, train_mse 0.051990\n",
      "batch 7: train_acc 0.9973, train_mse 0.052760\n",
      "batch 8: train_acc 0.9988, train_mse 0.052537\n",
      "batch 9: train_acc 0.9961, train_mse 0.051465\n",
      "batch 10: train_acc 0.9973, train_mse 0.052178\n",
      "batch 11: train_acc 0.9969, train_mse 0.051433\n",
      "batch 12: train_acc 0.9977, train_mse 0.052787\n",
      "batch 13: train_acc 0.9988, train_mse 0.053163\n",
      "iter 22 done. took 1111s test_acc 0.8618\n",
      "\n",
      "iter 23 ==========================\n",
      "batch 0: train_acc 0.9988, train_mse 0.049914\n",
      "batch 1: train_acc 0.9988, train_mse 0.050472\n",
      "batch 2: train_acc 0.9988, train_mse 0.051013\n",
      "batch 3: train_acc 0.9980, train_mse 0.049689\n",
      "batch 4: train_acc 0.9980, train_mse 0.050636\n",
      "batch 5: train_acc 0.9988, train_mse 0.050904\n",
      "batch 6: train_acc 0.9977, train_mse 0.050926\n",
      "batch 7: train_acc 0.9988, train_mse 0.050986\n",
      "batch 8: train_acc 0.9992, train_mse 0.050099\n",
      "batch 9: train_acc 0.9973, train_mse 0.049848\n",
      "batch 10: train_acc 0.9977, train_mse 0.049817\n",
      "batch 11: train_acc 0.9977, train_mse 0.049644\n",
      "batch 12: train_acc 0.9980, train_mse 0.051823\n",
      "batch 13: train_acc 0.9988, train_mse 0.051484\n",
      "iter 23 done. took 1111s test_acc 0.8626\n",
      "\n",
      "iter 24 ==========================\n",
      "batch 0: train_acc 0.9992, train_mse 0.048372\n",
      "batch 1: train_acc 0.9996, train_mse 0.048572\n",
      "batch 2: train_acc 0.9988, train_mse 0.048730\n",
      "batch 3: train_acc 0.9988, train_mse 0.048351\n",
      "batch 4: train_acc 0.9977, train_mse 0.049168\n",
      "batch 5: train_acc 0.9988, train_mse 0.049358\n",
      "batch 6: train_acc 0.9988, train_mse 0.049836\n",
      "batch 7: train_acc 0.9984, train_mse 0.049499\n",
      "batch 8: train_acc 0.9988, train_mse 0.049129\n",
      "batch 9: train_acc 0.9984, train_mse 0.048203\n",
      "batch 10: train_acc 0.9977, train_mse 0.048680\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-fa8b3dfd17e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mboosting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-62-7bce11ae3558>\u001b[0m in \u001b[0;36mboosting\u001b[0;34m(estimator, train_loader, test_loader, learning_rate, n_iter)\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mright_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_right_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_residual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0mcur_w_update\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_w_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mpred_change\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-1fb7bd0659ad>\u001b[0m in \u001b[0;36mget_w_update\u001b[0;34m(self, X, right_vector)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_model_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                 \u001b[0mupdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mw_updates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-1fb7bd0659ad>\u001b[0m in \u001b[0;36mto_model_features\u001b[0;34m(self, X, model)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcalc_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/LinearizedNNs/notebooks/LinearizedNNs-master/src/pytorch_impl/nns/myrtle.py\u001b[0m in \u001b[0;36mreadout\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/LinearizedNNs/notebooks/LinearizedNNs-master/src/pytorch_impl/nns/primitives.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1061\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1062\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "boosting(estimator, train_loader, test_loader, learning_rate=1e6, n_iter=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 ======================= \n",
      "train_acc 0.09921874850988388\n",
      "batch_0 took 57.80506658554077s\n",
      "\n",
      "train_acc 0.09140624850988388\n",
      "batch_1 took 55.33869671821594s\n",
      "\n",
      "train_acc 0.1328125\n",
      "batch_2 took 55.32820272445679s\n",
      "\n",
      "train_acc 0.09296875447034836\n",
      "batch_3 took 55.33759117126465s\n",
      "\n",
      "train_acc 0.09375\n",
      "batch_4 took 55.329872846603394s\n",
      "\n",
      "train_acc 0.1171875\n",
      "batch_5 took 55.33920168876648s\n",
      "\n",
      "train_acc 0.09765625\n",
      "batch_6 took 55.32954454421997s\n",
      "\n",
      "train_acc 0.17734375596046448\n",
      "batch_7 took 55.33659553527832s\n",
      "\n",
      "train_acc 0.09375\n",
      "batch_8 took 55.32691836357117s\n",
      "\n",
      "train_acc 0.17499999701976776\n",
      "batch_9 took 55.34224081039429s\n",
      "\n",
      "iter 0 done, test_acc = 0.1120000034570694\n",
      "iter 1 ======================= \n",
      "train_acc 0.11015625298023224\n",
      "batch_0 took 55.32840847969055s\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-9519a5e3e32e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"train_acc {train_acc}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-3e9447a2e52e>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_model_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-3e9447a2e52e>\u001b[0m in \u001b[0;36mto_model_features\u001b[0;34m(self, X, model)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcalc_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_models = 5000\n",
    "\n",
    "models = [Myrtle5(num_filters=32) for _ in range(n_models)]\n",
    "\n",
    "learning_rate = 0.2\n",
    "estimator = GpEstimator(models, num_classes, learning_rate, X_train[0], device)\n",
    "\n",
    "n_iter = 20\n",
    "\n",
    "for iter_ in range(n_iter):\n",
    "    print(f\"iter {iter_} ======================= \")\n",
    "    \n",
    "    for batch_i, (X, labels) in enumerate(train_loader):\n",
    "        batch_start = time.time()\n",
    "        \n",
    "        X      = X.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        y_pred = estimator.predict(X)\n",
    "        train_acc = (y_pred.argmax(dim=1) == labels).float().mean().item()\n",
    "        print(f\"train_acc {train_acc}\")\n",
    "        \n",
    "        y_pred.requires_grad = True\n",
    "\n",
    "        loss = nn.CrossEntropyLoss()(y_pred, labels)\n",
    "        loss.backward()\n",
    "        grad = y_pred.grad\n",
    "        \n",
    "        estimator.w += -learning_rate * estimator.get_w_update(X, grad)\n",
    "        \n",
    "        print(f\"batch_{batch_i} took {time.time() - batch_start:0}s\")\n",
    "        print()\n",
    "        \n",
    "    y_pred = estimator.predict(X_test)\n",
    "    test_acc = (y_pred.argmax(dim=1) == labels_test.to(device)).float().mean().item()\n",
    "    print(f\"iter {iter_} done, test_acc = {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "X_train, labels_train, X_test, labels_test = get_cifar_zca()\n",
    "\n",
    "N = 1280\n",
    "\n",
    "X_train      = torch.tensor(X_train[:N]).float()\n",
    "X_test       = torch.tensor(X_test[:N]).float()\n",
    "labels_train = torch.tensor(labels_train[:N], dtype=torch.long)\n",
    "labels_test  = torch.tensor(labels_test[:N],  dtype=torch.long)\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "y_train = to_one_hot(labels_train, num_classes).to(device)\n",
    "y_test  = to_one_hot(labels_test,  num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models = 5000\n",
    "\n",
    "models = [Myrtle5(num_filters=32) for _ in range(n_models)]\n",
    "\n",
    "estimator = GpEstimator(models, num_classes, 1e4, X_train[0], device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6102)\n",
      "\n",
      "CPU times: user 1min 16s, sys: 25.5 s, total: 1min 42s\n",
      "Wall time: 1min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_kernel, test_kernel = estimator.calc_kernels(X_train, X_test)\n",
    "\n",
    "right_vector = calc_right_vector(train_kernel, y_train, learning_rate=1e5, reg_param=0)\n",
    "y_pred = torch.matmul(test_kernel, right_vector).argmax(dim=1)\n",
    "\n",
    "test_acc = (y_pred.cpu() == labels_test).float().mean()\n",
    "print(test_acc)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 32, 10])\n",
      "\n",
      "CPU times: user 34.7 s, sys: 14.7 s, total: 49.4 s\n",
      "Wall time: 49.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "w_update = estimator.get_w_update(X_train, right_vector)\n",
    "\n",
    "print(w_update.size())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.w = w_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6102)\n",
      "\n",
      "CPU times: user 35.2 s, sys: 14.9 s, total: 50.1 s\n",
      "Wall time: 50 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "y_pred = estimator.predict(X_test).argmax(dim=1)\n",
    "\n",
    "test_acc = (y_pred.cpu() == labels_test).float().mean()\n",
    "print(test_acc)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
